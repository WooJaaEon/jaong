{
  
    
        "post0": {
            "title": "인구 데이터 기반 소득 예측 경진대회",
            "content": "&#51064;&#44396; &#45936;&#51060;&#53552; &#44592;&#48152; &#49548;&#46301; &#50696;&#52769; &#44221;&#51652;&#45824;&#54924; . 대회 링크 : https://dacon.io/competitions/official/235892/overview/description . | 목적 : 인구 데이터 바탕으로 소득이 5만달러 이하인지 초과인지 분류하기 . | 데이터: . train.csv : 학습 데이터 | test.csv : 테스트 데이터 | sample_submission.csv : 제출 양식 | | . train data . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . &#45936;&#51060;&#53552; &#54869;&#51064;&#54616;&#44592; . import pandas as pd . train = pd.read_csv(&#39;/content/drive/MyDrive/Colab/data/train.csv&#39;) train . id age workclass fnlwgt education education.num marital.status occupation relationship race sex capital.gain capital.loss hours.per.week native.country target . 0 0 | 32 | Private | 309513 | Assoc-acdm | 12 | Married-civ-spouse | Craft-repair | Husband | White | Male | 0 | 0 | 40 | United-States | 0 | . 1 1 | 33 | Private | 205469 | Some-college | 10 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 40 | United-States | 1 | . 2 2 | 46 | Private | 149949 | Some-college | 10 | Married-civ-spouse | Craft-repair | Husband | White | Male | 0 | 0 | 40 | United-States | 0 | . 3 3 | 23 | Private | 193090 | Bachelors | 13 | Never-married | Adm-clerical | Own-child | White | Female | 0 | 0 | 30 | United-States | 0 | . 4 4 | 55 | Private | 60193 | HS-grad | 9 | Divorced | Adm-clerical | Not-in-family | White | Female | 0 | 0 | 40 | United-States | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 17475 17475 | 35 | NaN | 320084 | Bachelors | 13 | Married-civ-spouse | NaN | Wife | White | Female | 0 | 0 | 55 | United-States | 1 | . 17476 17476 | 30 | NaN | 33811 | Bachelors | 13 | Never-married | NaN | Not-in-family | Asian-Pac-Islander | Female | 0 | 0 | 99 | United-States | 0 | . 17477 17477 | 71 | NaN | 287372 | Doctorate | 16 | Married-civ-spouse | NaN | Husband | White | Male | 0 | 0 | 10 | United-States | 1 | . 17478 17478 | 41 | NaN | 202822 | HS-grad | 9 | Separated | NaN | Not-in-family | Black | Female | 0 | 0 | 32 | United-States | 0 | . 17479 17479 | 72 | NaN | 129912 | HS-grad | 9 | Married-civ-spouse | NaN | Husband | White | Male | 0 | 0 | 25 | United-States | 0 | . 17480 rows × 16 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train.csv : 학습 데이터 | id : 샘플 아이디 | age : 나이 | workclass : 일 유형 | fnlwgt : CPS(Current Population Survey) 가중치 | education : 교육수준 | education.num : 교육수준 번호 | marital.status : 결혼 상태 | occupation : 직업 | relationship : 가족관계 | race : 인종 | sex : 성별 | capital.gain : 자본 이익 | capital.loss : 자본 손실 | hours.per.week : 주당 근무시간 | native.country : 본 국적 | target : 소득 . 0 = &lt;=50K (5만 달러 이하) 1 = &gt;50K (5만 달러 초과) . | . &#44208;&#52769;&#52824; &#54869;&#51064;&#54616;&#44592; . def check_missing_col(dataframe): missing_col = [] for col in dataframe.columns: missing_values = sum(dataframe[col].isna()) is_missing = True if missing_values &gt;= 1 else False if is_missing: print(f&#39;결측치가 있는 컬럼은: {col} 입니다&#39;) print(f&#39;해당 컬럼에 총 {missing_values} 개의 결측치가 존재합니다.&#39;) missing_col.append([col, dataframe[col].dtype]) if missing_col == []: print(&#39;결측치가 존재하지 않습니다&#39;) return missing_col missing_col = check_missing_col(train) . 결측치가 있는 컬럼은: workclass 입니다 해당 컬럼에 총 1836 개의 결측치가 존재합니다. 결측치가 있는 컬럼은: occupation 입니다 해당 컬럼에 총 1843 개의 결측치가 존재합니다. 결측치가 있는 컬럼은: native.country 입니다 해당 컬럼에 총 583 개의 결측치가 존재합니다. . print(train[&#39;workclass&#39;].unique()) print(train[&#39;occupation&#39;].unique()) print(train[&#39;native.country&#39;].unique()) . [&#39;Private&#39; &#39;State-gov&#39; &#39;Local-gov&#39; &#39;Self-emp-not-inc&#39; &#39;Self-emp-inc&#39; &#39;Federal-gov&#39; &#39;Without-pay&#39; nan &#39;Never-worked&#39;] [&#39;Craft-repair&#39; &#39;Exec-managerial&#39; &#39;Adm-clerical&#39; &#39;Prof-specialty&#39; &#39;Machine-op-inspct&#39; &#39;Other-service&#39; &#39;Sales&#39; &#39;Farming-fishing&#39; &#39;Transport-moving&#39; &#39;Handlers-cleaners&#39; &#39;Tech-support&#39; &#39;Protective-serv&#39; &#39;Priv-house-serv&#39; &#39;Armed-Forces&#39; nan] [&#39;United-States&#39; &#39;Poland&#39; &#39;Mexico&#39; &#39;Ireland&#39; &#39;Guatemala&#39; &#39;Dominican-Republic&#39; &#39;Greece&#39; &#39;El-Salvador&#39; &#39;Portugal&#39; &#39;Canada&#39; &#39;Philippines&#39; &#39;India&#39; &#39;Italy&#39; &#39;England&#39; &#39;Jamaica&#39; &#39;Columbia&#39; &#39;South&#39; &#39;Vietnam&#39; &#39;Cuba&#39; &#39;Laos&#39; &#39;Hong&#39; &#39;Haiti&#39; &#39;Germany&#39; &#39;Yugoslavia&#39; &#39;Ecuador&#39; &#39;France&#39; &#39;Puerto-Rico&#39; &#39;Outlying-US(Guam-USVI-etc)&#39; &#39;Taiwan&#39; &#39;China&#39; &#39;Japan&#39; &#39;Honduras&#39; &#39;Peru&#39; &#39;Nicaragua&#39; &#39;Hungary&#39; &#39;Cambodia&#39; &#39;Iran&#39; &#39;Trinadad&amp;Tobago&#39; &#39;Thailand&#39; &#39;Scotland&#39; &#39;Holand-Netherlands&#39; nan] . def handle_na(data, missing_col): temp = data.copy() for col, dtype in missing_col: if dtype == &#39;O&#39;: # 범주형 feature가 결측치인 경우 해당 행들을 삭제해 주었습니다. temp = temp.dropna(subset=[col]) return temp train = handle_na(train, missing_col) # 결측치 처리가 잘 되었는지 확인해 줍니다. missing_col = check_missing_col(train) . 결측치가 존재하지 않습니다 . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 15081 entries, 0 to 15080 Data columns (total 16 columns): # Column Non-Null Count Dtype -- -- 0 id 15081 non-null int64 1 age 15081 non-null int64 2 workclass 15081 non-null object 3 fnlwgt 15081 non-null int64 4 education 15081 non-null object 5 education.num 15081 non-null int64 6 marital.status 15081 non-null object 7 occupation 15081 non-null object 8 relationship 15081 non-null object 9 race 15081 non-null object 10 sex 15081 non-null object 11 capital.gain 15081 non-null int64 12 capital.loss 15081 non-null int64 13 hours.per.week 15081 non-null int64 14 native.country 15081 non-null object 15 target 15081 non-null int64 dtypes: int64(8), object(8) memory usage: 2.0+ MB . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 범주형 변수들을 라벨 인코딩 . def make_label_map(dataframe): label_maps = {} for col in dataframe.columns: if dataframe[col].dtype==&#39;object&#39;: label_map = {&#39;unknown&#39;:0} for i, key in enumerate(dataframe[col].unique()): label_map[key] = i #새로 등장하는 유니크 값들에 대해 1부터 1씩 증가시켜 키값을 부여해줍니다. label_maps[col] = label_map return label_maps # 각 범주형 변수에 인코딩 값을 부여하는 함수 def label_encoder(dataframe, label_map): for col in dataframe.columns: if dataframe[col].dtype==&#39;object&#39;: dataframe[col] = dataframe[col].map(label_map[col]) #dataframe[col] = dataframe[col].fillna(label_map[col][&#39;unknown&#39;]) #혹시 모를 결측값은 unknown의 값(0)으로 채워줍니다. return dataframe train = label_encoder(train, make_label_map(train)) . train . id age workclass fnlwgt education education.num marital.status occupation relationship race sex capital.gain capital.loss hours.per.week native.country target . 0 0 | 32 | 0 | 309513 | 0 | 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 40 | 0 | 0 | . 1 1 | 33 | 0 | 205469 | 1 | 10 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 40 | 0 | 1 | . 2 2 | 46 | 0 | 149949 | 1 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 40 | 0 | 0 | . 3 3 | 23 | 0 | 193090 | 2 | 13 | 1 | 2 | 1 | 0 | 1 | 0 | 0 | 30 | 0 | 0 | . 4 4 | 55 | 0 | 60193 | 3 | 9 | 2 | 2 | 2 | 0 | 1 | 0 | 0 | 40 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 15076 15076 | 35 | 0 | 337286 | 7 | 14 | 1 | 1 | 2 | 2 | 0 | 0 | 0 | 40 | 0 | 0 | . 15077 15077 | 36 | 0 | 182074 | 1 | 10 | 2 | 2 | 2 | 0 | 0 | 0 | 0 | 45 | 0 | 0 | . 15078 15078 | 50 | 4 | 175070 | 4 | 15 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 45 | 0 | 1 | . 15079 15079 | 39 | 0 | 202937 | 1 | 10 | 2 | 10 | 2 | 0 | 1 | 0 | 0 | 40 | 1 | 0 | . 15080 15080 | 33 | 0 | 96245 | 0 | 12 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 50 | 0 | 0 | . 15081 rows × 16 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; &#47784;&#45944; &#49440;&#50616;&#44284; &#54617;&#49845; . 변수 및 모델 정의 | . X = train.drop([&#39;id&#39;,&#39;target&#39;], axis=1) y = train[&#39;target&#39;] . from sklearn.linear_model import LogisticRegression # 모델 선언 model = LogisticRegression(solver=&#39;liblinear&#39;) # 모델 학습 model.fit(X, y) . LogisticRegression(solver=&#39;liblinear&#39;) . import numpy as np def ACCURACY(true, pred): score = np.mean(true==pred) return score . prediction = model.predict(X) score = ACCURACY(y, prediction) print(f&quot;모델의 정확도는 {score*100:.2f}% 입니다&quot;) . 모델의 정확도는 79.09% 입니다 . train.csv &#54617;&#49845;/&#44160;&#51613;&#49483; &#48516;&#47532;&#54616;&#44592; . from sklearn.model_selection import train_test_split data = train.drop(&#39;id&#39;, axis = 1).copy() #필요없는 id열 삭제 train_data, val_data = train_test_split(data, test_size=0.5) train_data.reset_index(inplace=True) #전처리 과정에서 데이터가 뒤섞이지 않도록 인덱스를 초기화 val_data.reset_index(inplace=True) . print( &#39;train 데이터 셋 모양 :&#39;, train_data.shape) print( &#39;val 데이터 셋 모양 :&#39;, val_data.shape) . train 데이터 셋 모양 : (7540, 16) val 데이터 셋 모양 : (7541, 16) . train_data.head() . index age workclass fnlwgt education education.num marital.status occupation relationship race sex capital.gain capital.loss hours.per.week native.country target . 0 8975 | 27 | 0 | 29904 | 2 | 13 | 1 | 10 | 2 | 0 | 1 | 0 | 0 | 40 | 0 | 0 | . 1 6357 | 39 | 3 | 202027 | 4 | 15 | 0 | 3 | 0 | 0 | 0 | 0 | 1977 | 50 | 0 | 1 | . 2 5418 | 21 | 0 | 191444 | 6 | 7 | 1 | 7 | 3 | 0 | 0 | 0 | 0 | 40 | 0 | 0 | . 3 1931 | 22 | 0 | 215546 | 0 | 12 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 55 | 0 | 0 | . 4 7742 | 61 | 0 | 28291 | 3 | 9 | 2 | 7 | 2 | 0 | 1 | 0 | 0 | 82 | 0 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; X_train = train_data.drop([&#39;index&#39;, &#39;target&#39;], axis=1) #training 데이터에서 독립변수 추출 y_train = train_data.target #training 데이터에서 라벨 추출 . X_train.shape . (7540, 14) . from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier() # 모델을 객체에 할당 model.fit(X_train, y_train) # 모델 학습 #pred = model.predict(test) . RandomForestClassifier() . 검증셋으로 모델 성능 검증 . X_val = val_data.drop([&#39;index&#39;, &#39;target&#39;], axis=1) #validation 데이터에서 전처리된 문서 추출 y_val = val_data.target #validation 데이터에서 라벨 추출 . y_pred = model.predict(X_val) print(y_pred) . [0 0 0 ... 1 1 0] . from sklearn import metrics print(&#39;RandomForestClassifier 의 예측 정확도는&#39;, round(metrics.accuracy_score(y_val, y_pred),3)) # 정확도 확인 . RandomForestClassifier 의 예측 정확도는 0.845 . &#47004;&#45924; &#54252;&#47112;&#49828;&#53944; &#54616;&#51060;&#54140; &#54028;&#46972;&#48120;&#53552; &#53916;&#45789; . from sklearn.ensemble import RandomForestClassifier model_md1 = RandomForestClassifier(max_depth = 1) # 모델을 객체에 할당 model_md10 = RandomForestClassifier(max_depth = 10) model_md1.fit(X_train, y_train) # 모델 학습 model_md10.fit(X_train, y_train) pred_md1 = model_md1.predict(X_val) pred_md10 = model_md10.predict(X_val) print(&#39;RF max_depth = 1 의 예측 정확도는&#39;, round(metrics.accuracy_score(y_val, pred_md1),3)) # 정확도 확인 print(&#39;RF max_depth = 10 의 예측 정확도는&#39;, round(metrics.accuracy_score(y_val, pred_md10),3)) # 정확도 확인 . RF max_depth = 1 의 예측 정확도는 0.747 RF max_depth = 10 의 예측 정확도는 0.852 . from sklearn.ensemble import RandomForestClassifier model_ne1 = RandomForestClassifier(n_estimators = 1) # 모델을 객체에 할당 model_ne200 = RandomForestClassifier(n_estimators = 200) model_ne1.fit(X_train, y_train) # 모델 학습 model_ne200.fit(X_train, y_train) pred_ne1 = model_ne1.predict(X_val) pred_ne200 = model_ne200.predict(X_val) print(&#39;RF n_estimators = 1 의 예측 정확도는&#39;, round(metrics.accuracy_score(y_val, pred_ne1),3)) # 정확도 확인 print(&#39;RF n_estimators = 200 의 예측 정확도는&#39;, round(metrics.accuracy_score(y_val, pred_ne200),3)) # 정확도 확인 . RF n_estimators = 1 의 예측 정확도는 0.796 RF n_estimators = 200 의 예측 정확도는 0.845 . from sklearn.ensemble import RandomForestClassifier model_mf1 = RandomForestClassifier(max_features = 1) # 모델을 객체에 할당 model_mf3 = RandomForestClassifier(max_features = 3) model_mf1.fit(X_train, y_train) # 모델 학습 model_mf3.fit(X_train, y_train) pred_mf1 = model_mf1.predict(X_val) pred_mf3 = model_mf3.predict(X_val) print(&#39;RF n_estimators = 1 의 예측 정확도는&#39;, round(metrics.accuracy_score(y_val, pred_mf1),3)) # 정확도 확인 print(&#39;RF n_estimators = 3 의 예측 정확도는&#39;, round(metrics.accuracy_score(y_val, pred_mf3),3)) # 정확도 확인 . RF n_estimators = 1 의 예측 정확도는 0.846 RF n_estimators = 3 의 예측 정확도는 0.845 . model_2 = RandomForestClassifier(max_depth = 10, n_estimators = 200, max_features = 3) # 최종 모델 model_2.fit(X_train, y_train) y_pred_2 = model_2.predict(X_val) print(y_pred_2) print(&#39;Tuned RandomForestClassifier 의 예측 정확도는&#39;, round(metrics.accuracy_score(y_val, y_pred_2),3)) # 정확도 확인 . [0 0 0 ... 1 1 0] Tuned RandomForestClassifier 의 예측 정확도는 0.85 . XGBoost . import xgboost as xgb # 모델 선언 model = xgb.XGBClassifier() # 모델 훈련 model.fit(X_train,y_train) # 모델 예측 y_pred_3 = model.predict(X_val) print(y_pred_3) print(&#39;XGBoost의 예측 정확도는&#39;, round(metrics.accuracy_score(y_val, y_pred_3),3)) # 정확도 확인 . [0 0 0 ... 1 1 0] XGBoost의 예측 정확도는 0.855 . xgb.plot_importance(model) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5074c890d0&gt; . : xgb.plot_importance() 메쏘드에 XGBoost 모형객체를 넣어 변수중요도를 파악할 수 있다. . 중요도가 높은 변수는 age : 나이와 capital.gain : 자본 이익인 것을 알 수 있다. . import matplotlib.pyplot as plt xgb.plot_tree(model, num_trees=0, rankdir=&#39;LR&#39;) fig = plt.gcf() fig.set_size_inches(150, 100) plt.show() . xgb.plot_importance(model)을 바탕으로 중요하지 않은 변수인 . workclass : 일 유형, sex : 성별, education : 교육수준, race : 인종, native.country : 본 국적 . 를 제외시키고 XGBoost을 사용하여 분석해볼 것입니다. . train_data.head() . index age workclass fnlwgt education education.num marital.status occupation relationship race sex capital.gain capital.loss hours.per.week native.country target . 0 8975 | 27 | 0 | 29904 | 2 | 13 | 1 | 10 | 2 | 0 | 1 | 0 | 0 | 40 | 0 | 0 | . 1 6357 | 39 | 3 | 202027 | 4 | 15 | 0 | 3 | 0 | 0 | 0 | 0 | 1977 | 50 | 0 | 1 | . 2 5418 | 21 | 0 | 191444 | 6 | 7 | 1 | 7 | 3 | 0 | 0 | 0 | 0 | 40 | 0 | 0 | . 3 1931 | 22 | 0 | 215546 | 0 | 12 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 55 | 0 | 0 | . 4 7742 | 61 | 0 | 28291 | 3 | 9 | 2 | 7 | 2 | 0 | 1 | 0 | 0 | 82 | 0 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; X_train_xg = train_data.drop([&#39;index&#39;, &#39;target&#39;,&#39;workclass&#39;, &#39;sex&#39;, &#39;education&#39;, &#39;race&#39;, &#39;native.country&#39;,&#39;relationship&#39;], axis=1) #training 데이터에서 독립변수 추출 y_train_xg = train_data.target #training 데이터에서 라벨 추출 . val_data.head() . index age workclass fnlwgt education education.num marital.status occupation relationship race sex capital.gain capital.loss hours.per.week native.country target . 0 9533 | 49 | 0 | 31807 | 1 | 10 | 1 | 1 | 3 | 0 | 1 | 0 | 0 | 40 | 0 | 0 | . 1 10785 | 48 | 0 | 149640 | 3 | 9 | 0 | 8 | 0 | 0 | 0 | 0 | 0 | 40 | 0 | 0 | . 2 9413 | 42 | 0 | 34278 | 1 | 10 | 0 | 6 | 0 | 0 | 0 | 0 | 0 | 50 | 0 | 1 | . 3 13561 | 30 | 0 | 206322 | 3 | 9 | 0 | 8 | 0 | 0 | 0 | 0 | 0 | 73 | 0 | 1 | . 4 11330 | 28 | 0 | 38309 | 10 | 11 | 0 | 2 | 0 | 0 | 0 | 2407 | 0 | 40 | 0 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; X_val_xg = val_data.drop([&#39;index&#39;, &#39;target&#39;,&#39;workclass&#39;, &#39;sex&#39;, &#39;education&#39;, &#39;race&#39;, &#39;native.country&#39;,&#39;relationship&#39;], axis=1) #validation 데이터에서 전처리된 문서 추출 y_val_xg = val_data.target #validation 데이터에서 라벨 추출 . import xgboost as xgb # 모델 선언 model = xgb.XGBClassifier() # 모델 훈련 model.fit(X_train_xg,y_train_xg) # 모델 예측 y_pred_4 = model.predict(X_val_xg) print(y_pred_4) print(&#39;XGBoost의 예측 정확도는&#39;, round(metrics.accuracy_score(y_val_xg, y_pred_4),3)) # 정확도 확인 . [0 0 0 ... 1 1 0] XGBoost의 예측 정확도는 0.854 . 위에서 했던 xgboost 보다 예측 정확도가 0.001가 낮아졌습니다. . 그러면, target과 다른 변수들의 상관관계를 살펴본 후, 변수를 제거하여 다시 xgboost로 모델을 훈련해 보겠습니다. . corr_matrix = train_data.corr() corr_matrix[&quot;target&quot;].sort_values(ascending=False) . target 1.000000 education.num 0.334992 age 0.256567 hours.per.week 0.241954 capital.gain 0.226078 workclass 0.148656 capital.loss 0.140511 fnlwgt 0.004013 native.country -0.005655 education -0.006617 index -0.011621 race -0.070404 occupation -0.119528 sex -0.214647 relationship -0.238034 marital.status -0.265266 Name: target, dtype: float64 . train data의 target과 다른 변수들의 상관관계를 살펴봤을 때, 1에 가까워 양의 관계를 가지고 있는 변수들은 . education.num 0.334992 age 0.256567 hours.per.week 0.241954 capital.gain 0.226078 . 이고, -1에 가까워 응의 관계를 가지고 있는 변수들은 . sex -0.214647 relationship -0.238034 marital.status -0.265266 . 입니다. 상관계수가 0에 가깝게 나오는 변수들을 target과의 선형관계가 없기때문에 제거하고 모델을 훈련해볼 것입니다. . X_train_cor = train_data.drop([&#39;index&#39;, &#39;target&#39;,&#39;workclass&#39;, &#39;capital.loss&#39;, &#39;education&#39;, &#39;race&#39;, &#39;native.country&#39;,&#39;fnlwgt&#39;,&#39;occupation&#39;], axis=1) #training 데이터에서 독립변수 추출 y_train_cor = train_data.target #training 데이터에서 라벨 추출 . X_val_cor = val_data.drop([&#39;index&#39;, &#39;target&#39;,&#39;workclass&#39;, &#39;capital.loss&#39;, &#39;education&#39;, &#39;race&#39;, &#39;native.country&#39;,&#39;fnlwgt&#39;,&#39;occupation&#39;], axis=1) #validation 데이터에서 전처리된 문서 추출 y_val_cor = val_data.target #validation 데이터에서 라벨 추출 . import xgboost as xgb # 모델 선언 model = xgb.XGBClassifier() # 모델 훈련 model.fit(X_train_cor,y_train_cor) # 모델 예측 y_pred_5 = model.predict(X_val_cor) print(y_pred_5) print(&#39;XGBoost의 예측 정확도는&#39;, round(metrics.accuracy_score(y_val_cor, y_pred_5),3)) # 정확도 확인 . [0 0 1 ... 1 1 0] XGBoost의 예측 정확도는 0.846 . 위에서 했던 xgboost 보다 예측 정확도가 0.008가 낮아졌습니다. . xgboost로 모델을 훈련시키기에는 제일 첫번째에 했던 train_data를 사용해서 하는 것이 좋다고 판단했습니다. . LightGBM . test data . 변수 및 모델정리 | . test = pd.read_csv(&#39;/content/drive/MyDrive/Colab/data/test.csv&#39;) test.head() . id age workclass fnlwgt education education.num marital.status occupation relationship race sex capital.gain capital.loss hours.per.week native.country . 0 0 | 47 | Private | 304133 | Some-college | 10 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 45 | United-States | . 1 1 | 34 | Self-emp-inc | 154227 | Some-college | 10 | Never-married | Sales | Not-in-family | White | Male | 0 | 0 | 75 | United-States | . 2 2 | 31 | Local-gov | 158291 | Bachelors | 13 | Never-married | Craft-repair | Not-in-family | White | Male | 8614 | 0 | 40 | United-States | . 3 3 | 28 | Private | 183155 | Bachelors | 13 | Married-civ-spouse | Prof-specialty | Husband | White | Male | 0 | 0 | 55 | United-States | . 4 4 | 54 | Local-gov | 182543 | Some-college | 10 | Widowed | Adm-clerical | Unmarried | White | Female | 0 | 0 | 40 | Mexico | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; test = label_encoder(test, make_label_map(test)) test = test.drop([&#39;id&#39;],axis=1) test.head() . age workclass fnlwgt education education.num marital.status occupation relationship race sex capital.gain capital.loss hours.per.week native.country . 0 47 | 0 | 304133 | 0 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 0 | . 1 34 | 1 | 154227 | 0 | 10 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 75 | 0 | . 2 31 | 2 | 158291 | 1 | 13 | 1 | 2 | 1 | 0 | 0 | 8614 | 0 | 40 | 0 | . 3 28 | 0 | 183155 | 1 | 13 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 55 | 0 | . 4 54 | 2 | 182543 | 0 | 10 | 2 | 4 | 2 | 0 | 1 | 0 | 0 | 40 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; prediction = model.predict(test) prediction . array([1, 0, 1, ..., 0, 0, 0]) . 랜덤 포레스트 하이퍼 파라미터 튜닝 | . test = pd.read_csv(&#39;/content/drive/MyDrive/Colab/data/test.csv&#39;) test = label_encoder(test, make_label_map(test)) test = test.drop(&#39;id&#39;, axis=1) test.head() . age workclass fnlwgt education education.num marital.status occupation relationship race sex capital.gain capital.loss hours.per.week native.country . 0 47 | 0 | 304133 | 0 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 0 | . 1 34 | 1 | 154227 | 0 | 10 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 75 | 0 | . 2 31 | 2 | 158291 | 1 | 13 | 1 | 2 | 1 | 0 | 0 | 8614 | 0 | 40 | 0 | . 3 28 | 0 | 183155 | 1 | 13 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 55 | 0 | . 4 54 | 2 | 182543 | 0 | 10 | 2 | 4 | 2 | 0 | 1 | 0 | 0 | 40 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; X_train= data.drop([&#39;target&#39;], axis=1) #전체 training 데이터에서 독립변수 추출 y_train = data.target #전체 training 데이터에서 라벨 추출 . model = RandomForestClassifier(max_depth = 10, n_estimators = 200, max_features = 3) #앞서 튜닝한 하이퍼 파라미터 max_depth, n_estimators, max_features model.fit(X_train, y_train) y_pred = model.predict(test) . import xgboost as xgb model = xgb.XGBClassifier() model.fit(X_train,y_train) y_pred = model.predict(test) . Dacon &#45824;&#54924;&#50640; &#51228;&#52636;&#54616;&#44592; (Submission) . submission = pd.read_csv(&#39;/content/drive/MyDrive/Colab/data/sample_submission.csv&#39;) submission.head() . id target . 0 0 | 0 | . 1 1 | 0 | . 2 2 | 0 | . 3 3 | 0 | . 4 4 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; submission[&#39;target&#39;] = prediction # 데이터가 잘 들어갔는지 확인합니다 submission . id target . 0 0 | 1 | . 1 1 | 0 | . 2 2 | 1 | . 3 3 | 1 | . 4 4 | 0 | . ... ... | ... | . 15076 15076 | 1 | . 15077 15077 | 1 | . 15078 15078 | 0 | . 15079 15079 | 0 | . 15080 15080 | 0 | . 15081 rows × 2 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; submission.to_csv(&#39;submit.csv&#39;, index=False) .",
            "url": "https://woojaaeon.github.io/jaong/jupyter/python/2022/06/21/population-input.html",
            "relUrl": "/jupyter/python/2022/06/21/population-input.html",
            "date": " • Jun 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Datamining Final Project",
            "content": "목차 . 서론 . 배경 | 목적 | 분석주제 문제제기 | 원인분석 | 주제선정 | . | 분석계획 | . | 데이터 전제 . 데이터설명 | 데이터 전처리 | . | 분석결과 . Q1. 어느 지역이 사람들의 수요가 많을까? A1. 주로 경기와 서울에 인구가 많이 몰려 수요도 많아지고 있다. | . | Q2. 수요가 많은 지역의 주택은 얼마나 있을까? A2. 수요가 많은 지역임에도 불구하고 주택 수는 부족하다. | . | Q3. 집값은 어떻게 변화할까? A3. 가구별로 매년마다 기준 중위소득이 증가하고 있어도 아파트 가격 또한, 더 높은 상승세를 가지고 있어 내 집 마련이 힘들어 질 것이다. | . | . | 결론 . 요약 | 한계점 | . | . &#49436;&#47200; . &#48176;&#44221; . 한국 부동산 데이터 시각화 경진대회 (Link) . 자산은 다양한 형태로 존재합니다. 현금이 될 수도 있고, 주식이 될 수도 있습니다. . 그 중에서도 “부동산”이라는 자산은 우리의 경제와 정치에 많은 영향을 주었습니다. 한국의 부동산 변화를 연구하여 사회 전반에 미치는 영향을 파악한다면 다가오는 미래에 관한 통찰을 얻을 수 있을 것입니다. 주어진 데이터와 외부 데이터를 활용하여 유의미한 인사이트를 발굴해봅시다. . &#47785;&#51201; . 한국의 부동산 데이터와 사용자가 직접 수집한 외부 데이터를 활용하여 부동산 가격 변화 및 사회와의 관련성을 분석하는 것입니다. . 한국 부동산 움직임이 쉽고 명확하게 보이도록 시각화 . | 부동산 데이터로부터 사회 현상을 설명하는 인사이트 발굴 . | 외부 데이터를 적극 활용한 인사이트 발굴 . | Python 및 R의 시각화 툴을 활용한 데이터 분석 학습 . | &#48516;&#49437;&#51452;&#51228; . &#47928;&#51228;&#51228;&#44592; . 주택가격 상승이 멈추지 않고 있으며, KB국민은행 자료에 따르면 전국 아파트 중위가격은 올해 7월 기준 5억76만원으로 1년 만에 1억2,794만원 올랐다고 합니다. . 주택 가격이 상승하면 내 집마련의 목표를 가진 20~30대 청년들은 어떻게 집을 마련할 수 있을까요? . &#50896;&#51064;&#48516;&#49437; . 집값이 상승하는 원인에는 크게 4가지로 나눌 수 있습니다. (출처 : 한국아파트신문) . 저금리와 풍부한 유동성 : 저금리와 풍부한 유동성은 주택시장의 순환을 용이하게 하고 변동성을 키우는 원인입니다. . | 수요에 비한 공급 부족 : 수요에 비해 공급이 부족하거나 원활하지 못합니다. 주택공급이 지속적으로 확충되지 못함으로써 주택시장 안전 기반이 조성되지 못하고 있습니다. . | 한국인의 아파트 선호와 수도권 인구집중 : 한국인의 아파트 선호와 수도권 인구집중이 또 하나의 원인으로 지목되었습니다. 수도권은 지속적으로 인구 및 세대수가 증가 추세며 주택 유형 중 아파트를 가장 선호해 아파트 수요가 증가하고 있습니다. . | 사회 심리적 요인이 작용 : 사회 심리적 요인이 작용하고 있습니다. 주택가격은 지속적으로 상승할 것이라는 기대심리가 팽배해지고 있습니다. 주택가격이 급등하는 시기에는 실수요자보다 투기를 목적으로 하는 불법, 탈법적인 가수요가 주택가격 상승을 부추깁니다. . | . &#51452;&#51228;&#49440;&#51221; . 집값이 어떻게 변화되는지 시도별 주택 및 아파트 값은 어떻게 변화하는지 알아볼려고 합니다. . 전년도 데이터를 분석하여 이전의 집값이 어떻게 변화되었는 지 살펴보고, 내집마련을 할 수 있을 지 알아봅시다. . &#48516;&#49437;&#44228;&#54925; . 4가지의 집값 상승 원인 중 수요에 비한 공급 부족 과 한국인의 아파트 선호와 수도권 인구집중 두 가지의 원인을 가지고 시도별 주택 및 아파트 값이 어떻게 변화하는 지 알아보겠습니다. . 또한 아래의 Question에 따라서 사용할 데이터를 나눠서 분석을 진행할 것입니다. . Q1. 어느 지역이 사람들의 수요가 많을까? | . 지역별로 사람들의 순유입과 순유출은 어떻게 되는지 | 순유입과 순유출이 큰 지역에 가는 연령은 어떻게 되는지 | 두 지역간의 차이 :순유입과 순유출의 이유 | Q2. 수요가 많은 지역의 주택은 얼마나 있을까? | . 건설 인허가 현황 | 두 지역간의 미분양 주택은 얼마나 있는지, 얼마나 차이가 나는지 | Q3. 집값은 어떻게 변화할까? | . 지난 몇년간 집값은 어떻게 변하고 있는지 | 소득과 비교해 봤을 때, 내집마련을 할 수 있을지 | &#45936;&#51060;&#53552; &#51204;&#51228; . &#45936;&#51060;&#53552; &#49444;&#47749; . 제공된 데이터셋 (출처 : 한국은행 경제통계시스템 - OpenAPI 서비스) . house_j = 주택전세가격지수(KB).csv . : 2019년 1월 가격지수 100을 기준으로 한 1986년 1월부터 2021년 3월까지 주택유형별 전세 가격지수 : 주택유형으로 아파트, 연립주택, 단독주택이 있으며, 이들을 종합한 &#39;총지수&#39;유형이 존재 : 전국 데이터와 서울 데이터 나누어져 있으며, 서울의 경우 총지수와 아파트 데이터만 존재 . | house_m = 주택매매가격지수(KB).csv . : 2019년 1월 가격지수 100 기준으로 한 1986년 1월부터 2021년 3월까지 주택유형별 매매 가격지수 : 주택유형으로 아파트, 연립주택, 단독주택이 있으며, 이들을 종합한 &#39;총지수&#39;유형이 존재 :전국 데이터와 서울 데이터 나누어져 있으며, 서울의 경우 총지수와 아파트 데이터만 존재 . | monthly = 유형별_주택월세통합가격지수.csv . : 2017년 11월의 가격지수 100을 기준으로, 2015년 6월부터 2021년 3월까지 월별로 상대적인 지역별 주택 월세 통합 가격 지수 : 주택유형에는 아파트, 연립다세대, 단독주택이 있으며, 이들을 종합한 &#39;종합&#39;유형까지 데이터에는 총 4가지 유형이 존재 . | non_house = 미분양주택현황.csv . : 2007년 1월부터 2021년 2월까지의 지역별 미분양주택 호수 . | apt_price = 아파트_실거래가격지수.csv . : 2006년 1월부터 2021년 1월까지, 가격기준점인 2017년 11월 100을 기준으로한 지역별 아파트 실거래 가격 지수 . | build_h = 주택건설인허가실적.csv . : 2007년 1월부터 2021년 2월까지 매달 1일씩 지역별 주택 건설 인허가 호수 . | . import pandas as pd import numpy as np ### 시각화 import matplotlib import matplotlib.pyplot as plt import seaborn as sns plt.rcParams[&#39;font.family&#39;] = &#39;Malgun Gothic&#39; matplotlib.rcParams[&#39;axes.unicode_minus&#39;] = False plt.style.use (&#39;ggplot&#39;) . house_m = pd.read_csv(&quot;data/주택매매가격지수(KB).csv&quot;) # 1986.01~2021.03 house_j = pd.read_csv(&quot;data/주택전세가격지수(KB).csv&quot;) # 1986.01~2021.03 monthly = pd.read_csv(&quot;data/유형별_주택월세통합가격지수.csv&quot;) # 2015.06~2021.03 non_house = pd.read_csv(&quot;data/미분양주택현황.csv&quot;) # 2007.01~2021.02 apt_price = pd.read_csv(&quot;data/아파트_실거래가격지수.csv&quot;) # 2006.01.~2021.01 build_h = pd.read_csv(&quot;data/주택건설인허가실적.csv&quot;) # 2007.01~2021.02 (매달 1일씩) df_list = [house_m,house_j,monthly,non_house,apt_price,build_h] . df_list_title = [&#39;house_m&#39;,&#39;house_j&#39;,&#39;monthly&#39;,&#39;non_house&#39;,&#39;apt_price&#39;,&#39;build_h&#39;] for i in range(len(df_list)): print(&quot;=&quot;*10, df_list_title[i],&quot;=&quot;*10) print(df_list[i].columns) print() . ========== house_m ========== Index([&#39;Unnamed: 0&#39;, &#39;총지수[2019.01=100]&#39;, &#39;단독주택[2019.01=100]&#39;, &#39;연립주택[2019.01=100]&#39;, &#39;아파트[2019.01=100]&#39;, &#39;아파트(서울)[2019.01=100]&#39;, &#39;총지수(서울)[2019.01=100]&#39;], dtype=&#39;object&#39;) ========== house_j ========== Index([&#39;Unnamed: 0&#39;, &#39;총지수[2019.01=100]&#39;, &#39;단독주택[2019.01=100]&#39;, &#39;연립주택[2019.01=100]&#39;, &#39;아파트[2019.01=100]&#39;, &#39;아파트(서울)[2019.01=100]&#39;, &#39;총지수(서울)[2019.01=100]&#39;], dtype=&#39;object&#39;) ========== monthly ========== Index([&#39;Unnamed: 0&#39;, &#39;종합-전국[2017.11=100]&#39;, &#39;종합-수도권[2017.11=100]&#39;, &#39;종합-지방[2017.11=100]&#39;, &#39;종합-5대광역시[2017.11=100]&#39;, &#39;종합-8개도[2017.11=100]&#39;, &#39;종합-서울[2017.11=100]&#39;, &#39;종합-경기[2017.11=100]&#39;, &#39;종합-인천[2017.11=100]&#39;, &#39;종합-부산[2017.11=100]&#39;, &#39;종합-대구[2017.11=100]&#39;, &#39;종합-광주[2017.11=100]&#39;, &#39;종합-대전[2017.11=100]&#39;, &#39;종합-울산[2017.11=100]&#39;, &#39;종합-세종[2017.11=100]&#39;, &#39;종합-강원[2017.11=100]&#39;, &#39;종합-충북[2017.11=100]&#39;, &#39;종합-충남[2017.11=100]&#39;, &#39;종합-전북[2017.11=100]&#39;, &#39;종합-전남[2017.11=100]&#39;, &#39;종합-경북[2017.11=100]&#39;, &#39;종합-경남[2017.11=100]&#39;, &#39;종합-제주[2017.11=100]&#39;, &#39;종합-6대광역시[2017.11=100]&#39;, &#39;종합-9개도[2017.11=100]&#39;, &#39;아파트-전국[2017.11=100]&#39;, &#39;아파트-수도권[2017.11=100]&#39;, &#39;아파트-지방[2017.11=100]&#39;, &#39;아파트-5대광역시[2017.11=100]&#39;, &#39;아파트-8개도[2017.11=100]&#39;, &#39;아파트-서울[2017.11=100]&#39;, &#39;아파트-경기[2017.11=100]&#39;, &#39;아파트-인천[2017.11=100]&#39;, &#39;아파트-부산[2017.11=100]&#39;, &#39;아파트-대구[2017.11=100]&#39;, &#39;아파트-광주[2017.11=100]&#39;, &#39;아파트-대전[2017.11=100]&#39;, &#39;아파트-울산[2017.11=100]&#39;, &#39;아파트-세종[2017.11=100]&#39;, &#39;아파트-강원[2017.11=100]&#39;, &#39;아파트-충북[2017.11=100]&#39;, &#39;아파트-충남[2017.11=100]&#39;, &#39;아파트-전북[2017.11=100]&#39;, &#39;아파트-전남[2017.11=100]&#39;, &#39;아파트-경북[2017.11=100]&#39;, &#39;아파트-경남[2017.11=100]&#39;, &#39;아파트-제주[2017.11=100]&#39;, &#39;아파트-6대광역시[2017.11=100]&#39;, &#39;아파트-9개도[2017.11=100]&#39;, &#39;연립다세대-전국[2017.11=100]&#39;, &#39;연립다세대-수도권[2017.11=100]&#39;, &#39;연립다세대-지방[2017.11=100]&#39;, &#39;연립다세대-5대광역시[2017.11=100]&#39;, &#39;연립다세대-8개도[2017.11=100]&#39;, &#39;연립다세대-서울[2017.11=100]&#39;, &#39;연립다세대-경기[2017.11=100]&#39;, &#39;연립다세대-인천[2017.11=100]&#39;, &#39;연립다세대-부산[2017.11=100]&#39;, &#39;연립다세대-대구[2017.11=100]&#39;, &#39;연립다세대-광주[2017.11=100]&#39;, &#39;연립다세대-대전[2017.11=100]&#39;, &#39;연립다세대-울산[2017.11=100]&#39;, &#39;연립다세대-세종[2017.11=100]&#39;, &#39;연립다세대-강원[2017.11=100]&#39;, &#39;연립다세대-충북[2017.11=100]&#39;, &#39;연립다세대-충남[2017.11=100]&#39;, &#39;연립다세대-전북[2017.11=100]&#39;, &#39;연립다세대-전남[2017.11=100]&#39;, &#39;연립다세대-경북[2017.11=100]&#39;, &#39;연립다세대-경남[2017.11=100]&#39;, &#39;연립다세대-제주[2017.11=100]&#39;, &#39;연립다세대-6대광역시[2017.11=100]&#39;, &#39;연립다세대-9개도[2017.11=100]&#39;, &#39;단독주택-전국[2017.11=100]&#39;, &#39;단독주택-수도권[2017.11=100]&#39;, &#39;단독주택-지방[2017.11=100]&#39;, &#39;단독주택-5대광역시[2017.11=100]&#39;, &#39;단독주택-8개도[2017.11=100]&#39;, &#39;단독주택-서울[2017.11=100]&#39;, &#39;단독주택-경기[2017.11=100]&#39;, &#39;단독주택-인천[2017.11=100]&#39;, &#39;단독주택-부산[2017.11=100]&#39;, &#39;단독주택-대구[2017.11=100]&#39;, &#39;단독주택-광주[2017.11=100]&#39;, &#39;단독주택-대전[2017.11=100]&#39;, &#39;단독주택-울산[2017.11=100]&#39;, &#39;단독주택-세종[2017.11=100]&#39;, &#39;단독주택-강원[2017.11=100]&#39;, &#39;단독주택-충북[2017.11=100]&#39;, &#39;단독주택-충남[2017.11=100]&#39;, &#39;단독주택-전북[2017.11=100]&#39;, &#39;단독주택-전남[2017.11=100]&#39;, &#39;단독주택-경북[2017.11=100]&#39;, &#39;단독주택-경남[2017.11=100]&#39;, &#39;단독주택-제주[2017.11=100]&#39;, &#39;단독주택-6대광역시[2017.11=100]&#39;, &#39;단독주택-9개도[2017.11=100]&#39;], dtype=&#39;object&#39;) ========== non_house ========== Index([&#39;Unnamed: 0&#39;, &#39;전국[호]&#39;, &#39;서울[호]&#39;, &#39;부산[호]&#39;, &#39;대구[호]&#39;, &#39;인천[호]&#39;, &#39;광주[호]&#39;, &#39;대전[호]&#39;, &#39;울산[호]&#39;, &#39;경기[호]&#39;, &#39;강원[호]&#39;, &#39;충북[호]&#39;, &#39;충남[호]&#39;, &#39;전북[호]&#39;, &#39;전남[호]&#39;, &#39;경북[호]&#39;, &#39;경남[호]&#39;, &#39;제주[호]&#39;, &#39;수도권[호]&#39;, &#39;세종[호]&#39;], dtype=&#39;object&#39;) ========== apt_price ========== Index([&#39;Unnamed: 0&#39;, &#39;도심권[2017.11=100]&#39;, &#39;동북권[2017.11=100]&#39;, &#39;동남권[2017.11=100]&#39;, &#39;부산[2017.11=100]&#39;, &#39;대구[2017.11=100]&#39;, &#39;인천[2017.11=100]&#39;, &#39;광주[2017.11=100]&#39;, &#39;대전[2017.11=100]&#39;, &#39;울산[2017.11=100]&#39;, &#39;세종[2017.11=100]&#39;, &#39;경기[2017.11=100]&#39;, &#39;강원[2017.11=100]&#39;, &#39;충북[2017.11=100]&#39;, &#39;충남[2017.11=100]&#39;, &#39;서북권[2017.11=100]&#39;, &#39;서남권[2017.11=100]&#39;, &#39;전북[2017.11=100]&#39;, &#39;전남[2017.11=100]&#39;, &#39;경북[2017.11=100]&#39;, &#39;경남[2017.11=100]&#39;, &#39;제주[2017.11=100]&#39;, &#39;광역시[2017.11=100]&#39;, &#39;지방광역시[2017.11=100]&#39;, &#39;지방도[2017.11=100]&#39;, &#39;전국[2017.11=100]&#39;, &#39;서울[2017.11=100]&#39;, &#39;수도권[2017.11=100]&#39;, &#39;지방[2017.11=100]&#39;], dtype=&#39;object&#39;) ========== build_h ========== Index([&#39;Unnamed: 0&#39;, &#39;전국[호]&#39;, &#39;부산[호]&#39;, &#39;충북[호]&#39;, &#39;충남[호]&#39;, &#39;대구[호]&#39;, &#39;대전[호]&#39;, &#39;강원[호]&#39;, &#39;광주[호]&#39;, &#39;경기[호]&#39;, &#39;인천[호]&#39;, &#39;제주[호]&#39;, &#39;전북[호]&#39;, &#39;전남[호]&#39;, &#39;경북[호]&#39;, &#39;경남[호]&#39;, &#39;민간부문[호]&#39;, &#39;공공부문[호]&#39;, &#39;부문별[호]&#39;, &#39;세종[호]&#39;, &#39;서울[호]&#39;, &#39;울산[호]&#39;], dtype=&#39;object&#39;) . # &#39;house_m&#39;,&#39;house_j&#39;,&#39;monthly&#39;,&#39;non_house&#39;,&#39;apt_price&#39;,&#39;build_h&#39; 순서대로 fig, axes = plt.subplots(6, 1, figsize=(20, 10)) sns.lineplot(data = df_list[0], ax=axes[0],legend=False) sns.lineplot(data = df_list[1], ax=axes[1],legend=False) sns.lineplot(data = df_list[2], ax=axes[2],legend=False) sns.lineplot(data = df_list[3], ax=axes[3],legend=False) sns.lineplot(data = df_list[4], ax=axes[4],legend=False) sns.lineplot(data = df_list[5], ax=axes[5],legend=False) . &lt;AxesSubplot:&gt; . 추가 데이터셋 . 출처 : KOSIS 국가통계포털 . pop_sido = 전출지전입지_시도별_순이동자수.csv . : 2001년 1월부터 2022년 1월까지의 시도별 전출, 전입에 대한 인구 순이동 데이터 : 단위는 명으로 구성 . | pop_age = 시군구_연령(5세)별_이동자수.csv . : 2001년 1월부터 2022년 1월까지의 시군구 및 연령별 인구이동 데이터 : 연령은 5세 단위로 나눠져 있고 단위는 명으로 구성 . | . 출처 : e-나라지표 . income = 연도별_중위소득.xls . : 2015년부터 2022년까지 가구별, 연도별 기준 중위 소득 : 기준 중위소득이란,「국민기초생활보장법」제20조의 제2항에 따라 중앙생활보장위원회 심의·의결을 거쳐 고시하는 국민 가구소득의 중위값을 의미 : 가구 형태는 1인가구, 2인가구, 3인가구, 4인가구, 5인가구, 6인가구 존재 . | . pop_sido = pd.read_csv(&quot;data/전출지전입지_시도별_순이동자수.csv&quot;,encoding=&#39;cp949&#39;,header=0) # 2001.01~2022.01 / 단위 : 명 pop_age = pd.read_csv(&quot;data/시군구_연령(5세)별_이동자수.csv&quot;,encoding=&#39;cp949&#39;,header=0,index_col=1) # 2001.01~2022.01 / 단위 : 명 income = pd.read_excel(&quot;data/연도별_중위소득.xls&quot;,index_col=0) # 2015~2022 / 단위 : 원 . print(&quot;=&quot;*10, &quot;pop_sido&quot;,&quot;=&quot;*10) print(&quot;행 :&quot;, len(pop_sido), &quot;/ 열 :&quot;,len(pop_sido.columns)) print(pop_sido.columns) print(&quot;=&quot;*10, &quot;pop_age&quot;,&quot;=&quot;*10) print(&quot;행 :&quot;, len(pop_age), &quot;/ 열 :&quot;,len(pop_age.columns)) print(pop_age.columns) print(&quot;=&quot;*10, &quot;income&quot;,&quot;=&quot;*10) print(&quot;행 :&quot;, len(income), &quot;/ 열 :&quot;,len(income.columns)) print(income.columns) . ========== pop_sido ========== 행 : 72390 / 열 : 4 Index([&#39;전출지별&#39;, &#39;전입지별&#39;, &#39;시점&#39;, &#39;순이동자수[명]&#39;], dtype=&#39;object&#39;) ========== pop_age ========== 행 : 2592 / 열 : 24 Index([&#39;행정구역(시군구)별&#39;, &#39;연령별&#39;, &#39;2001.01 월&#39;, &#39;2002.01 월&#39;, &#39;2003.01 월&#39;, &#39;2004.01 월&#39;, &#39;2005.01 월&#39;, &#39;2006.01 월&#39;, &#39;2007.01 월&#39;, &#39;2008.01 월&#39;, &#39;2009.01 월&#39;, &#39;2010.01 월&#39;, &#39;2011.01 월&#39;, &#39;2012.01 월&#39;, &#39;2013.01 월&#39;, &#39;2014.01 월&#39;, &#39;2015.01 월&#39;, &#39;2016.01 월&#39;, &#39;2017.01 월&#39;, &#39;2018.01 월&#39;, &#39;2019.01 월&#39;, &#39;2020.01 월&#39;, &#39;2021.01 월&#39;, &#39;2022.01 월&#39;], dtype=&#39;object&#39;) ========== income ========== 행 : 6 / 열 : 8 Index([&#39;2015&#39;, &#39;2016&#39;, &#39;2017&#39;, &#39;2018&#39;, &#39;2019&#39;, &#39;2020&#39;, &#39;2021&#39;, &#39;2022&#39;], dtype=&#39;object&#39;) . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 제공된 데이터 가공 | 날짜별로 index를 만들기 위해 제공된 데이터셋의 날짜를 datetime 형식으로 바꾼 뒤 index로 설정합니다. . def date(df): df[&#39;Unnamed: 0&#39;] = pd.to_datetime(df[&#39;Unnamed: 0&#39;],format=&#39;%Y-%m-%d&#39;) df.rename(columns = {&#39;Unnamed: 0&#39;:&#39;날짜&#39;}, inplace = True) df.set_index(&#39;날짜&#39;, inplace = True ) return df for i in df_list: date(i) . Q1 에서 사용할 데이터 가공 | 전국에서 사람들이 어느 지역으로 가장 많이 이동하는 확인하기 위해 전출지전입지_시도별_순이동자수.csv의 전출지별을 전국만 있는 것만 추출한 데이터를 사용합니다. . pop_sido . 전출지별 전입지별 시점 순이동자수[명] . 0 전국 | 서울특별시 | 2001.01 월 | 408 | . 1 전국 | 서울특별시 | 2001.02 월 | 4871 | . 2 전국 | 서울특별시 | 2001.03 월 | -8270 | . 3 전국 | 서울특별시 | 2001.04 월 | -12831 | . 4 전국 | 서울특별시 | 2001.05 월 | -15773 | . ... ... | ... | ... | ... | . 72385 제주특별자치도 | 경상남도 | 2021.09 월 | -29 | . 72386 제주특별자치도 | 경상남도 | 2021.10 월 | 2 | . 72387 제주특별자치도 | 경상남도 | 2021.11 월 | -70 | . 72388 제주특별자치도 | 경상남도 | 2021.12 월 | 16 | . 72389 제주특별자치도 | 경상남도 | 2022.01 월 | -21 | . 72390 rows × 4 columns . pop_nationwide = pop_sido[pop_sido[&#39;전출지별&#39;] == &#39;전국&#39;] # 2022년 데이터는 1월달 밖에 없기때문에 제거(4166번째 행이 2022.01 월 데이터이다.) pop_nationwide = pop_nationwide.drop(4166) # 각 지역별 순이동자수 pop_nationwide[&quot;시점&quot;] = pop_nationwide[&quot;시점&quot;].str[:4] pop_nationwide_group = pop_nationwide.groupby([&#39;시점&#39;,&#39;전입지별&#39;]).mean() pop_nationwide_rst = pop_nationwide_group.unstack() pop_nationwide_rst = pop_nationwide_rst.drop(&#39;2022&#39;,axis = 0) pop_nationwide_rst = pop_nationwide_rst.droplevel(level=0, axis=1) # 멀티인덱스 제거 pop_nationwide_rst.head(5) #&gt; multi index remove 참고 링크 : https://datascientyst.com/pandas-drop-multiindex-level/ . 전입지별 강원도 경기도 경상남도 경상북도 광주광역시 대구광역시 대전광역시 부산광역시 서울특별시 세종특별자치시 울산광역시 인천광역시 전라남도 전라북도 제주특별자치도 충청남도 충청북도 . 시점 . 2001 -676.083333 | 20745.583333 | -430.500000 | -1775.416667 | -10.083333 | -1186.083333 | 570.000000 | -3432.333333 | -9495.750000 | NaN | 499.000000 | 93.083333 | -3035.333333 | -159.250000 | -29.636364 | -969.000000 | -710.666667 | . 2002 -1431.750000 | 26315.166667 | 574.666667 | -2606.500000 | 362.916667 | -803.416667 | 712.916667 | -4120.166667 | -8868.416667 | NaN | 218.750000 | 19.166667 | -3846.000000 | -4727.916667 | 19.500000 | -1135.416667 | -683.500000 | . 2003 -1088.583333 | 18503.583333 | 357.333333 | -3069.416667 | -807.250000 | -254.666667 | 485.500000 | -3521.916667 | -5747.333333 | NaN | 156.000000 | -1347.583333 | -3008.583333 | -253.250000 | -120.583333 | 223.500000 | -506.750000 | . 2004 -695.166667 | 15739.583333 | -467.583333 | -2243.416667 | -262.333333 | -1110.583333 | 218.000000 | -2769.083333 | -3933.666667 | NaN | 147.000000 | -111.250000 | -2687.666667 | -4247.416667 | -6.916667 | 2915.250000 | -484.750000 | . 2005 -956.083333 | 14070.916667 | 468.250000 | -959.750000 | -611.083333 | -2079.166667 | 267.500000 | -3081.250000 | -4250.583333 | NaN | -32.666667 | 913.750000 | -1667.250000 | -2039.750000 | -67.083333 | 416.583333 | -392.333333 | . Q2에서 사용할 데이터 가공 | 우리나라는 다른 주택형식보다 아파트를 선호하는 경향이 있으며 그 이유에는 대표적으로 사고 팔 때 수익이 나는 시세차익, 주차장과 생활환경등의 편의성 측면에서 아파트의 선호 사상이 크다고 생각됩니다. ( 참고 ) 이를 반영해, 주택형식은 아파트에 초점을 맞춰서 분석을 할 것 입니다. . fig = plt.figure(figsize = (20, 7)) sns.lineplot(data = build_h) plt.show() . 건설 인허가 추세를 확인했을 때, 연말에 집중되어 있는 것으로 보입니다. . 따라서 각 연도별의 추세를 확인하기 위하여, 12월 자료만으로 데이터프레임을 만들어 분석을 진행합니다. . mon_12 = [] mon_12 = pd.DataFrame(mon_12) for i in range(2007,2021,1): a= build_h.loc[str(i)+&#39;-12&#39;] mon_12 = pd.concat([mon_12,a], axis =0) mon_12[[&#39;전국[호]&#39;,&#39;서울[호]&#39;,&#39;경기[호]&#39;]].head() . 전국[호] 서울[호] 경기[호] . 날짜 . 2007-12-01 555792.0 | 62842.0 | 198138.0 | . 2008-12-01 371285.0 | 48417.0 | 115531.0 | . 2009-12-01 381787.0 | 36090.0 | 159549.0 | . 2010-12-01 386542.0 | 69190.0 | 143551.0 | . 2011-12-01 549594.0 | 88060.0 | 148191.0 | . Q3에서 사용할 데이터 가공 | 아파트_실거래가격지수.csv 는 가격기준점인 2017년 11월은 가격 지수가 100.0으로 나타나 이것을 기준으로, 2017년 11월 이후 가격이 하락한 경우와 상승한 가격 두 지역으로 나눠 사용합니다. . apt_price.iloc[142].head(5) . 도심권[2017.11=100] 100.0 동북권[2017.11=100] 100.0 동남권[2017.11=100] 100.0 부산[2017.11=100] 100.0 대구[2017.11=100] 100.0 Name: 2017-11-01 00:00:00, dtype: float64 . apt_price.loc[&#39;2017&#39;][apt_price&lt;100] asc_list = [] #ascending_list for i in range(len(apt_price.columns)): if (apt_price.loc[&#39;2017-12&#39;].iloc[0,i] &gt;= 100) : asc_list.append(apt_price.columns[i]) print(&#39;가격 상승지역&#39;,&#39; n&#39;,asc_list) print() dec_list = [] #decreasing_list for i in range(len(apt_price.columns)): if (apt_price.loc[&#39;2017-12&#39;].iloc[0,i] &lt; 100) : dec_list.append(apt_price.columns[i]) print(&#39;가격 하락지역&#39;,&#39; n&#39;,dec_list) . 가격 상승지역 [&#39;도심권[2017.11=100]&#39;, &#39;동북권[2017.11=100]&#39;, &#39;동남권[2017.11=100]&#39;, &#39;광주[2017.11=100]&#39;, &#39;대전[2017.11=100]&#39;, &#39;서북권[2017.11=100]&#39;, &#39;서남권[2017.11=100]&#39;, &#39;전북[2017.11=100]&#39;, &#39;서울[2017.11=100]&#39;, &#39;수도권[2017.11=100]&#39;] 가격 하락지역 [&#39;부산[2017.11=100]&#39;, &#39;대구[2017.11=100]&#39;, &#39;인천[2017.11=100]&#39;, &#39;울산[2017.11=100]&#39;, &#39;세종[2017.11=100]&#39;, &#39;경기[2017.11=100]&#39;, &#39;강원[2017.11=100]&#39;, &#39;충북[2017.11=100]&#39;, &#39;충남[2017.11=100]&#39;, &#39;전남[2017.11=100]&#39;, &#39;경북[2017.11=100]&#39;, &#39;경남[2017.11=100]&#39;, &#39;제주[2017.11=100]&#39;, &#39;지방광역시[2017.11=100]&#39;, &#39;지방도[2017.11=100]&#39;, &#39;전국[2017.11=100]&#39;, &#39;지방[2017.11=100]&#39;] . fig = plt.figure(figsize = (15,15)) plt.subplot(2, 1, 1) sns.lineplot(data = apt_price[&#39;2018&#39;:][asc_list]) plt.hlines(100,17525,18635,color=&quot;red&quot;) plt.title(&quot;2017년 11월 이후 2018년도부터 가격 상승지역&quot;) plt.legend(apt_price[asc_list].columns.values, bbox_to_anchor=(1, 1)) plt.show() fig = plt.figure(figsize = (15, 15)) plt.subplot(2, 1, 2) sns.lineplot(data = apt_price[&#39;2018&#39;:][dec_list]) plt.hlines(100,17525,18635,color=&quot;red&quot;) plt.legend(apt_price[dec_list].columns.values, bbox_to_anchor=(1, 1)) plt.title(&quot;2017년 11월 이후 2018년도부터 가격 하락지역&quot;) plt.show() . 연도별_중위소득.xls 을 연도별로 가구마다 기준 중위소득이 어떤 지 보기 위해서 데이터를 변환하여 사용합니다. . 기준 중위 소득 : 「국민기초생활보장법」제 20조의 제2항에 따라 중앙생활보장위원회 심의·의결을 거쳐 고시하는 국민 가구소득의 중위값을 의미 | . income . 2015 2016 2017 2018 2019 2020 2021 2022 . 1인가구 1,562,337 | 1,624,831 | 1,652,931 | 1,672,105 | 1,707,008 | 1,757,194 | 1,827,831 | 1,944,812 | . 2인가구 2,660,196 | 2,766,603 | 2,814,449 | 2,847,097 | 2,906,528 | 2,991,980 | 3,088,079 | 3,260,085 | . 3인가구 3,441,364 | 3,579,019 | 3,640,915 | 3,683,150 | 3,760,032 | 3,870,577 | 3,983,950 | 4,194,701 | . 4인가구 4,222,533 | 4,391,434 | 4,467,380 | 4,519,202 | 4,613,536 | 4,749,174 | 4,876,290 | 5,121,080 | . 5인가구 5,003,702 | 5,203,849 | 5,293,845 | 5,355,254 | 5,467,040 | 5,627,771 | 5,757,373 | 6,024,515 | . 6인가구 5,784,870 | 6,016,265 | 6,120,311 | 6,191,307 | 6,320,544 | 6,506,368 | 6,628,603 | 6,907,004 | . income = income[0:4].transpose().reset_index() income . index 1인가구 2인가구 3인가구 4인가구 . 0 2015 | 1,562,337 | 2,660,196 | 3,441,364 | 4,222,533 | . 1 2016 | 1,624,831 | 2,766,603 | 3,579,019 | 4,391,434 | . 2 2017 | 1,652,931 | 2,814,449 | 3,640,915 | 4,467,380 | . 3 2018 | 1,672,105 | 2,847,097 | 3,683,150 | 4,519,202 | . 4 2019 | 1,707,008 | 2,906,528 | 3,760,032 | 4,613,536 | . 5 2020 | 1,757,194 | 2,991,980 | 3,870,577 | 4,749,174 | . 6 2021 | 1,827,831 | 3,088,079 | 3,983,950 | 4,876,290 | . 7 2022 | 1,944,812 | 3,260,085 | 4,194,701 | 5,121,080 | . &#48516;&#49437;&#44208;&#44284; . Q1. &#50612;&#45712; &#51648;&#50669;&#51060; &#49324;&#46988;&#46308;&#51032; &#49688;&#50836;&#44032; &#47566;&#51012;&#44620;? . 사람들이 많이 사는 지역일수록 그 지역에 사람들의 아파트 및 주택 수요가 많을 것입니다. . 전국에서 사람들이 어느 지역으로 가장 많이 이동하며, 전년도와 차이가 많이 나는 지역을 찾아보아 어디로 밀집되어 있는 지 확인해보겠습니다. . 아래의 데이터를 활용해서 알아봅시다. . 사용할 데이터 . pop_sido = 전출지전입지_시도별_순이동자수.csv . | pop_age = 시군구_연령(5세)별_이동자수.csv :시군구 및 연령(5세 단위)별 인구이동 데이터 . | . fig, ax = plt.subplots(figsize=(15,8)) ax.plot(pop_nationwide_rst.index,pop_nationwide_rst.values, linestyle = &#39;-&#39;) plt.xlabel(&#39;연도&#39;) plt.ylabel(&#39;순이동인구수[명]&#39;) plt.title(&quot;지역별 순이동자수&quot;) plt.legend(pop_nationwide_rst.columns, bbox_to_anchor=(1.14, 1)) plt.show() . 전체 지역별 순이동자수 그래프에서 0을 중심으로 봤을 때, 대부분의 지역들은 0 주변에서 크게 변화하지 않는 반면에 . 경기도와 서울특별시는 순이동자수가 가장 크게 변화하는 것으로 보입니다. . def dif(area1, area2): fig, ax = plt.subplots(figsize=(15,8)) ax.plot(pop_nationwide_rst.index,pop_nationwide_rst[area1].values, linestyle = &#39;-&#39;) ax.plot(pop_nationwide_rst.index,pop_nationwide_rst[area2].values, linestyle = &#39;-&#39;) plt.hlines(0,0,20,color=&quot;gray&quot;) plt.xlabel(&#39;연도&#39;) plt.ylabel(&#39;순이동인구수[명]&#39;) plt.title(area1+&quot;와(과) &quot;+area2+&quot;의 순이동자수[명]&quot;) plt.legend([area1, area2], bbox_to_anchor=(1.12, 1)) plt.show() # 특정 지역 비교 - function dif dif(&quot;경기도&quot;,&quot;서울특별시&quot;) . 0을 기준으로 했을 때 경기도는 위쪽에, 서울특별시는 아래쪽에 그려지는 것으로 보아 전국에서 경기도로는 순유입이, 전국에서 서울특별시로는 순유출이 많을 것을 알 수 있습니다. . 그렇다면, 서울과 경기도로 이주한 사람들의 연령대가 어떤지 알아보겠습니다. (연령대는 일을 시작하는 20대부터 일자리를 그만두는 나이인 50대까지 설정했습니다.) . def sido_df(area): age_area = pop_age[pop_age[&#39;행정구역(시군구)별&#39;] == area] age_area = age_area[age_area.index == &quot;순이동[명]&quot;] age_area = age_area.drop([&#39;행정구역(시군구)별&#39;],axis = 1) age_area = age_area.transpose() age_area.columns = age_area.iloc[0] age_area = age_area.drop(age_area.index[0]) rst = age_area.drop([&#39;계&#39;], axis = 1) return rst # 행정구역(시군구)별이 서울특별시와 경기도인 연령별 순이동자수 age_seoul = sido_df(&quot;서울특별시&quot;) age_gyeonggi = sido_df(&quot;경기도&quot;) . def age_df(df,area_name): plt.style.use (&#39;ggplot&#39;) fig, ax = plt.subplots(figsize=(20,8)) ax.plot(df.index,df[&#39;20 - 24세&#39;], &#39;-&#39;, color=&#39;mediumpurple&#39;) ax.plot(df.index,df[&#39;25 - 29세&#39;], &#39;-&#39;, color=&#39;saddlebrown&#39;) ax.plot(df.index,df[&#39;30 - 34세&#39;], &#39;-&#39;, color=&#39;lightpink&#39;) ax.plot(df.index,df[&#39;35 - 39세&#39;], &#39;-&#39;, color=&#39;slategray&#39;) ax.plot(df.index,df[&#39;40 - 44세&#39;], &#39;-&#39;, color=&#39;skyblue&#39;) ax.plot(df.index,df[&#39;45 - 49세&#39;], &#39;-&#39;, color=&#39;green&#39;) ax.plot(df.index,df[&#39;50 - 54세&#39;], &#39;-&#39;, color=&#39;orange&#39;) ax.plot(df.index,df[&#39;55 - 59세&#39;], &#39;-&#39;, color=&#39;blue&#39;) plt.hlines(0,0,21,color=&quot;red&quot;) plt.xlabel(&#39;연도&#39;) plt.ylabel(&#39;순이동인구수[명]&#39;) plt.title(area_name + &quot;의 연령별 순이동자수&quot;) plt.legend(df.columns.values[4:12], bbox_to_anchor=(1, 1)) plt.show() . age_df(age_seoul,&quot;서울특별시&quot;) # 경기도 연령별 순이동자수 age_df(age_gyeonggi,&quot;경기도&quot;) . 서울지역의 연령별 순이동인구수를 살펴보면, 20대는 지속적으로 순유입이 나타나나 30대부터는 순유출이 발생합니다. 특히, 30-34세에서 지속적으로 순유출이 많은 것으로 나타났습니다. 경기지역의 연령별 순이동인구수를 살펴보면, 전반적으로 2002년에 순유입이 나타나다가 점차 낮아지면서 순유출이 발생합니다. . A1. &#51452;&#47196; &#44221;&#44592;&#50752; &#49436;&#50872;&#50640; &#51064;&#44396;&#44032; &#47566;&#51060; &#47792;&#47140; &#49688;&#50836;&#46020; &#47566;&#50500;&#51648;&#44256; &#51080;&#45796;. . 대부분의 사람들은 인구가 많이 밀집하며 일자리와 교통이 편리한 수도권으로 이동하고, 수도권 주변의 외곽 지역으로도 이동하는 것을 알 수 있습니다. 그렇기 때문에 수도권 부근은 지속적으로 인구 및 세대수가 증가하는 추세라는 것을 확인할 수 있습니다. (출처) . 주로 서울특별시와 경기도에서 크게 순이동자수가 변동된 것을 볼 수있었고, 서울특별시로는 순유출이, 경기도로는 순유입이 많은 것으로 나타났습니다. . 그리고 두 지역으로 순이동하는 연령대를 살펴봤을 때, 서울특별시로는 20대가 순유입되는 것을 볼 수 있었는데 이것은 일자리의 이유가 컸고, 30대부터는 주택의 이유로 순유출로 특히 30대 인구수가 줄어들어다는 것을 알 수 있습니다. ( 출처 ) . 경기도에서는 20~50대가 순유입이 많지만, 점차 순유출되고 있어 인구가 감소하고 있습니다. 서울에 비해서는 전입자 증가로 도내 전체 인구가 증가세를 보이고 있지만, 전반적인 출생률 감소와 재개발·재건축 등의 영향으로 인구 감소 지역이 늘어나는 것으로 예상됩니다. ( 출처 ) . Q2. &#49688;&#50836;&#44032; &#47566;&#51008; &#51648;&#50669;&#51032; &#51452;&#53469;&#51008; &#50620;&#47560;&#45208; &#51080;&#51012;&#44620;? . 수요가 많은 지역을 Q1에서 알아보았습니다. 그러면 현재 수요가 많은 지역에 주택은 얼마나 있을 지 알아보고, . 만약 주택이 부족하여 주택난이 생기는 지역이 있다면, 어떻게 해결 할 수 있을 지 생각해봅시다. . 우리나라는 다른 주택형식보다 아파트를 선호하는 경향이 있으며 그 이유에는 대표적으로 사고 팔 때 수익이 나는 시세차익, 주차장과 생활환경등의 편의성 측면에서 아파트의 선호 사상이 크다고 생각됩니다. ( 참고 ) 이를 반영해, 분석에서도 주택형식은 아파트에 초점을 맞춰서 분석을 할 것 입니다. . 사용할 데이터 . build_h = 주택건설인허가실적.csv | . non_house = 미분양주택현황.csv | . fig = plt.figure(figsize = (17, 7)) sns.lineplot(data = mon_12[[&#39;전국[호]&#39;,&#39;서울[호]&#39;,&#39;경기[호]&#39;]]) plt.title(&quot;서울과 경기의 건설 인허가 변화&quot;) plt.show() . fig = plt.figure(figsize = (17, 7)) sns.lineplot(data = mon_12[[&#39;서울[호]&#39;,&#39;경기[호]&#39;]]) plt.text(mon_12[&#39;서울[호]&#39;].index[0],mon_12[&#39;서울[호]&#39;][0],mon_12[&#39;서울[호]&#39;][0].round(3), fontsize = 15, weight = &#39;bold&#39;, ha = &#39;right&#39;) plt.text(mon_12[&#39;서울[호]&#39;].index[-1],mon_12[&#39;서울[호]&#39;][-1],mon_12[&#39;서울[호]&#39;][-1].round(3), fontsize = 15, weight = &#39;bold&#39;, ma = &#39;center&#39;) plt.text(mon_12[&#39;경기[호]&#39;].index[0],mon_12[&#39;경기[호]&#39;][0],mon_12[&#39;경기[호]&#39;][0].round(3), fontsize = 15, weight = &#39;bold&#39;, ha = &#39;right&#39;) plt.text(mon_12[&#39;경기[호]&#39;].index[-1],mon_12[&#39;경기[호]&#39;][-1],mon_12[&#39;경기[호]&#39;][-1].round(3), fontsize = 15, weight = &#39;bold&#39;, ma = &#39;center&#39;) plt.title(&quot;서울과 경기 내의 인허가 변화&quot;) plt.show() . 2016년 이후 전국적으로 주택 공급이 감소하고 있습니다. . 서울은 2007년부터 계속해서 10만호 이하인 상태를 유지하고 있습니다. (단, 2016년도에는 101235호, 2018년도에는 113131호) . 경기도는 서울에 비해서 매년 대부분이 10만호 이상이지만, 2016년도부터 감소되고 있는 것을 확인할 수 있습니다. . non_house.head() . 전국[호] 서울[호] 부산[호] 대구[호] 인천[호] 광주[호] 대전[호] 울산[호] 경기[호] 강원[호] 충북[호] 충남[호] 전북[호] 전남[호] 경북[호] 경남[호] 제주[호] 수도권[호] 세종[호] . 날짜 . 2007-01-01 75616.0 | 697.0 | 8680.0 | 9467.0 | 243.0 | 7005.0 | 819.0 | 1180.0 | 3604.0 | 5312.0 | 2548.0 | 7449.0 | 5179.0 | 5374.0 | 6210.0 | 11772.0 | 77.0 | 4544.0 | NaN | . 2007-02-01 73619.0 | 590.0 | 8653.0 | 9134.0 | 312.0 | 6246.0 | 803.0 | 1210.0 | 3361.0 | 5344.0 | 2645.0 | 7363.0 | 5098.0 | 5253.0 | 5948.0 | 11563.0 | 96.0 | 4263.0 | NaN | . 2007-03-01 73162.0 | 687.0 | 8548.0 | 9189.0 | 266.0 | 5905.0 | 1048.0 | 1687.0 | 2579.0 | 5801.0 | 2565.0 | 7446.0 | 5184.0 | 4887.0 | 5854.0 | 11328.0 | 188.0 | 3532.0 | NaN | . 2007-04-01 73393.0 | 685.0 | 8074.0 | 9009.0 | 219.0 | 5829.0 | 1020.0 | 1521.0 | 2488.0 | 5460.0 | 3632.0 | 8250.0 | 4812.0 | 4664.0 | 5419.0 | 12149.0 | 162.0 | 3392.0 | NaN | . 2007-05-01 78571.0 | 704.0 | 7964.0 | 10888.0 | 362.0 | 7842.0 | 957.0 | 1500.0 | 2488.0 | 5212.0 | 4012.0 | 9195.0 | 4625.0 | 4640.0 | 6303.0 | 11723.0 | 156.0 | 3554.0 | NaN | . fig = plt.figure(figsize = (17, 7)) sns.lineplot(data = non_house[[&#39;전국[호]&#39;,&#39;서울[호]&#39;,&#39;경기[호]&#39;]]) plt.title(&quot;서울, 경기 지역의 미분양주택 수 변화&quot;) plt.show() . fig = plt.figure(figsize = (17, 7)) plt.subplot(1, 2, 1) sns.lineplot(data = non_house[&#39;서울[호]&#39;]) plt.text(non_house[&#39;서울[호]&#39;].index[-1],non_house[&#39;서울[호]&#39;][-1],non_house[&#39;서울[호]&#39;][-1], fontsize = 15, weight = &#39;bold&#39;, ma = &#39;center&#39;) plt.title(&quot;서울 내의 미분양 주택 변화&quot;) plt.subplot(1, 2, 2) sns.lineplot(data = non_house[&#39;경기[호]&#39;],color=&#39;blue&#39;) plt.text(non_house[&#39;경기[호]&#39;].index[-1],non_house[&#39;경기[호]&#39;][-1],non_house[&#39;경기[호]&#39;][-1], fontsize = 15, weight = &#39;bold&#39;, ma = &#39;center&#39;) plt.title(&quot;경기도 내의 미분양 주택 변화&quot;) plt.show() . 2014년 이후 서울과 경기도 지역의 미분양 주택이 점차 줄어들고 있습니다. . 서울의 경우, 2021년 2월 미분양 주택이 88호로 근래들어 미분양 주택이 거의 없습니다. 경기도 또한 1367호로 전년도에 비해서 미분양 주택이 감소되었다는 것을 알 수 있습니다. . A2. &#49688;&#50836;&#44032; &#47566;&#51008; &#51648;&#50669;&#51076;&#50640;&#46020; &#48520;&#44396;&#54616;&#44256; &#51452;&#53469; &#49688;&#45716; &#48512;&#51313;&#54616;&#45796;. . 전국적으로 건축 공급이 줄어들고 있으며, 미분양 물량도 줄어들고 있습니다. . 인구가 빠지고 있어도 순유입이 많은 경기도 지역에서도 건설 인허가와 미분양 주택이 감소하고 있고, 특히 서울의 경우 그 정도가 심한 것을 미분양 물량이 거의 없는 것을 통해 확인 할 수 있다. . 주택 물량이 없는 이유는 전국 주택시장을 짓눌렀던 &#39;물량 공세&#39;가 어느 정도 사라졌기 때문입니다. 지방과 수도권, 서울 등이 전체적으로 재작년보다 준공실적이 줄어 최근 1~2년 지속된 물량부담이 약화될 것입니다. 따라서 순유입에 따라 인구가 집중되는 지역에 주택 건설이 필요할 것입니다. ( 출처 ) . Q3. &#51665;&#44050;&#51008; &#50612;&#46523;&#44172; &#48320;&#54868;&#54624;&#44620;? . 수요가 많은 지역에 건설 인허가와 미분양 주택 변화에 대해 알아보고 해결책을 생각해보았습니다. . 그렇다면 수요가 많은 지역의 집값은 어떻게 변하고 지금과 얼마나 차이가 날까요? . 지난 몇년간 데이터를 통해 집값이 어떻게 변하고 있는 지 알아보고, 내 집 마련을 할 수 있을 지 생각해보겠습니다. . 사용할 데이터 . house_j = 주택전세가격지수(KB).csv :2019년 1월을 기준으로 한 주택유형별 전세 가격지수 &gt; &gt; - house_m = 주택매매가격지수(KB).csv :2019년 1월을 기준으로 한 주택유형별 매매 가격지수 &gt; &gt; - monthly = 유형별_주택월세통합가격지수.csv :2017년 11월을 기준으로 한 유형 및 지역별 주택 월세 통합 가격 지수 &gt; &gt; - apt_price = 아파트_실거래가격지수.csv :2017년 11월을 기준으로한 지역별 아파트 실거래 가격 지수 &gt; &gt; - income = 연도별_중위소득.xls :가구별, 연도별 기준 중위소득 | . (1) 아파트 실거래가격지수 . fig = plt.figure(figsize = (15,15)) plt.subplot(2, 1, 1) sns.lineplot(data = apt_price[&#39;2018&#39;:][asc_list]) plt.hlines(100,17525,18635,color=&quot;red&quot;) plt.title(&quot;2017년 11월 이후 2018년도부터 가격 상승지역&quot;) plt.legend(apt_price[asc_list].columns.values, bbox_to_anchor=(1, 1)) plt.show() fig = plt.figure(figsize = (15, 15)) plt.subplot(2, 1, 2) sns.lineplot(data = apt_price[&#39;2018&#39;:][dec_list]) plt.hlines(100,17525,18635,color=&quot;red&quot;) plt.legend(apt_price[dec_list].columns.values, bbox_to_anchor=(1, 1)) plt.title(&quot;2017년 11월 이후 2018년도부터 가격 하락지역&quot;) plt.show() . 2017년 11월 이후, 상승세 였던 지역에선 대부분이 2018년 이후 급격하게 치솟는 지역들이 존재합니다. . 하락세 였던 지역들도 2020년을 기점으로 점차 상승하는 추세를 보여주고 있습니다. . 그렇다면 2018년도와 2021년도 사이에 가격 격차는 어떻게 되는지 알아보겠습니다. . asc_gap_list = [] for i in range(len(apt_price[asc_list].columns)): gap = apt_price.loc[&#39;2021-01&#39;][asc_list].iloc[0,i] - apt_price.loc[&#39;2018-01&#39;][asc_list].iloc[0,i] gap = str(round(gap,2)) + &quot; - &quot; + apt_price[asc_list].columns[i] asc_gap_list.append(gap) asc_gap_list.sort(reverse = True) print(&#39;가격 상승지역의 가격차이&#39;,&#39; n&#39;,asc_gap_list) print(&quot;_&quot;*150) dec_gap_list = [] for i in range(len(apt_price[dec_list].columns)): gap = apt_price.loc[&#39;2021-01&#39;][dec_list].iloc[0,i] - apt_price.loc[&#39;2018-01&#39;][dec_list].iloc[0,i] gap = str(round(gap,2)) + &quot; - &quot; + apt_price[dec_list].columns[i] dec_gap_list.append(gap) dec_gap_list.sort(reverse = True) print(&#39;가격 하락지역의 가격차이&#39;,&#39; n&#39;,dec_gap_list) #지역이름까지 나오게 하기 위해 str으로 설정해줬기에, 8.19가 가장 위로 나왔지만, 79.4인 세종이 가장 격차가 큼. . 가격 상승지역의 가격차이 [&#39;69.9 - 동북권[2017.11=100]&#39;, &#39;59.3 - 서울[2017.11=100]&#39;, &#39;57.9 - 서북권[2017.11=100]&#39;, &#39;57.9 - 서남권[2017.11=100]&#39;, &#39;52.0 - 도심권[2017.11=100]&#39;, &#39;51.6 - 동남권[2017.11=100]&#39;, &#39;42.9 - 대전[2017.11=100]&#39;, &#39;41.0 - 수도권[2017.11=100]&#39;, &#39;3.3 - 전북[2017.11=100]&#39;, &#39;23.8 - 광주[2017.11=100]&#39;] ______________________________________________________________________________________________________________________________________________________ 가격 하락지역의 가격차이 [&#39;8.5 - 울산[2017.11=100]&#39;, &#39;8.2 - 전남[2017.11=100]&#39;, &#39;79.4 - 세종[2017.11=100]&#39;, &#39;5.4 - 충남[2017.11=100]&#39;, &#39;36.0 - 경기[2017.11=100]&#39;, &#39;29.1 - 대구[2017.11=100]&#39;, &#39;25.5 - 전국[2017.11=100]&#39;, &#39;22.3 - 지방광역시[2017.11=100]&#39;, &#39;19.0 - 인천[2017.11=100]&#39;, &#39;17.3 - 부산[2017.11=100]&#39;, &#39;12.4 - 지방[2017.11=100]&#39;, &#39;1.9 - 지방도[2017.11=100]&#39;, &#39;1.8 - 충북[2017.11=100]&#39;, &#39;-1.1 - 경남[2017.11=100]&#39;, &#39;-0.6 - 경북[2017.11=100]&#39;, &#39;-0.3 - 제주[2017.11=100]&#39;, &#39;-0.2 - 강원[2017.11=100]&#39;] . 격차가 큰 상위 3개의 지역을 살펴보겠습니다. . 가격 상승지역 : 1. 동북권 / 2. 서울 / 3. 서북권,서남권 | 가격 하락지역 : 1. 세종 / 2. 경기 / 3. 대구 | . asc_gap_top3 = [&#39;동북권[2017.11=100]&#39;,&#39;서울[2017.11=100]&#39;,&#39;서북권[2017.11=100]&#39;,&#39;서남권[2017.11=100]&#39;] dec_gap_top3 = [&#39;세종[2017.11=100]&#39;,&#39;경기[2017.11=100]&#39;,&#39;대구[2017.11=100]&#39;] fig = plt.figure(figsize = (17, 5)) sns.lineplot(data = apt_price[&#39;2018&#39;:][asc_gap_top3]) plt.title(&quot;가격 상승지역 중 격차 Top3&quot;) plt.show() fig = plt.figure(figsize = (17, 5)) sns.lineplot(data = apt_price[&#39;2018&#39;:][dec_gap_top3]) plt.title(&quot;가격 하락지역 중 격차 Top3&quot;) plt.show() . print(&quot;서울의 아파트 최대,최소 가격 지수 :&quot;,apt_price[[&#39;서울[2017.11=100]&#39;]].iloc[0,0],&quot;,&quot;,apt_price[[&#39;서울[2017.11=100]&#39;]].iloc[-1,0], &quot;/ 증가 비율 : &quot;, round(apt_price[[&#39;서울[2017.11=100]&#39;]].iloc[-1,0]/ apt_price[[&#39;서울[2017.11=100]&#39;]].iloc[0,0],2),&quot;배&quot;) print(&quot;경기도의 아파트 최대,최소 가격 지수 :&quot;,apt_price[[&#39;경기[2017.11=100]&#39;]].iloc[0,0],&quot;,&quot;,apt_price[[&#39;경기[2017.11=100]&#39;]].iloc[-1,0], &quot;/ 증가 비율 : &quot;, round(apt_price[[&#39;경기[2017.11=100]&#39;]].iloc[-1,0]/ apt_price[[&#39;경기[2017.11=100]&#39;]].iloc[0,0],2),&quot;배&quot;) . 서울의 아파트 최대,최소 가격 지수 : 60.6 , 162.2 / 증가 비율 : 2.68 배 경기도의 아파트 최대,최소 가격 지수 : 69.2 , 135.6 / 증가 비율 : 1.96 배 . 위의 그래프를 봤을 때, . 가격 상승지역 중 격차가 큰 상위 3개의 지역 모두 서울의 권역생활권이며, 서울은 2년 사이 전국에서 두번째로 아파트 가격비율이 높게 상승한 지역입니다. 서울시 아파트 가격은 2006년 60.6에서 2021년 1월 162.2로, 2.68배 증가하였습니다. . 가격 하락지역 중 격차가 큰 상위 3개 지역 중 세종이 2017년에 실질적 대한민국 행정수도 계획 등으로 세종이 관심을 받아 집값도 같이 상승하게 된 것이라고 생각됩니다. 급격하게 집값이 상승한 세종을 제외하면 경기 지역도 아파트 가격이 상승되었습니다. 경기도 아파트 가격은 2006년에는 69.2에서 2021년 1월 135.6으로, 1.95배 증가하였습니다. . (2) 유형별_주택월세통합가격지수 . 주택유형에는 아파트, 연립다세대, 단독주택이 있으며, 이들을 종합한 &#39;종합&#39;유형까지 데이터에는 총 4가지 유형이 존재하고, 한국인의 아파트 선호 요인에 의해 아파트 유형에 대해서만 분석하였습니다. . 지역은 전국과 서울, 경기도만 분석에 사용하였습니다. . d_monthly = monthly[[&#39;아파트-전국[2017.11=100]&#39;,&#39;아파트-서울[2017.11=100]&#39;,&#39;아파트-경기[2017.11=100]&#39;]] d_monthly . 아파트-전국[2017.11=100] 아파트-서울[2017.11=100] 아파트-경기[2017.11=100] . 날짜 . 2015-06-01 100.4 | 100.0 | 99.3 | . 2015-07-01 100.4 | 100.0 | 99.4 | . 2015-08-01 100.4 | 100.0 | 99.4 | . 2015-09-01 100.5 | 100.2 | 99.5 | . 2015-10-01 100.6 | 100.3 | 99.6 | . ... ... | ... | ... | . 2020-11-01 98.5 | 100.6 | 99.6 | . 2020-12-01 99.0 | 100.9 | 100.0 | . 2021-01-01 99.4 | 101.2 | 100.3 | . 2021-02-01 99.7 | 101.4 | 100.7 | . 2021-03-01 99.9 | 101.5 | 100.9 | . 70 rows × 3 columns . fig = plt.figure(figsize = (17, 7)) d =sns.lineplot(data = d_monthly) plt.title(&quot;아파트 월세 변화 시각화&quot;) #전국 가격표시 d.text(d_monthly.index[0],d_monthly.iloc[0,0],d_monthly.iloc[0,0], fontsize = 15, weight = &#39;bold&#39;, ha = &#39;right&#39;) d.text(d_monthly.index[-1],d_monthly.iloc[-1,0],d_monthly.iloc[-1,0], fontsize = 15, weight = &#39;bold&#39;, ma = &#39;center&#39;) #최저점 표시 d.text(d_monthly.idxmin()[0],d_monthly.loc[d_monthly.idxmin()[0],&#39;아파트-전국[2017.11=100]&#39;], d_monthly.loc[d_monthly.idxmin()[0],&#39;아파트-전국[2017.11=100]&#39;], fontsize = 15, weight = &#39;bold&#39;, ha = &#39;center&#39;, va = &#39;top&#39;) #서울시 가격표시 d.text(d_monthly.index[0],d_monthly.iloc[0,1],d_monthly.iloc[0,1], fontsize = 15, weight = &#39;bold&#39;, ha = &#39;right&#39;) d.text(d_monthly.index[-1],d_monthly.iloc[-1,1],d_monthly.iloc[-1,1], fontsize = 15, weight = &#39;bold&#39;, ma = &#39;center&#39;) d.text(d_monthly.idxmin()[1],d_monthly.loc[d_monthly.idxmin()[1],&#39;아파트-서울[2017.11=100]&#39;], d_monthly.loc[d_monthly.idxmin()[1],&#39;아파트-서울[2017.11=100]&#39;], fontsize = 15, weight = &#39;bold&#39;, ha = &#39;center&#39;, va = &#39;top&#39;) #경기도 가격표시 d.text(d_monthly.index[0],d_monthly.iloc[0,2],d_monthly.iloc[0,2], fontsize = 15, weight = &#39;bold&#39;, ha = &#39;right&#39;) d.text(d_monthly.index[-1],d_monthly.iloc[-1,2],d_monthly.iloc[-1,2], fontsize = 15, weight = &#39;bold&#39;, ma = &#39;center&#39;) d.text(d_monthly.idxmin()[2],d_monthly.loc[d_monthly.idxmin()[2],&#39;아파트-경기[2017.11=100]&#39;], d_monthly.loc[d_monthly.idxmin()[2],&#39;아파트-경기[2017.11=100]&#39;], fontsize = 15, weight = &#39;bold&#39;, ha = &#39;center&#39;, va = &#39;top&#39;) plt.show() . d_monthly.idxmin() . 아파트-전국[2017.11=100] 2019-09-01 아파트-서울[2017.11=100] 2019-05-01 아파트-경기[2017.11=100] 2019-08-01 dtype: datetime64[ns] . 근래 들어 서울시와 경기도 아파트 월세가격이 증가하는 추세임을 알 수 있으며,전국은 그 상승세가 더 가파릅니다. . 전국과 서울, 경기 모두 2019년에 월세가격이 최저점을 찍을 것을 생각해봤을 때, 월세 가격 상승세가 매우 급격한 것을 알 수 있습니다. . (3) 주택매매가격지수와 주택전세가격지수 . 주택유형으로 아파트, 연립주택, 단독주택이 있으며, 이들을 종합한 &#39;총지수&#39;유형이 존재하고, 한국인의 아파트 선호 요인에 의해 아파트 유형에 대해서만 분석하였습니다. . 전국 데이터와 서울 데이터 나누어져 있으며, 서울의 경우 총지수와 아파트 데이터만 존재합니다. . hm_apt = house_m.loc[&#39;2017&#39;:,[&#39;총지수[2019.01=100]&#39;,&#39;아파트[2019.01=100]&#39;, &#39;아파트(서울)[2019.01=100]&#39;,&#39;총지수(서울)[2019.01=100]&#39;]] # 주택전세가 - 총지수와 아파트만 추출 hj_apt = house_j.loc[&#39;2017&#39;:,[&#39;총지수[2019.01=100]&#39;,&#39;아파트[2019.01=100]&#39;, &#39;아파트(서울)[2019.01=100]&#39;,&#39;총지수(서울)[2019.01=100]&#39;]] . fig = plt.figure(figsize = (13, 7)) m = sns.lineplot(data = hm_apt) plt.vlines(17899,85,120,color=&quot;green&quot;) plt.title(&quot;서울의 아파트 매매가&quot;) m.text(house_m.index[-1],house_m.iloc[-1,3],house_m.iloc[-1,3].round(3), fontsize = 15, weight = &#39;bold&#39;, ma = &#39;center&#39;) m.text(house_m.index[-1],house_m.iloc[-1,4],house_m.iloc[-1,4].round(3), fontsize = 15, weight = &#39;bold&#39;, ma = &#39;center&#39;) # 주택전세가 fig = plt.figure(figsize = (13, 7)) j = sns.lineplot(data = hj_apt) plt.vlines(17899,85,120,color=&quot;green&quot;) plt.title(&quot;서울의 아파트 전세가&quot;) j.text(house_j.index[-1],house_j.iloc[-1,3],house_j.iloc[-1,3].round(3), fontsize = 15, weight = &#39;bold&#39;, ma = &#39;center&#39;) j.text(house_j.index[-1],house_j.iloc[-1,4],house_j.iloc[-1,4].round(3), fontsize = 15, weight = &#39;bold&#39;, ma = &#39;center&#39;) plt.show() . 아파트 매매 가격은 계속해서 상승세를 보이고 있고, 아파트 전세가격 또한 계속해서 상승세를 보이고 있습니다. . 하지만, 아파트 가격이 근래에 들어 그 정도가 심하게 증가했다는 것을 알 수 있습니다. . 또한, 서울의 경우 그 상승세가 전국기준에 비해 더 가파릅니다. . (4) 가격지표 종합 . 기간은 아파트실거래가의 시작점인 2006년에 맞추었고, 월세가는 2015년 이후 자료부터 존재합니다. . 주택매매가격지수와 주택전세가격지수가 서울 지역만 나와있기 때문에 서울은 아파트 가격에 대한 모든 가격지표들을 종합하여 시각화 하였습니다. 경기지역은 아파트 실거래가와 주택월세가 가격지표만 종합하여 시각화 하였습니다. . fig = plt.figure(figsize = (17, 7)) # 아파트 실거래가 apt_seoul = apt_price[[&#39;서울[2017.11=100]&#39;]] sns.lineplot(data = apt_seoul, label = &#39;아파트 실거래가&#39;, legend = False ) # 주택월세가 sns.lineplot(data = d_monthly.iloc[:,1], label = &#39;아파트 월세가&#39;) # 주택매매가 sns.lineplot(data = house_m.loc[&#39;2006&#39;:,&#39;아파트(서울)[2019.01=100]&#39;], label = &#39;아파트 매매가&#39;) # 주택전세가 sns.lineplot(data = house_j.loc[&#39;2006&#39;:,&#39;아파트(서울)[2019.01=100]&#39;], label = &#39;아파트 전세가&#39;) plt.title(&quot;서울시 아파트 가격지표&quot;) plt.show() . fig = plt.figure(figsize = (17, 7)) # 아파트 실거래가 apt_gg = apt_price[[&#39;경기[2017.11=100]&#39;]] sns.lineplot(data = apt_gg, label = &#39;아파트 실거래가&#39;, legend = False ) # 주택월세가 sns.lineplot(data = d_monthly.iloc[:,2], label = &#39;아파트 월세가&#39;) plt.title(&quot;경기도 아파트 가격지표&quot;) plt.show() . 서울과 경기도 지역 모두, 모든 가격지표에서 상승세를 보여주고 있습니다. . 아파트의 가격이 상승세를 이루고 있는데 내집마련을 할 수 있을까요? . 우리나라의 가구별 소득이 어떠한지 알아보고, 내집마련의 목표를 이룰 수 있을 것인지 살펴봅시다. . person = [&quot;1인가구&quot;,&quot;2인가구&quot;,&quot;3인가구&quot;,&quot;4인가구&quot;] plt.subplots(figsize=(15,8)) for p in person: plt.plot(income[&quot;index&quot;],income[p]) plt.xlabel(&#39;연도&#39;) plt.ylabel(&#39;기준중위소득[원]&#39;) plt.title(&quot;가구별 기준중위소득&quot;) plt.legend(income.columns[1:9], bbox_to_anchor=(1.14, 1)) plt.show() . A3. &#44032;&#44396;&#48324;&#47196; &#47588;&#45380;&#47560;&#45796; &#44592;&#51456; &#51473;&#50948;&#49548;&#46301;&#51060; &#51613;&#44032;&#54616;&#44256; &#51080;&#50612;&#46020; &#50500;&#54028;&#53944; &#44032;&#44201; &#46608;&#54620;, &#45908; &#45458;&#51008; &#49345;&#49849;&#49464;&#47484; &#44032;&#51648;&#44256; &#51080;&#50612; &#45236; &#51665; &#47560;&#47144;&#51060; &#55192;&#46308;&#50612; &#51656; &#44163;&#51060;&#45796;. . 부동산 가격 데이터 분석을 통해 서울과 경기도를 비롯해 대부분의 지역의 아파트 가격이 계속해서 상승하고 있다는 것이 확인되었습니다. . 가구별로 매년마다 기준 중위소득이 증가하고 있지만, 아파트 가격은 소득상승 정도를 고려하여도 집을 구매하는 것이 전년도와 변함이 없거나 점점 힘들어질 것입니다. . 참고 기사 : 강북도 돌아섰다…서울 집값 상승 전환 | . &#44208;&#47200; . &#50836;&#50557; . 시도별로 전반적으로 집값이 오르는 추세이고, 또한 한국인이 선호하는 아파트 가격도 증가하고 있습니다. . 주로 서울과 경기지역에 집값이 많이 오른 상태인데, 그 이유를 생각해봤을 때 사람이 밀집되고 있어 수요가 많은 것에 비해 미분양 주택의 수가 적기 때문에 점차 가격이 오르는 것으로 생각합니다. . 전체 소득도 년마다 증가해도 아파트 값도 그만큼 오르고 있기 때문에 내집마련을 하기에는 아직 많이 힘들 것이라고 예상됩니다. 특히, 이제 돈을 벌기 시작하는 20대나 30대들은 집을 사기엔 더욱 힘들 것입니다. 그래도 청년들 같은 경우, 청년주택 산업과 같은 주거 지원 정책을 활용해 집을 마련하는 방법을 찾아보아야 할 것입니다. . 참고 기사 : 20대, 서울에 내집 마련하려면…몇년 꼬박 모아야 할까? | . &#54620;&#44228;&#51216; . 부동산 가격에 영향을 끼치는 것은 수요와 공급, 아파트선호, 인구집중 뿐만 아니라 저금리와 유동성, 사회적 심리요인등 다양한 요인들이 존재합니다. . 이 분석 및 시각화에서는 수요와 공급, 아파트선호, 인구집중 요인 말고 다른 요인들은 부동산 가격과 어떠한 관계가 있는지 고려되지 않았기 때문에 데이터 분석 및 시각화에 대한 결과를 완전히 신뢰할 수 없다는 한계가 있습니다. . 이 문제는 부동산 가격과 어떤 요인들이 가장 영향을 끼치는 지 몇가지 더 알아보고 분석을 한 후 시각화를 통해 나타낼 수 있을 것 입니다. . 또한 소득을 고려하여 부동산 가격지표를 분석할 수도 있을 것입니다. 예를 들어, 소득 대비 주택 가격 비율(PIR)이나 소득 대비 주택 전세가격(J-PIR)을 계산해보면 한 가구의 소득에 비해 주택가격이 어느정도로 비싼지, 가구소득 대비 전세가격이 얼마나 비싼지에 대해 알 수 있어 활용하면 더 유의미한 인사이트들을 도출해낼 수 있을 것입니다. . 참고자료 및 데이터 출처 . 한국 부동산 데이터 시각화 경진대회 : https://dacon.io/competitions/official/235724/overview/description | 집값 상승 원인부터 정확히 [하성규 칼럼] : https://www.hapt.co.kr/news/articleView.html?idxno=153607 | KOSIS 국가통계포털 : https://kosis.kr/index/index.do | e-나라지표 : https://www.index.go.kr/potal/main/EachDtlPageDetail.do?idx_cd=2762 | 서울·인천·경기에 우리나라 인구 절반이 산다...인구 증가폭은 5년 연속 &#39;0%대&#39; : https://www.ajunews.com/view/20210729112506010 | 20대만 서울살이 늘었다… 다른 연령대는 집 때문에 ‘탈출’ : https://www.chosun.com/national/regional/seoul/2022/05/12/OF64HH35ZVHFHCKLLN6CZJOI7I/ | 경기도 인구 증가세에도 14개 시군은 되레 감소…감소지역 늘어 : https://www.yna.co.kr/view/AKR20220207119600061 | 작년 주택공급 4대지표 모두 감소 : https://www.mk.co.kr/news/realestate/view/2020/01/97116/ | 강북도 돌아섰다…서울 집값 상승 전환 : https://www.hankyung.com/realestate/article/2022050549496 | 전국 집값 상승 폭 5개월 만에 확대…전망도 &#39;회복세&#39; : https://www.hankyung.com/realestate/article/2022042481427 | 20대, 서울에 내집 마련하려면…몇년 꼬박 모아야 할까? : https://www.mk.co.kr/news/economy/view/2021/10/1026808/ | .",
            "url": "https://woojaaeon.github.io/jaong/jupyter/vscode/python/2022/06/04/final.html",
            "relUrl": "/jupyter/vscode/python/2022/06/04/final.html",
            "date": " • Jun 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Geospatial Analysis exercise - proximity analysis",
            "content": "Introduction . You are part of a crisis response team, and you want to identify how hospitals have been responding to crash collisions in New York City. . Before you get started, run the code cell below to set everything up. . import math import geopandas as gpd import pandas as pd from shapely.geometry import MultiPolygon import folium from folium import Choropleth, Marker from folium.plugins import HeatMap, MarkerCluster from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex5 import * . You&#39;ll use the embed_map() function to visualize your maps. . 지도를 시각화하기 위해 embed_map() 함수를 사용할 것입니다. . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . Exercises . 1) Visualize the collision data. . Run the code cell below to load a GeoDataFrame collisions tracking major motor vehicle collisions in 2013-2018. . 아래 코드 셀을 실행하여 2013-2018년의 주요 자동차 충돌을 추적하는 GeoDataFramecollisions을 로드합니다. . collisions = gpd.read_file(&quot;../input/geospatial-learn-course-data/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp&quot;) collisions.head() . Use the &quot;LATITUDE&quot; and &quot;LONGITUDE&quot; columns to create an interactive map to visualize the collision data. What type of map do you think is most effective? . &quot;LATITUDE&quot; 및 &quot;LONGITUDE&quot; 열을 사용하여 충돌 데이터를 시각화하는 대화형 맵을 만듭니다. 어떤 유형의 지도가 가장 효과적이라고 생각하십니까? . m_1 = folium.Map(location=[40.7, -74], zoom_start=11) # Your code here: Visualize the collision data HeatMap(data=collisions[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=10).add_to(m_1) # Uncomment to see a hint q_1.hint() # Show the map embed_map(m_1, &quot;q_1.html&quot;) . q_1.check() # Uncomment to see our solution (your code may look different!) #q_1.solution() . 2) Understand hospital coverage. . Run the next code cell to load the hospital data. . 다음 코드 셀을 실행하여 병원 데이터를 로드합니다. . hospitals = gpd.read_file(&quot;../input/geospatial-learn-course-data/nyu_2451_34494/nyu_2451_34494/nyu_2451_34494.shp&quot;) hospitals.head() . Use the &quot;latitude&quot; and &quot;longitude&quot; columns to visualize the hospital locations. . &quot;위도&quot; 및 &quot;경도&quot; 열을 사용하여 병원 위치를 시각화합니다. . m_2 = folium.Map(location=[40.7, -74], zoom_start=11) # Your code here: Visualize the hospital locations for idx, row in hospitals.iterrows(): Marker([row[&#39;latitude&#39;], row[&#39;longitude&#39;]], popup=row[&#39;name&#39;]).add_to(m_2) # Uncomment to see a hint q_2.hint() # Show the map embed_map(m_2, &quot;q_2.html&quot;) . q_2.check() # Uncomment to see our solution (your code may look different!) #q_2.solution() . 3) When was the closest hospital more than 10 kilometers away? . Create a DataFrame outside_range containing all rows from collisions with crashes that occurred more than 10 kilometers from the closest hospital. . Note that both hospitals and collisions have EPSG 2263 as the coordinate reference system, and EPSG 2263 has units of meters. . . 3) 10km 이상 떨어진 가장 가까운 병원은 언제였습니까? . 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌이 있는 collisions 의 모든 행을 포함하는 DataFrame outside_range를 만듭니다. . hospitals과 collisions 모두 좌표 참조 시스템으로 EPSG 2263을 사용하고 EPSG 2263은 미터 단위를 사용합니다. . hos_df_buffer = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000) print(hos_df_buffer) hos_union = hos_df_buffer.geometry.unary_union hos_union . outside_range = collisions.loc[~collisions[&quot;geometry&quot;].apply(lambda x: hos_union.contains(x))] outside_range . q_3.check() . q_3.hint() #q_3.solution() . The next code cell calculates the percentage of collisions that occurred more than 10 kilometers away from the closest hospital. . 다음 코드 셀은 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌의 비율을 계산합니다. . percentage = round(100*len(outside_range)/len(collisions), 2) print(&quot;Percentage of collisions more than 10 km away from the closest hospital: {}%&quot;.format(percentage)) . 4) Make a recommender. . When collisions occur in distant locations, it becomes even more vital that injured persons are transported to the nearest available hospital. . With this in mind, you decide to create a recommender that: . takes the location of the crash (in EPSG 2263) as input, | finds the closest hospital (where distance calculations are done in EPSG 2263), and | returns the name of the closest hospital. | . . 멀리 떨어진 곳에서 충돌이 발생하면 부상자를 가까운 병원으로 이송하는 것이 더욱 중요해집니다. . 이를 염두에 두고 다음과 같은 추천자를 만들기로 결정합니다. . 충돌 위치(EPSG 2263에서)를 입력으로 사용합니다. | 가장 가까운 병원을 찾습니다(거리 계산은 EPSG 2263에서 수행됨). | 가장 가까운 병원의 이름을 반환합니다. | . def best_hospital(collision_location): # Your code here idx_min = hospitals.geometry.distance(collision_location).idxmin() my_hospital = hospitals.iloc[idx_min] name = my_hospital[&quot;name&quot;] return name # Test your function: this should suggest CALVARY HOSPITAL INC print(best_hospital(outside_range.geometry.iloc[0])) . q_4.check() . q_4.hint() #q_4.solution() . 5) Which hospital is under the highest demand? . Considering only collisions in the outside_range DataFrame, which hospital is most recommended? . Your answer should be a Python string that exactly matches the name of the hospital returned by the function you created in 4). . . 5) 수요가 가장 많은 병원은? . outside_range DataFrame에서 충돌만 고려한다면 어느 병원을 가장 추천하는가? . 4)에서 생성한 함수가 반환한 병원 이름과 정확히 일치하는 Python 문자열이어야 합니다. . print(best_hospital(outside_range.geometry.iloc[0])) #&gt; CALVARY HOSPITAL INC . highest_demand = outside_range.geometry.apply(best_hospital).value_counts().idxmax highest_demand . q_5.check() . q_5.hint() #q_5.solution() . 6) Where should the city construct new hospitals? . Run the next code cell (without changes) to visualize hospital locations, in addition to collisions that occurred more than 10 kilometers away from the closest hospital. . 다음 코드 셀(변경 없이)을 실행하여 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌 외에도 병원 위치를 시각화합니다. . m_6 = folium.Map(location=[40.7, -74], zoom_start=11) coverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000) folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m_6) HeatMap(data=outside_range[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m_6) folium.LatLngPopup().add_to(m_6) embed_map(m_6, &#39;m_6.html&#39;) . Click anywhere on the map to see a pop-up with the corresponding location in latitude and longitude. . The city of New York reaches out to you for help with deciding locations for two brand new hospitals. They specifically want your help with identifying locations to bring the calculated percentage from step 3) to less than ten percent. Using the map (and without worrying about zoning laws or what potential buildings would have to be removed in order to build the hospitals), can you identify two locations that would help the city accomplish this goal? . Put the proposed latitude and longitude for hospital 1 in lat_1 and long_1, respectively. (Likewise for hospital 2.) . Then, run the rest of the cell as-is to see the effect of the new hospitals. Your answer will be marked correct, if the two new hospitals bring the percentage to less than ten percent. . . 지도의 아무 곳이나 클릭하면 해당 위치의 위도 및 경도 팝업이 표시됩니다. . New York 시는 두 곳의 새로운 병원을 지을 위치를 결정하는 데 도움을 드리기 위해 연락을 드립니다. 그들은 특히 3) 단계에서 계산된 백분율을 10% 미만으로 만들기 위해 위치 식별에 대한 귀하의 도움을 원합니다. 지도를 사용하여(지역 설정법이나 병원을 건설하기 위해 제거해야 할 잠재적 건물에 대해 걱정하지 않고) 도시가 이 목표를 달성하는 데 도움이 될 두 위치를 식별할 수 있습니까? . 병원 1에 대해 제안된 위도와 경도를 각각 lat_1과 long_1에 넣습니다. (병원2도 마찬가지) . 그런 다음 나머지 셀을 그대로 실행하여 새 병원의 효과를 확인하십시오. 두 개의 새로운 병원에서 백분율을 10% 미만으로 낮추면 답이 정답으로 표시됩니다. . lat_1 = 40.6714 long_1 = -73.8492 # Your answer here: proposed location of hospital 2 lat_2 = 40.6702 long_2 = -73.7612 # Do not modify the code below this line try: new_df = pd.DataFrame( {&#39;Latitude&#39;: [lat_1, lat_2], &#39;Longitude&#39;: [long_1, long_2]}) new_gdf = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitude, new_df.Latitude)) new_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} new_gdf = new_gdf.to_crs(epsg=2263) # get new percentage new_coverage = gpd.GeoDataFrame(geometry=new_gdf.geometry).buffer(10000) new_my_union = new_coverage.geometry.unary_union new_outside_range = outside_range.loc[~outside_range[&quot;geometry&quot;].apply(lambda x: new_my_union.contains(x))] new_percentage = round(100*len(new_outside_range)/len(collisions), 2) print(&quot;(NEW) Percentage of collisions more than 10 km away from the closest hospital: {}%&quot;.format(new_percentage)) # Did you help the city to meet its goal? q_6.check() # make the map m = folium.Map(location=[40.7, -74], zoom_start=11) folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m) folium.GeoJson(new_coverage.geometry.to_crs(epsg=4326)).add_to(m) for idx, row in new_gdf.iterrows(): Marker([row[&#39;Latitude&#39;], row[&#39;Longitude&#39;]]).add_to(m) HeatMap(data=new_outside_range[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m) folium.LatLngPopup().add_to(m) display(embed_map(m, &#39;q_6.html&#39;)) except: q_6.hint() . q_6.solution() . Congratulations! . You have just completed the Geospatial Analysis micro-course! Great job! . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://woojaaeon.github.io/jaong/jupyter/python/2022/05/24/Ex5_proximity_analysis.html",
            "relUrl": "/jupyter/python/2022/05/24/Ex5_proximity_analysis.html",
            "date": " • May 24, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Geospatial Analysis exercise - manipulating geospatial data",
            "content": "Introduction . You are a Starbucks big data analyst (that’s a real job!) looking to find the next store into a Starbucks Reserve Roastery. These roasteries are much larger than a typical Starbucks store and have several additional features, including various food and wine options, along with upscale lounge areas. You&#39;ll investigate the demographics of various counties in the state of California, to determine potentially suitable locations. . Before you get started, run the code cell below to set everything up. . import math import pandas as pd import geopandas as gpd #from geopy.geocoders import Nominatim # What you&#39;d normally run from learntools.geospatial.tools import Nominatim # Just for this exercise import folium from folium import Marker from folium.plugins import MarkerCluster from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex4 import * . You&#39;ll use the embed_map() function from the previous exercise to visualize your maps. . 이전 연습의 &#39;embed_map()&#39; 함수를 사용하여 지도를 시각화합니다. . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . Exercises . 1) Geocode the missing locations. . Run the next code cell to create a DataFrame starbucks containing Starbucks locations in the state of California. . 다음 코드 셀을 실행하여 캘리포니아 주에 있는 Starbucks 위치를 포함하는 DataFrame &#39;starbucks&#39;를 만듭니다. . starbucks = pd.read_csv(&quot;../input/geospatial-learn-course-data/starbucks_locations.csv&quot;) starbucks.head() . Most of the stores have known (latitude, longitude) locations. But, all of the locations in the city of Berkeley are missing. . 대부분의 상점은 (위도, 경도) 위치를 알고 있습니다. 그러나 버클리시의 모든 위치가 누락되었습니다. . print(starbucks.isnull().sum()) # View rows with missing locations rows_with_missing = starbucks[starbucks[&quot;City&quot;]==&quot;Berkeley&quot;] rows_with_missing . Use the code cell below to fill in these values with the Nominatim geocoder. . Note that in the tutorial, we used Nominatim() (from geopy.geocoders) to geocode values, and this is what you can use in your own projects outside of this course. . In this exercise, you will use a slightly different function Nominatim() (from learntools.geospatial.tools). This function was imported at the top of the notebook and works identically to the function from GeoPandas. . So, in other words, as long as: . you don&#39;t change the import statements at the top of the notebook, and | you call the geocoding function as geocode() in the code cell below, | . your code will work as intended! . . 아래 코드 셀을 사용하여 Nominatim 지오코더로 이 값을 채우십시오. . 튜토리얼에서 우리는 값을 지오코딩하기 위해 Nominatim()(geopy.geocoders에서)을 사용했으며 이것은 이 과정 이외의 자신의 프로젝트에서 사용할 수 있는 것입니다. . 이 연습에서는 약간 다른 함수 Nominatim()(learntools.geospatial.tools에서)을 사용합니다. 이 기능은 노트북 상단에서 가져온 것으로 GeoPandas의 기능과 동일하게 작동합니다. . 즉, . 노트북 상단의 import 문을 변경하지 않고, | 아래 코드 셀에서 지오코딩 함수를 geocode()로 호출합니다. | . 코드가 의도한 대로 작동합니다! . geolocator = Nominatim(user_agent=&quot;kaggle_learn&quot;) # Your code here def my_geocoder(row): try: point = geolocator.geocode(row).point return pd.Series({&#39;Latitude&#39;: point.latitude, &#39;Longitude&#39;: point.longitude}) except: return None bk_locations = rows_with_missing.apply(lambda x: my_geocoder(x[&#39;Address&#39;]), axis=1) bk_locations # starbucks DataFrame에는 여전히 10개의 누락된 항목이 있습니다. 이 값을 입력하십시오. # starbucks DataFrame에 bk_locations을 추가 starbucks.update(berkeley_locations) starbucks # missimg value에 값이 들어갔는지 확인 rows_with_missing = starbucks[starbucks[&quot;City&quot;]==&quot;Berkeley&quot;] rows_with_missing . q_1.check() . #q_1.solution() #&gt; starbucks에 berkeley_locations의 값을 missimg_value에 업데이트한다. #&gt; starbucks.update(berkeley_locations) . 2) View Berkeley locations. . Let&#39;s take a look at the locations you just found. Visualize the (latitude, longitude) locations in Berkeley in the OpenStreetMap style. . 방금 찾은 위치(버클리 위치)를 살펴보겠습니다. OpenStreetMap 스타일로 버클리의 (위도, 경도) 위치를 시각화합니다. . m_2 = folium.Map(location=[37.88,-122.26], zoom_start=13) # Your code here: Add a marker for each Berkeley location df = starbucks[starbucks[&quot;City&quot;]==&#39;Berkeley&#39;] for idx, row in df.iterrows(): Marker([row[&#39;Latitude&#39;], row[&#39;Longitude&#39;]], popup=row[&#39;Store Name&#39;]).add_to(m_2) # Uncomment to see a hint #q_2.a.hint() # Show the map embed_map(m_2, &#39;q_2.html&#39;) . q_2.a.check() # Uncomment to see our solution (your code may look different!) #q_2.a.solution() . Considering only the five locations in Berkeley, how many of the (latitude, longitude) locations seem potentially correct (are located in the correct city)? . 버클리의 5개 위치만 고려하면, (위도, 경도) 위치 중 몇 개(위도, 경도)가 잠재적으로 올바른 것처럼 보입니까(올바른 도시에 위치)? . A. Solano Ave, Safeway-Berkeley#691, 2224 Shattuck-Berkeley, 2128 Oxford St.,Telegraph &amp; Ashby 5개가 올바른 위치에 있어 보인다. . q_2.b.solution() . 3) Consolidate your data. . Run the code below to load a GeoDataFrame CA_counties containing the name, area (in square kilometers), and a unique id (in the &quot;GEOID&quot; column) for each county in the state of California. The &quot;geometry&quot; column contains a polygon with county boundaries. . 아래 코드를 실행하여 캘리포니아 주의 각 카운티에 대한 이름, 면적(제곱 킬로미터) 및 고유 ID(&quot;GEOID&quot; 열에 있음)가 포함된 GeoDataFrame CA_counties를 로드합니다. &quot;형상&quot; 열에는 카운티 경계가 있는 다각형이 포함되어 있습니다. . CA_counties = gpd.read_file(&quot;../input/geospatial-learn-course-data/CA_county_boundaries/CA_county_boundaries/CA_county_boundaries.shp&quot;) CA_counties.head() . Next, we create three DataFrames: . CA_pop contains an estimate of the population of each county. | CA_high_earners contains the number of households with an income of at least $150,000 per year. | CA_median_age contains the median age for each county. | . . 다음으로 3개의 DataFrame을 생성합니다. . &#39;CA_pop&#39;은 각 카운티의 인구 추정치를 포함합니다. | CA_high_earners에는 연간 소득이 $150,000 이상인 가구 수가 포함됩니다. | &#39;CA_median_age&#39;는 각 카운티의 중위 연령을 포함합니다. | . CA_pop = pd.read_csv(&quot;../input/geospatial-learn-course-data/CA_county_population.csv&quot;, index_col=&quot;GEOID&quot;) CA_high_earners = pd.read_csv(&quot;../input/geospatial-learn-course-data/CA_county_high_earners.csv&quot;, index_col=&quot;GEOID&quot;) CA_median_age = pd.read_csv(&quot;../input/geospatial-learn-course-data/CA_county_median_age.csv&quot;, index_col=&quot;GEOID&quot;) . Use the next code cell to join the CA_counties GeoDataFrame with CA_pop, CA_high_earners, and CA_median_age. . Name the resultant GeoDataFrame CA_stats, and make sure it has 8 columns: &quot;GEOID&quot;, &quot;name&quot;, &quot;area_sqkm&quot;, &quot;geometry&quot;, &quot;population&quot;, &quot;high_earners&quot;, and &quot;median_age&quot;. Also, make sure the CRS is set to {&#39;init&#39;: &#39;epsg:4326&#39;}. . . 다음 코드 셀을 사용하여 CA_counties GeoDataFrame을 CA_pop, CA_high_earners 및 CA_median_age와 결합합니다. . 결과 GeoDataFrame의 이름을 CA_stats로 지정하고 &quot;GEOID&quot;, &quot;name&quot;, &quot;area_sqkm&quot;, &quot;geometry&quot;, &quot;population&quot;, &quot;high_earners&quot; 및 &quot;median_age&quot;의 8개 열이 있는지 확인합니다. 또한 CRS가 {&#39;init&#39;: &#39;epsg:4326&#39;}으로 설정되어 있는지 확인하십시오. . print(CA_pop.head(), &quot; n n&quot;, CA_high_earners.head(), &quot; n n&quot;, CA_median_age.head()) . cols_to_add = CA_pop.join([CA_high_earners, CA_median_age]).reset_index() CA_stats = CA_counties.merge(cols_to_add, on=&quot;GEOID&quot;) CA_stats.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} CA_stats.head() . q_3.check() . q_3.hint() #&gt; pd.DataFrame.join()을 사용하여 3개의 DataFrame을 함께 결합하는 것으로 시작합니다. #&gt; 그런 다음 이 새 DataFrame을 gpd.GeoDataFrame.merge()를 사용하여 CA_countiesGeoDataFrame에 추가합니다. #q_3.solution() . Now that we have all of the data in one place, it&#39;s much easier to calculate statistics that use a combination of columns. Run the next code cell to create a &quot;density&quot; column with the population density. . 이제 모든 데이터가 한 곳에 있으므로 열 조합을 사용하는 통계를 훨씬 쉽게 계산할 수 있습니다. 다음 코드 셀을 실행하여 인구 밀도가 있는 &quot;밀도&quot; 열을 만듭니다. . CA_stats[&quot;density&quot;] = CA_stats[&quot;population&quot;] / CA_stats[&quot;area_sqkm&quot;] CA_stats.head() . 4) Which counties look promising? . Collapsing all of the information into a single GeoDataFrame also makes it much easier to select counties that meet specific criteria. . Use the next code cell to create a GeoDataFrame sel_counties that contains a subset of the rows (and all of the columns) from the CA_stats GeoDataFrame. In particular, you should select counties where: . there are at least 100,000 households making $150,000 per year, | the median age is less than 38.5, and | the density of inhabitants is at least 285 (per square kilometer). | . Additionally, selected counties should satisfy at least one of the following criteria: . there are at least 500,000 households making $150,000 per year, | the median age is less than 35.5, or | the density of inhabitants is at least 1400 (per square kilometer). | . . 모든 정보를 단일 GeoDataFrame으로 축소하면 특정 기준을 충족하는 카운티를 훨씬 더 쉽게 선택할 수 있습니다. . 다음 코드 셀을 사용하여 CA_stats GeoDataFrame에서 행(및 모든 열)의 하위 집합을 포함하는 GeoDataFrame sel_counties를 만듭니다. 특히 다음과 같은 카운티를 선택해야 합니다. . 연간 소득이 15만 달러 이상인 가구가 10만 가구 이상인 경우 | 중위 연령이 38.5세 미만이고, | 주민 밀도는 최소 285명(제곱 킬로미터당)입니다. | . 또한 선택한 카운티는 다음 기준 중 하나 이상을 충족해야 합니다. . 연간 $150,000를 버는 최소 500,000 가구가 있고, | 중위 연령이 35.5세 미만이거나 | 주민 밀도는 최소 1400명(제곱 킬로미터당)입니다. | . # CA_stats GeoDataFrame에서 행(및 모든 열)의 하위 집합을 포함하는 GeoDataFrame sel_counties를 만듭니다. sel_counties = CA_stats[( # 다음과 같은 카운티를 선택 (CA_stats.high_earners &gt; 100000) &amp; # 연간 소득이 15만 달러 이상인 가구가 10만 가구 이상인 경우 (CA_stats.median_age &lt; 38.5) &amp; # 중위 연령이 38.5세 미만 (CA_stats.density &gt; 285) &amp; # 주민 밀도는 최소 285명 # 기준 중 하나 이상을 충족 ((CA_stats.median_age &lt; 35.5) | # 연간 $150,000를 버는 최소 500,000 가구 (CA_stats.density &gt; 1400) | # 중위 연령이 35.5세 미만 (CA_stats.high_earners &gt; 500000)))] # 주민 밀도는 최소 1400명 sel_counties . q_4.check() . q_4.hint() #q_4.solution() . 5) How many stores did you identify? . When looking for the next Starbucks Reserve Roastery location, you&#39;d like to consider all of the stores within the counties that you selected. So, how many stores are within the selected counties? . To prepare to answer this question, run the next code cell to create a GeoDataFrame starbucks_gdf with all of the starbucks locations. . . 다음 Starbucks Reserve Roastery 위치를 찾을 때 선택한 카운티 내의 모든 매장을 고려하고 싶습니다. 그렇다면 선택한 카운티 내에 몇 개의 매장이 있습니까? . 이 질문에 답할 준비를 하려면 다음 코드 셀을 실행하여 모든 스타벅스 위치가 포함된 GeoDataFrame starbucks_gdf를 만듭니다. . starbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude)) starbucks_gdf.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} starbucks_gdf . So, how many stores are in the counties you selected? . 그렇다면 선택한 카운티에는 몇 개의 매장이 있습니까? . A. 1043개의 매장이 있습니다. . df_sjoin = gpd.sjoin(starbucks_gdf, sel_counties) num_stores = len(df_sjoin) num_stores . q_5.check() . q_5.hint() #q_5.solution() . 6) Visualize the store locations. . Create a map that shows the locations of the stores that you identified in the previous question. . 이전 질문에서 식별한 상점의 위치를 보여주는 지도를 만드십시오. . m_6 = folium.Map(location=[37,-120], zoom_start=6) # Your code here: show selected store locations mc = MarkerCluster() df_sjoin = gpd.sjoin(starbucks_gdf, sel_counties) for idx, row in df_sjoin.iterrows(): if not math.isnan(row[&#39;Longitude&#39;]) and not math.isnan(row[&#39;Latitude&#39;]): mc.add_child(folium.Marker([row[&#39;Latitude&#39;], row[&#39;Longitude&#39;]])) m_6.add_child(mc) # Uncomment to see a hint q_6.hint() # Show the map embed_map(m_6, &#39;q_6.html&#39;) . q_6.check() # Uncomment to see our solution (your code may look different!) q_6.solution() . Keep going . Learn about how proximity analysis can help you to understand the relationships between points on a map. . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://woojaaeon.github.io/jaong/jupyter/python/2022/05/24/Ex4_manipulating_geospatial_data.html",
            "relUrl": "/jupyter/python/2022/05/24/Ex4_manipulating_geospatial_data.html",
            "date": " • May 24, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Geospatial Analysis exercise - interactive maps",
            "content": "Introduction . You are an urban safety planner in Japan, and you are analyzing which areas of Japan need extra earthquake reinforcement. Which areas are both high in population density and prone to earthquakes? . Before you get started, run the code cell below to set everything up. . import pandas as pd import geopandas as gpd import folium from folium import Choropleth from folium.plugins import HeatMap from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex3 import * . We define a function embed_map() for displaying interactive maps. It accepts two arguments: the variable containing the map, and the name of the HTML file where the map will be saved. . This function ensures that the maps are visible in all web browsers. . 대화형 지도를 표시하기 위해 &#39;embed_map()&#39; 함수를 정의합니다. 지도를 포함하는 변수와 지도가 저장될 HTML 파일의 이름이라는 두 가지 인수를 허용합니다. . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . Exercises . 1) Do earthquakes coincide with plate boundaries? . Run the code cell below to create a DataFrame plate_boundaries that shows global plate boundaries. The &quot;coordinates&quot; column is a list of (latitude, longitude) locations along the boundaries. . 1) 지진은 판 경계와 일치합니까? . 아래 코드 셀을 실행하여 전역 플레이트 경계를 표시하는 DataFrame plate_boundaries를 만듭니다. &quot;좌표&quot; 열은 경계를 따라 (위도, 경도) 위치의 목록입니다. . plate_boundaries = gpd.read_file(&quot;../input/geospatial-learn-course-data/Plate_Boundaries/Plate_Boundaries/Plate_Boundaries.shp&quot;) plate_boundaries[&#39;coordinates&#39;] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis=&#39;columns&#39;) plate_boundaries.drop(&#39;geometry&#39;, axis=1, inplace=True) plate_boundaries.head() . Next, run the code cell below without changes to load the historical earthquake data into a DataFrame earthquakes. . 다음으로 아래의 코드 셀을 변경 없이 실행하여 과거 지진 데이터를 DataFrame &#39;earthquakes&#39;에 로드합니다. . earthquakes = pd.read_csv(&quot;../input/geospatial-learn-course-data/earthquakes1970-2014.csv&quot;, parse_dates=[&quot;DateTime&quot;]) earthquakes.head() . The code cell below visualizes the plate boundaries on a map. Use all of the earthquake data to add a heatmap to the same map, to determine whether earthquakes coincide with plate boundaries. . 아래 코드 셀은 지도에서 판 경계를 시각화합니다. 모든 지진 데이터를 사용하여 동일한 지도에 히트맵을 추가하여 지진이 판 경계와 일치하는지 확인합니다. . m_1 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) for i in range(len(plate_boundaries)): folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color=&#39;black&#39;).add_to(m_1) # Your code here: Add a heatmap to the map HeatMap(data=earthquakes[[&#39;Latitude&#39;, &#39;Longitude&#39;]], radius=12).add_to(m_1) # Uncomment to see a hint #q_1.a.hint() # Show the map embed_map(m_1, &#39;q_1.html&#39;) . q_1.a.check() # Uncomment to see our solution (your code may look different!) #q_1.a.solution() . So, given the map above, do earthquakes coincide with plate boundaries? . 따라서 위의 지도에서 지진이 판 경계와 일치합니까? . A. 지진과 판 경계가 일치합니다. . q_1.b.solution() . 2) Is there a relationship between earthquake depth and proximity to a plate boundary in Japan? . You recently read that the depth of earthquakes tells us important information about the structure of the earth. You&#39;re interested to see if there are any intereresting global patterns, and you&#39;d also like to understand how depth varies in Japan. . 2) 일본에서 지진 깊이와 판 경계와의 근접성 사이에 관계가 있습니까? . 최근에 지진의 깊이가 지구의 구조에 대해 알려준다는 기사를 읽었습니다. 흥미로운 글로벌 패턴이 있는지 확인하고 싶고 일본의 깊이가 어떻게 다른지 이해하고 싶습니다. . m_2 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) for i in range(len(plate_boundaries)): folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color=&#39;black&#39;).add_to(m_2) # Your code here: Add a map to visualize earthquake depth def color_producer(val): if val &lt; 50: return &#39;forestgreen&#39; elif val &lt; 100: return &#39;darkorange&#39; else: return &#39;darkred&#39; for i in range(0,len(earthquakes)): folium.Circle( location=[earthquakes.iloc[i][&#39;Latitude&#39;], earthquakes.iloc[i][&#39;Longitude&#39;]], radius=2000, color=color_producer(earthquakes.iloc[i][&#39;Depth&#39;])).add_to(m_2) # View the map embed_map(m_2, &#39;q_2.html&#39;) . q_2.a.hint() . q_2.a.check() # Uncomment to see our solution (your code may look different!) q_2.a.solution() . Can you detect a relationship between proximity to a plate boundary and earthquake depth? Does this pattern hold globally? In Japan? . 판 경계와의 근접성과 지진 깊이 사이의 관계를 감지할 수 있습니까? 이 패턴이 전 세계적으로 유지됩니까? 일본에서? . A. 일본 북부쪽에서는 판 경계에 가까운 지진이 더 얕아지기 때문에 판 경계와의 근접성과 지진 깊이 사이의 관계를 감지할 수 있습니다. 이 패턴은 남미 서부 해안과 같은 다른 지역에서도 반복됩니다. 하지만 중국, 몽골, 러시아에는 적용되지 않기에 모든 곳에서 적용되는 것은 아닙니다. . q_2.b.solution() . 3) Which prefectures have high population density? . Run the next code cell (without changes) to create a GeoDataFrame prefectures that contains the geographical boundaries of Japanese prefectures. . 3) 인구 밀도가 높은 현은 어디입니까? . 다음 코드 셀을 변경 없이 실행하여 일본 현의 지리적 경계를 포함하는 GeoDataFrame &#39;현&#39;을 만듭니다. . prefectures = gpd.read_file(&quot;../input/geospatial-learn-course-data/japan-prefecture-boundaries/japan-prefecture-boundaries/japan-prefecture-boundaries.shp&quot;) prefectures.set_index(&#39;prefecture&#39;, inplace=True) prefectures.head() . The next code cell creates a DataFrame stats containing the population, area (in square kilometers), and population density (per square kilometer) for each Japanese prefecture. Run the code cell without changes. . 다음 코드 셀은 각 일본 현에 대한 인구, 면적(제곱 킬로미터 단위) 및 인구 밀도(제곱 킬로미터당)를 포함하는 DataFrame &#39;통계&#39;를 생성합니다. 변경 없이 코드 셀을 실행합니다. . population = pd.read_csv(&quot;../input/geospatial-learn-course-data/japan-prefecture-population.csv&quot;) population.set_index(&#39;prefecture&#39;, inplace=True) # Calculate area (in square kilometers) of each prefecture area_sqkm = pd.Series(prefectures.geometry.to_crs(epsg=32654).area / 10**6, name=&#39;area_sqkm&#39;) stats = population.join(area_sqkm) # Add density (per square kilometer) of each prefecture stats[&#39;density&#39;] = stats[&quot;population&quot;] / stats[&quot;area_sqkm&quot;] stats.head() . Use the next code cell to create a choropleth map to visualize population density. . 다음 코드 셀을 사용하여 등치 지도를 만들어 인구 밀도를 시각화합니다. . m_3 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) # Your code here: create a choropleth map to visualize population density Choropleth(geo_data=prefectures[&#39;geometry&#39;].__geo_interface__, data=stats[&#39;density&#39;], key_on=&quot;feature.id&quot;, fill_color=&#39;YlGnBu&#39;, legend_name=&#39;Population density&#39; ).add_to(m_3) # Uncomment to see a hint q_3.a.hint() # View the map embed_map(m_3, &#39;q_3.html&#39;) . q_3.a.check() # Uncomment to see our solution (your code may look different!) #q_3.a.solution() . Which three prefectures have relatively higher density than the others? Are they spread throughout the country, or all located in roughly the same geographical region? (If you&#39;re unfamiliar with Japanese geography, you might find this map useful to answer the questions.) . 어느 3개 현이 다른 현보다 밀도가 더 높습니까? 그들은 전국에 퍼져 있습니까, 아니면 모두 거의 같은 지리적 지역에 있습니까? (일본 지리에 익숙하지 않은 경우 질문에 답하는 데 도움이 될 수 있습니다.) . A. tokyo, osaka, kanagawa가 인구 밀도가 가장 높습니다.모두 일본 중부에 위치하고 있으며 tokyo와 kanagawa이 근처에 위치하고 있습니다. . q_3.b.solution() . 4) Which high-density prefecture is prone to high-magnitude earthquakes? . Create a map to suggest one prefecture that might benefit from earthquake reinforcement. Your map should visualize both density and earthquake magnitude. . 4) 규모가 큰 지진이 발생하기 쉬운 고밀도 현은? . 지진 보강의 혜택을 받을 수 있는 한 현을 제안하는 지도를 만듭니다. 지도는 밀도와 지진 규모를 모두 시각화해야 합니다. . m_4 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) # Your code here: create a map def color_producer(magnitude): if magnitude &gt; 6.5: return &#39;red&#39; else: return &#39;green&#39; Choropleth( geo_data=prefectures[&#39;geometry&#39;].__geo_interface__, data=stats[&#39;density&#39;], key_on=&quot;feature.id&quot;, fill_color=&#39;YlGnBu&#39;, legend_name=&#39;Population density&#39;).add_to(m_4) for i in range(0,len(earthquakes)): folium.Circle( location=[earthquakes.iloc[i][&#39;Latitude&#39;], earthquakes.iloc[i][&#39;Longitude&#39;]], popup=(&quot;{} ({})&quot;).format( earthquakes.iloc[i][&#39;Magnitude&#39;], earthquakes.iloc[i][&#39;DateTime&#39;].year), radius=earthquakes.iloc[i][&#39;Magnitude&#39;]**5.5, color=color_producer(earthquakes.iloc[i][&#39;Magnitude&#39;])).add_to(m_4) # Uncomment to see a hint q_4.a.hint() # View the map embed_map(m_4, &#39;q_4.html&#39;) . q_4.a.check() # Uncomment to see our solution (your code may look different!) q_4.a.solution() . Which prefecture do you recommend for extra earthquake reinforcement? . 추가 지진 보강을 위해 어느 현을 추천합니까? . A. tokyo는 단연 가장 인구 밀도가 높은 현이며 지진이 많이 발생한 지역입니다. osaka는 상대적으로 인구 밀도가 낮지만 도쿄 근처보다 상대적으로 강한 지진이 발생했습니다. 그리고 Kanagawa는 바다와 인접하여 지진으로 인해 쓰나미가 발생할 수 있는 위험이 있습니다. 따라서 추가 지진 보강은 먼저 인구밀도가 높은 tokyo에 추가 인명피해를 막기 위해 보강을 강하게 해야한다고 생각합니다. kanagawa에는 바다와 인접하기 때문에 방파제등과 같은 쓰나미를 대비할 수 있는 것을 설치해야 합니다. . q_4.b.solution() . Keep going . Learn how to convert names of places to geographic coordinates with geocoding. You&#39;ll also explore special ways to join information from multiple GeoDataFrames. . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://woojaaeon.github.io/jaong/jupyter/python/2022/05/20/Ex3_interactive_maps.html",
            "relUrl": "/jupyter/python/2022/05/20/Ex3_interactive_maps.html",
            "date": " • May 20, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Geospatial Analysis exercise - coordinate reference systems",
            "content": "Introduction . You are a bird conservation expert and want to understand migration patterns of purple martins. In your research, you discover that these birds typically spend the summer breeding season in the eastern United States, and then migrate to South America for the winter. But since this bird is under threat of endangerment, you&#39;d like to take a closer look at the locations that these birds are more likely to visit. . There are several protected areas in South America, which operate under special regulations to ensure that species that migrate (or live) there have the best opportunity to thrive. You&#39;d like to know if purple martins tend to visit these areas. To answer this question, you&#39;ll use some recently collected data that tracks the year-round location of eleven different birds. . Before you get started, run the code cell below to set everything up. . import pandas as pd import geopandas as gpd from shapely.geometry import LineString from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex2 import * . Exercises . 1) Load the data. . Run the next code cell (without changes) to load the GPS data into a pandas DataFrame birds_df. . birds_df = pd.read_csv(&quot;../input/geospatial-learn-course-data/purple_martin.csv&quot;, parse_dates=[&#39;timestamp&#39;]) print(&quot;There are {} different birds in the dataset.&quot;.format(birds_df[&quot;tag-local-identifier&quot;].nunique())) birds_df.head() . There are 11 birds in the dataset, where each bird is identified by a unique value in the &quot;tag-local-identifier&quot; column. Each bird has several measurements, collected at different times of the year. . Use the next code cell to create a GeoDataFrame birds. . birds should have all of the columns from birds_df, along with a &quot;geometry&quot; column that contains Point objects with (longitude, latitude) locations. | Set the CRS of birds to {&#39;init&#39;: &#39;epsg:4326&#39;}. | . 데이터 세트에는 11마리의 새가 있으며 각 새는 &quot;tag-local-identifier&quot; 열의 고유한 값으로 식별됩니다. 각 새는 일년 중 다른시기에 수집 된 여러 측정 값을 가지고 있습니다. . 다음 코드 셀을 사용하여 GeoDataFrame &#39;새&#39;를 만듭니다. . birds에는 (경도, 위도) 위치가 있는 Point 개체를 포함하는 &quot;기하학&quot; 열과 함께 birds_df의 모든 열이 있어야 합니다. | birds의 CRS를 {&#39;init&#39;: &#39;epsg:4326&#39;}으로 설정합니다. | . birds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df[&#39;location-long&#39;], birds_df[&quot;location-lat&quot;])) # Your code here: Set the CRS to {&#39;init&#39;: &#39;epsg:4326&#39;} birds.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} # Check your answer q_1.check() . birds . #q_1.hint() #q_1.solution() . 2) Plot the data. . Next, we load in the &#39;naturalearth_lowres&#39; dataset from GeoPandas, and set americas to a GeoDataFrame containing the boundaries of all countries in the Americas (both North and South America). Run the next code cell without changes. . 다음으로 GeoPandas에서 &#39;naturalearth_lowres 데이터 세트를 로드하고 americas를 미주(북미와 남미 모두)의 모든 국가 경계를 포함하는 GeoDataFrame으로 설정합니다. 변경 없이 다음 코드 셀을 실행합니다. . world = gpd.read_file(gpd.datasets.get_path(&#39;naturalearth_lowres&#39;)) americas = world.loc[world[&#39;continent&#39;].isin([&#39;North America&#39;, &#39;South America&#39;])] americas.head() . Use the next code cell to create a single plot that shows both: (1) the country boundaries in the americas GeoDataFrame, and (2) all of the points in the birds_gdf GeoDataFrame. . Don&#39;t worry about any special styling here; just create a preliminary plot, as a quick sanity check that all of the data was loaded properly. In particular, you don&#39;t have to worry about color-coding the points to differentiate between birds, and you don&#39;t have to differentiate starting points from ending points. We&#39;ll do that in the next part of the exercise. . 다음 코드 셀을 사용하여 (1) americas GeoDataFrame의 국가 경계와 (2) birds_gdf GeoDataFrame의 모든 점을 모두 표시하는 단일 플롯을 만듭니다. . 특별한 스타일링에 대해 걱정하지 마십시오. 모든 데이터가 제대로 로드되었는지 신속하게 확인하기 위해 예비 플롯을 생성하기만 하면 됩니다. 특히 새를 구분하기 위해 포인트를 색으로 구분할 필요가 없고, 시작점과 끝점을 구분할 필요가 없습니다. 연습의 다음 부분에서 그렇게 할 것입니다. . ax = americas.plot(figsize=(8,8), color=&#39;whitesmoke&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;black&#39;) ax birds.plot(color=&#39;red&#39;,markersize=10,ax=ax) # Uncomment to see a hint #q_2.hint() . q_2.check() # Uncomment to see our solution (your code may look different!) ##q_2.solution() . 3) Where does each bird start and end its journey? (Part 1) . Now, we&#39;re ready to look more closely at each bird&#39;s path. Run the next code cell to create two GeoDataFrames: . path_gdf contains LineString objects that show the path of each bird. It uses the LineString() method to create a LineString object from a list of Point objects. | start_gdf contains the starting points for each bird. | . path_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: LineString(x)).reset_index() path_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry) path_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # GeoDataFrame showing starting point for each bird start_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: x[0]).reset_index() start_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry) start_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # Show first five rows of GeoDataFrame start_gdf.head() . Use the next code cell to create a GeoDataFrame end_gdf containing the final location of each bird. . The format should be identical to that of start_gdf, with two columns (&quot;tag-local-identifier&quot; and &quot;geometry&quot;), where the &quot;geometry&quot; column contains Point objects. | Set the CRS of end_gdf to {&#39;init&#39;: &#39;epsg:4326&#39;}. | . 다음 코드 셀을 사용하여 각 새의 최종 위치를 포함하는 GeoDataFrame end_gdf를 만듭니다. . 형식은 두 개의 열(&quot;tag-local-identifier&quot; 및 &quot;geometry&quot;)이 있는 start_gdf의 형식과 동일해야 합니다. 여기서 &quot;geometry&quot; 열은 Point 개체를 포함합니다. | end_gdf의 CRS를 {&#39;init&#39;: &#39;epsg:4326&#39;}으로 설정합니다. | . end_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: x[-1]).reset_index() end_df . end_gdf = gpd.GeoDataFrame(end_df, geometry=end_df.geometry) end_gdf.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} end_gdf . q_3.check() . #q_3.hint() #q_3.solution() . 4) Where does each bird start and end its journey? (Part 2) . Use the GeoDataFrames from the question above (path_gdf, start_gdf, and end_gdf) to visualize the paths of all birds on a single map. You may also want to use the americas GeoDataFrame. . 위 질문의 GeoDataFrames(path_gdf, start_gdf, end_gdf)를 사용하여 단일 지도에서 모든 새의 경로를 시각화하세요. americas GeoDataFrame을 사용할 수도 있습니다. . ax = americas.plot(figsize=(10, 10), color=&#39;whitesmoke&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;black&#39;) start_gdf.plot(color=&#39;red&#39;, markersize=20, ax=ax) path_gdf.plot(color=&#39;green&#39;,linestyle=&#39;:&#39;, linewidth=1, zorder=1, ax=ax) end_gdf.plot(color=&#39;blue&#39;, markersize=20, ax=ax) # Uncomment to see a hint #q_4.hint() . q_4.check() # Uncomment to see our solution (your code may look different!) #q_4.solution() . 5) Where are the protected areas in South America? (Part 1) . It looks like all of the birds end up somewhere in South America. But are they going to protected areas? . In the next code cell, you&#39;ll create a GeoDataFrame protected_areas containing the locations of all of the protected areas in South America. The corresponding shapefile is located at filepath protected_filepath. . 다음 코드 셀에서는 남미의 모든 보호 지역 위치를 포함하는 GeoDataFrame protected_areas를 생성합니다. 해당 shapefile은 파일 경로 protected_filepath에 있습니다. . protected_filepath = &quot;../input/geospatial-learn-course-data/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp&quot; # Your code here protected_areas = gpd.read_file(protected_filepath) # Check your answer q_5.check() . #q_5.hint() #q_5.solution() . 6) Where are the protected areas in South America? (Part 2) . Create a plot that uses the protected_areas GeoDataFrame to show the locations of the protected areas in South America. (You&#39;ll notice that some protected areas are on land, while others are in marine waters.) . &#39;protected_areas&#39; GeoDataFrame을 사용하여 남아메리카의 보호 지역 위치를 표시하는 플롯을 만듭니다. (일부 보호 구역은 육지에 있고 다른 보호 구역은 바다에 있음을 알 수 있습니다.) . south_america = americas.loc[americas[&#39;continent&#39;]==&#39;South America&#39;] # Your code here: plot protected areas in South America ax = south_america.plot(figsize=(10, 10), color=&#39;whitesmoke&#39;, edgecolor=&#39;black&#39;) protected_areas.plot(color=&#39;red&#39;, markersize=20, ax=ax, alpha = 0.5) # Uncomment to see a hint #q_6.hint() . q_6.check() # Uncomment to see our solution (your code may look different!) #q_6.solution() . 7) What percentage of South America is protected? . You&#39;re interested in determining what percentage of South America is protected, so that you know how much of South America is suitable for the birds. . As a first step, you calculate the total area of all protected lands in South America (not including marine area). To do this, you use the &quot;REP_AREA&quot; and &quot;REP_M_AREA&quot; columns, which contain the total area and total marine area, respectively, in square kilometers. . Run the code cell below without changes. . 남아메리카의 몇 퍼센트가 보호되는지 확인하여 남아메리카의 어느 부분이 새에게 적합한지 알고 싶습니다. . 첫 번째 단계로 남아메리카의 모든 보호 구역(해양 지역 제외)의 총 면적을 계산합니다. 이렇게 하려면 총 면적과 총 해양 면적을 각각 평방 킬로미터 단위로 포함하는 &quot;REP_AREA&quot; 및 &quot;REP_M_AREA&quot; 열을 사용합니다. . 변경 없이 아래 코드 셀을 실행합니다. . P_Area = sum(protected_areas[&#39;REP_AREA&#39;]-protected_areas[&#39;REP_M_AREA&#39;]) print(&quot;South America has {} square kilometers of protected areas.&quot;.format(P_Area)) . Then, to finish the calculation, you&#39;ll use the south_america GeoDataFrame. . 그런 다음 계산을 완료하기 위해 south_america GeoDataFrame을 사용합니다. . south_america.head() . Calculate the total area of South America by following these steps: . Calculate the area of each country using the area attribute of each polygon (with EPSG 3035 as the CRS), and add up the results. The calculated area will be in units of square meters. | Convert your answer to have units of square kilometeters. | . 다음 단계에 따라 남아메리카의 총 면적을 계산합니다. . 각 폴리곤(CRS로 EPSG 3035 사용)의 &#39;area&#39; 속성을 사용하여 각 국가의 면적을 계산하고 결과를 합산합니다. 계산된 면적은 평방 미터 단위입니다. | 답을 제곱킬로미터 단위로 변환합니다. | . 참고 링크 : http://mwultong.blogspot.com/2008/01/km2-km2-m2-calc.html . south_america_crs = south_america.geometry.to_crs(epsg=3035).area # 평방미터(m**2) sum_area = sum(south_america_crs) sum_area # 제곱킬로미터(km**2)로 변환(m2 / 10**6) totalArea = sum_area / 10**6 totalArea # Check your answer q_7.check() . #q_7.hint() #q_7.solution() . Run the code cell below to calculate the percentage of South America that is protected. . percentage_protected = P_Area/totalArea print(&#39;Approximately {}% of South America is protected.&#39;.format(round(percentage_protected*100, 2))) . 8) Where are the birds in South America? . So, are the birds in protected areas? . Create a plot that shows for all birds, all of the locations where they were discovered in South America. Also plot the locations of all protected areas in South America. . To exclude protected areas that are purely marine areas (with no land component), you can use the &quot;MARINE&quot; column (and plot only the rows in protected_areas[protected_areas[&#39;MARINE&#39;]!=&#39;2&#39;], instead of every row in the protected_areas GeoDataFrame). . 모든 새, 남미에서 발견된 모든 위치를 보여주는 플롯을 만듭니다. 또한 남아메리카의 모든 보호 지역의 위치를 표시합니다. . 순수한 해양 지역(토지 구성요소 없음)인 보호 지역을 제외하려면 &quot;MARINE&quot; 열을 사용할 수 있습니다(그리고 protected_areas[protected_areas[&#39;MARINE&#39;]!=&#39;2&#39;] protected_areas GeoDataFrame의 모든 행). . ax = south_america.plot(figsize=(8,8), color=&#39;whitesmoke&#39;, edgecolor=&#39;black&#39;) # 남아메리카의 모든 보호 지역의 위치를 표시 birds[birds.geometry.y &lt; 0].plot(ax=ax, color=&#39;red&#39;, alpha=0.5, markersize=10, zorder=2) # 순수한 해양 지역(토지 구성요소 없음)인 보호 지역을 제외 protected_areas[protected_areas[&#39;MARINE&#39;]!=&#39;2&#39;].plot(ax=ax, alpha=0.5, zorder=1) # Uncomment to see a hint #q_8.hint() . q_8.check() # Uncomment to see our solution (your code may look different!) #q_8.solution() . Keep going . Create stunning interactive maps with your geospatial data. . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://woojaaeon.github.io/jaong/jupyter/python/2022/05/20/Ex2_coordinate_reference_systems.html",
            "relUrl": "/jupyter/python/2022/05/20/Ex2_coordinate_reference_systems.html",
            "date": " • May 20, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Geospatial Analysis exercise - your first map",
            "content": "Introduction . Kiva.org is an online crowdfunding platform extending financial services to poor people around the world. Kiva lenders have provided over $1 billion dollars in loans to over 2 million people. . Kiva reaches some of the most remote places in the world through their global network of &quot;Field Partners&quot;. These partners are local organizations working in communities to vet borrowers, provide services, and administer loans. . In this exercise, you&#39;ll investigate Kiva loans in the Philippines. Can you identify regions that might be outside of Kiva&#39;s current network, in order to identify opportunities for recruiting new Field Partners? . To get started, run the code cell below to set up our feedback system. . import geopandas as gpd from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex1 import * . 1) Get the data. . Use the next cell to load the shapefile located at loans_filepath to create a GeoDataFrame world_loans. . 다음 셀을 사용하여 loans_filepath에 있는 shapefile을 로드하여 GeoDataFrame world_loans를 생성합니다. . loans_filepath = &quot;../input/geospatial-learn-course-data/kiva_loans/kiva_loans/kiva_loans.shp&quot; # Your code here: Load the data world_loans = gpd.read_file(loans_filepath) # Check your answer q_1.check() # Uncomment to view the first five rows of the data #world_loans.head() . #q_1.hint() #q_1.solution() . 2) Plot the data. . Run the next code cell without changes to load a GeoDataFrame world containing country boundaries. . 변경 없이 다음 코드 셀을 실행하여 국가 경계가 포함된 GeoDataFrame &#39;world&#39;를 로드합니다. . world_filepath = gpd.datasets.get_path(&#39;naturalearth_lowres&#39;) world = gpd.read_file(world_filepath) world.head() . Use the world and world_loans GeoDataFrames to visualize Kiva loan locations across the world. . world 및 world_loans GeoDataFrames를 사용하여 전 세계의 Kiva 대출 위치를 시각화합니다. . ax = world.plot(figsize=(20,20), color=&#39;none&#39;, edgecolor=&#39;gainsboro&#39;) world_loans.plot(color=&#39;maroon&#39;, markersize=2,ax=ax) # Uncomment to see a hint #q_2.hint() . q_2.check() # Uncomment to see our solution (your code may look different!) #q_2.solution() . 3) Select loans based in the Philippines. . Next, you&#39;ll focus on loans that are based in the Philippines. Use the next code cell to create a GeoDataFrame PHL_loans which contains all rows from world_loans with loans that are based in the Philippines. . 다음으로 필리핀에 기반을 둔 대출에 중점을 둘 것입니다. 다음 코드 셀을 사용하여 필리핀에 기반을 둔 대출이 있는 world_loans의 모든 행을 포함하는 GeoDataFrame PHL_loans를 만듭니다. . PHL_loans = world_loans.loc[world_loans.country==&quot;Philippines&quot;].copy() # Check your answer q_3.check() . #q_3.hint() #q_3.solution() . 4) Understand loans in the Philippines. . Run the next code cell without changes to load a GeoDataFrame PHL containing boundaries for all islands in the Philippines. . 필리핀의 모든 섬에 대한 경계를 포함하는 GeoDataFrame PHL을 로드하려면 변경 없이 다음 코드 셀을 실행하십시오. . gpd.io.file.fiona.drvsupport.supported_drivers[&#39;KML&#39;] = &#39;rw&#39; PHL = gpd.read_file(&quot;../input/geospatial-learn-course-data/Philippines_AL258.kml&quot;, driver=&#39;KML&#39;) PHL.head() . Use the PHL and PHL_loans GeoDataFrames to visualize loans in the Philippines. . ax = PHL.plot(figsize=(10,10), color=&#39;none&#39;, edgecolor=&#39;gainsboro&#39;) PHL_loans.plot(color=&#39;maroon&#39;, markersize=2,ax=ax) # Uncomment to see a hint #q_4.a.hint() . q_4.a.check() # Uncomment to see our solution (your code may look different!) #q_4.a.solution() . Can you identify any islands where it might be useful to recruit new Field Partners? Do any islands currently look outside of Kiva&#39;s reach? . You might find this map useful to answer the question. . q_4.b.solution() . Keep going . Continue to learn about coordinate reference systems. . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://woojaaeon.github.io/jaong/jupyter/python/2022/05/20/Ex1_your_first_map.html",
            "relUrl": "/jupyter/python/2022/05/20/Ex1_your_first_map.html",
            "date": " • May 20, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Visualization_with_Matplotlib",
            "content": "import matplotlib import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(&quot;ignore&quot;) plt.rcParams[&#39;figure.figsize&#39;]=(5,3) # 그래프 크기 조정 . &#44592;&#48376; &#44536;&#47000;&#54532; &#44536;&#47532;&#44592; . 데이터 몇 개로 plot 함수를 호출한 다음, show 함수를 호출해주면 간단히 그래프를 그려볼 수 있습니다! . plot 함수에 단일 배열의 데이터가 주어진다면, 수직 축의 좌표로서 이를 사용하게 되며, 각 데이터의 배열상 색인(인덱스)을 수평 좌표로서 사용합니다. 두 개의 배열을 넣어줄 수도 있습니다: 그러면, 하나는 x 축에 대한것이며, 다른 하나는 y 축에 대한것이 됩니다: . plt.plot([1, 2, 4, 9, 5, 3]) plt.show() . 같은 그림을 object oriented API를 이용해 그려보겠습니다. . object oriented API는 그래프의 각 부분을 객체로 지정하고 그리는 것으로, 다음과 같은 패턴을 가지고 있습니다. . # 1. `도화지(Figure: fig)`를 깔고 그래프를 `그릴 구역(Axes: ax)`을 정의합니다. fig, ax = plt.subplots() # 2. ax 위에 그래프를 그립니다. ax.plot([1, 2, 4, 9, 5, 3]) # 3. 그래프를 화면에 출력합니다. plt.show() . [&lt;matplotlib.lines.Line2D at 0x1dc218ee670&gt;] . 이번에는 수학적인 함수를 그려보겠습니다. . NumPy의 linespace 함수를 사용하여 -2 ~ 2 범위에 속하는 500개의 부동소수로 구성된 x 배열을 생성합니다. 그 다음 x의 각 값의 거듭제곱된 값을 포함하는 y 배열을 생성합니다 . 또한, 타이틀과 x 및 y축에 대한 라벨, 그리고 모눈자를 추가적으로 그려보겠습니다. . import numpy as np x = np.linspace(-2, 2, 500) y = x**2 plt.plot(x, y) plt.title(&quot;Square function&quot;) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y = x**2&quot;) plt.grid(True) plt.show() . object-oriented API는 축 이름과 같은 설정 명령어가 pyplot과 다소 다릅니다. | 대체로 축 이름(label), 범위(limits) 등을 지정하는 명령어는 set_대상(), 거꾸로 그래프에서 설정값을 가져오는 명령어는 get_대상()으로 통일되어 있습니다. | . fig, ax = plt.subplots() ax.plot(x,y) ax.set_title(&quot;Square function&quot;) ax.set_xlabel(&quot;x&quot;) ax.set_ylabel(&quot;y = x**2&quot;) ax.grid(True) plt.show() . &#44536;&#47000;&#54532; &#49828;&#53440;&#51068;&#44284; &#49353;&#49345; . 선의 스타일과 색상을 바꿀 수 있습니다. 예를 들어서 &quot;g--&quot;는 &quot;초록색 파선&quot;을 의미합니다. . 선 대신에 간단한 점을 그려보는 것도 가능합니다. 아래는 초록색 파선, 빨강 점선, 파랑 삼각형의 예를 보여줍니다. . x = np.linspace(-1.4, 1.4, 30) plt.plot(x, x, &#39;g--&#39;, x, x**2, &#39;r:&#39;, x, x**3, &#39;b^&#39;) plt.show() . plot 함수는 Line2D객체로 구성된 리스트를 반환합니다 (각 객체가 각 선에 대응됩니다). . 이 선들에 대한 추가적인 속성을 설정할 수도 있습니다. 가령 선의 두께, 스타일, 투명도 같은것의 설정이 가능합니다. 공식 문서에서 설정 가능한 모든 속성을 확인해볼 수 있습니다. . 그리고 범례를 추가하는 가장 간단한 방법은 모든 선에 라벨을 설정 해 주고, legend 함수를 호출하는 것입니다. . x = np.linspace(-1.4, 1.4, 30) fig, ax = plt.subplots() # plot을 나누어 그리면 어디에 어떤 설정이 적용되었는지 알아보기 편합니다. # linewidth, alpha와 같은 line style도 plot() 안에 넣으면 혼동을 방지할 수 있습니다. line1 = ax.plot(x, x, &#39;g--&#39;, linewidth=3, dash_capstyle=&#39;round&#39;,label=&quot;x&quot;) line2 = ax.plot(x, x**2, &#39;r:&#39;,label=&quot;x**2&quot;) line3 = ax.plot(x, x**3, &#39;b^&#39;, alpha=0.2,label=&quot;x**3&quot;) plt.legend(loc=&quot;best&quot;) plt.show() . text 함수를 호출하여 텍스트를 그래프의 원하는 위치에 추가할 수 있습니다. 출력을 원하는 텍스트와 수평 및 수직 좌표를 지정하고, 추가적으로 몇 가지 속성을 지정해 주기만 하면 됩니다. matplotlib의 모든 텍스트는 TeX 방정식 표현을 포함할 수 있습니다. 더 자세한 내용은 공식 문서를 참조하시기 바랍니다. . x = np.linspace(-1.5, 1.5, 30) # -1.5 ~ 1.5까지(원하는 범위에서) 30개로 나누어서 출력 px = 0.8 py = px**2 plt.plot(x, x**2, &quot;b-&quot;, # 이차함수, 파란색 선을 그림 px, py, &quot;ro&quot;) # 선 위에 빨간점을 찍음 plt.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, # $y = x^2$ : 수학 수식을 쓰기위한 방법 fontsize=20, color=&#39;blue&#39;, horizontalalignment=&quot;center&quot;) plt.text(px - 0.08, py, &quot;Beautiful point&quot;, ha=&quot;right&quot;, weight=&quot;heavy&quot;) plt.text(px, py, &quot;x = %0.2f ny = %0.2f&quot;%(px, py), rotation=50, color=&#39;gray&#39;) plt.show() . fig, ax = plt.subplots() ax.plot(x, x**2, &quot;b-&quot;) ax.plot(px, py, &quot;ro&quot;) ax.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;blue&#39;, horizontalalignment=&quot;center&quot;) ax.text(px - 0.08, py, &quot;Beautiful point&quot;, ha=&quot;right&quot;, weight=&quot;heavy&quot;) ax.text(px, py, &quot;x = %0.2f ny = %0.2f&quot;%(px, py), rotation=50, color=&#39;gray&#39;) plt.show() . 더 많은 텍스트 속성을 알고 싶다면, 공식 문서를 참조하시기 바랍니다. . &#44536;&#47548;&#51200;&#51109; . 그래프를 그림파일로 저장하는 방법은 간단합니다. 단순히 파일이름을 지정하여 savefig 함수를 호출해 주기만 하면 됩니다. . 가능한 이미지 포맷은 사용하는 그래픽 백엔드에 따라서 지원 여부가 결정됩니다. . x = np.linspace(-1.4, 1.4, 30) plt.plot(x, x**2) plt.savefig(&quot;my_square_function.png&quot;, transparent=True) . [&lt;matplotlib.lines.Line2D at 0x1dc21b11f10&gt;] . &#48512;&#48516; &#44536;&#47000;&#54532; (subplot) . matplotlib는 하나의 그림(figure)에 여러개의 부분 그래프를 포함할 수 있습니다. 이 부분 그래프는 격자 형식으로 관리됩니다. . subplot 함수를 호출하여 부분 그래프를 생성할 수 있습니다. 이 때 격자의 행/열의 수 및 그래프를 그리고자 하는 부분 그래프의 색인을 파라미터로서 지정해줄 수 있습니다 (색인은 1부터 시작하며, 좌-&gt;우, 상단-&gt;하단의 방향입니다). . x = np.linspace(-1.4, 1.4, 30) # subplot(2,2,1)은 subplot(221)로 축약할 수 있습니다. plt.subplot(2, 2, 1) # 2 행 2 열 크기의 격자 중 첫 번째 부분 그래프 = 좌측 상단 plt.plot(x, x) plt.subplot(2, 2, 2) # 2 행 2 열 크기의 격자 중 두 번째 부분 그래프 = 우측 상단 plt.plot(x, x**2) plt.subplot(2, 2, 3) # 2 행 2 열 크기의 격자 중 세 번째 부분 그래프 = 좌측 하단 plt.plot(x, x**3) plt.subplot(2, 2, 4) # 2 행 2 열 크기의 격자 중 네 번째 부분 그래프 = 우측 하단 plt.plot(x, x**4) plt.show() . x = np.linspace(-1.4, 1.4, 30) fig, ax = plt.subplots(2, 2) # 순서대로 row의 갯수, col의 갯수입니다. nrows=2, cols=2로 지정할 수도 있습니다. # plot위치는 ax[row, col] 또는 ax[row][col]로 지정합니다. ax[0, 0].plot(x, x) # 2 행 2 열 크기의 격자 중 첫 번째 부분 그래프 = 좌측 상단 ax[0, 1].plot(x, x**2) # 2 행 2 열 크기의 격자 중 두 번째 부분 그래프 = 우측 상단 ax[1, 0].plot(x, x**3) # 2 행 2 열 크기의 격자 중 세 번째 부분 그래프 = 좌측 하단 ax[1, 1].plot(x, x**4) # 2 행 2 열 크기의 격자 중 네 번째 부분 그래프 = 우측 하단 plt.show() . 보다 복잡한 부분 그래프의 위치 선정이 필요하다면, subplot2grid를 대신 사용할 수 있습니다. . 격자의 행과 열의 번호 및 격자에서 해당 부분 그래프를 그릴 위치를 지정해줄 수 있습니다 (좌측상단 = (0,0). 또한 몇 개의 행/열로 확장되어야 하는지도 추가적으로 지정할 수 있습니다. . plt.subplot2grid((3,3), (0, 0), rowspan=2, colspan=2) plt.plot(x, x**2) plt.subplot2grid((3,3), (0, 2)) plt.plot(x, x**3) plt.subplot2grid((3,3), (1, 2), rowspan=2) plt.plot(x, x**4) plt.subplot2grid((3,3), (2, 0), colspan=2) plt.plot(x, x**5) plt.show() . gridsize = (3, 3) # 2행 2열 크기의 격자를 준비합니다. ax1 = plt.subplot2grid(gridsize, (0,0), rowspan=2, colspan=2) ax2 = plt.subplot2grid(gridsize, (0,2)) ax3 = plt.subplot2grid(gridsize, (1,2), rowspan=2) ax4 = plt.subplot2grid(gridsize, (2,0), colspan=2) ax1.plot(x, x**2) ax2.plot(x, x**3) ax3.plot(x, x**4) ax4.plot(x, x**5) plt.show() . &#50668;&#47084;&#44060;&#51032; &#44536;&#47548; (figure) . 여러개의 그림을 그리는것도 가능합니다. 각 그림은 하나 이상의 부분 그래프를 가질 수 있습니다. . 기본적으로는 matplotlib이 자동으로 figure(1)을 생성합니다. 그림간 전환을 할 때, pyplot은 현재 활성화된 그림을 계속해서 추적합니다 (이에대한 참조는 plt.gcf()의 호출로 알 수 있습니다). 또한 활성화된 그림의 활성화된 부분 그래프가 현재 그래프가 그려질 부분 그래프가 됩니다. . x = np.linspace(-1.4, 1.4, 30) plt.figure(1) plt.subplot(211) plt.plot(x, x**2) plt.title(&quot;Square and Cube&quot;) plt.subplot(212) plt.plot(x, x**3) plt.figure(2, figsize=(10, 5)) plt.subplot(121) plt.plot(x, x**4) plt.title(&quot;y = x**4&quot;) plt.subplot(122) plt.plot(x, x**5) plt.title(&quot;y = x**5&quot;) plt.figure(1) # 그림 1로 돌아가며, 활성화된 부분 그래프는 212 (하단)이 됩니다 plt.plot(x, -x**3, &quot;r:&quot;) plt.show() . x = np.linspace(-1.4, 1.4, 30) fig1, ax1 = plt.subplots(nrows=2, ncols=1) ax1[0].plot(x, x**2) ax1[0].set_title(&quot;Square and Cube&quot;) ax1[1].plot(x, x**3) fig2, ax2 = plt.subplots(nrows=1, ncols=2, figsize=(10, 5)) ax2[0].plot(x, x**4) ax2[0].set_title(&quot;y = x**4&quot;) ax2[1].plot(x, x**5) ax2[1].set_title(&quot;y = x**5&quot;) ax1[1].plot(x, -x**3, &quot;r:&quot;) # 그림 1로 돌아가며, 활성화된 부분 그래프는 ax1[1] (하단)이 됩니다. plt.show() . &#45796;&#50577;&#54620; &#44536;&#47000;&#54532; . &#49328;&#51216;&#46020;(Scatter plot) . 각 점에 대한 x 및 y 좌표를 제공하면 산점도를 그릴 수 있습니다. . 부수적으로 각 점의 크기를 정할 수도 있습니다. (s = scale) . 마찬가지로 여러 속성을 설정할 수 있습니다. 가령 테두리 및 모양의 내부 색상, 그리고 투명도와 같은것의 설정이 가능합니다. . from numpy.random import rand for color in [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]: n = 100 x, y = rand(2, n) scale = 500.0 * rand(n) ** 5 plt.scatter(x, y, s=scale, c=color, alpha=0.3, edgecolors=&#39;blue&#39;) plt.grid(True) plt.show() . from numpy.random import rand fig, ax = plt.subplots() for color in [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]: n = 100 x, y = rand(2, n) scale = 500.0 * rand(n) ** 5 ax.scatter(x, y, s=scale, c=color, alpha=0.3, edgecolors=&#39;blue&#39;) ax.grid(True) plt.show() . &#55176;&#49828;&#53664;&#44536;&#47016;(hist) . data1 = np.random.randn(400) data2 = np.random.randn(500) + 3 data3 = np.random.randn(450) + 6 data4a = np.random.randn(200) + 9 data4b = np.random.randn(100) + 10 plt.hist(data1, bins=5, color=&#39;g&#39;, alpha=0.75, label=&#39;bar hist&#39;) # default histtype=&#39;bar&#39; plt.hist(data2, color=&#39;b&#39;, alpha=0.65, histtype=&#39;stepfilled&#39;, label=&#39;stepfilled hist&#39;) plt.hist(data3, color=&#39;r&#39;, histtype=&#39;step&#39;, label=&#39;step hist&#39;) plt.hist((data4a, data4b), color=(&#39;r&#39;,&#39;m&#39;), alpha=0.55, histtype=&#39;barstacked&#39;, label=(&#39;barstacked a&#39;, &#39;barstacked b&#39;)) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Frequency&quot;) plt.legend() plt.grid(True) plt.show() . fig, ax = plt.subplots() ax.hist(data1, bins=5, color=&#39;g&#39;, alpha=0.75, label=&#39;bar hist&#39;) # default histtype=&#39;bar&#39; ax.hist(data2, color=&#39;b&#39;, alpha=0.65, histtype=&#39;stepfilled&#39;, label=&#39;stepfilled hist&#39;) ax.hist(data3, color=&#39;r&#39;, histtype=&#39;step&#39;, label=&#39;step hist&#39;) ax.hist((data4a, data4b), color=(&#39;r&#39;,&#39;m&#39;), alpha=0.55, histtype=&#39;barstacked&#39;, label=(&#39;barstacked a&#39;, &#39;barstacked b&#39;)) ax.set_xlabel(&quot;Value&quot;) ax.set_ylabel(&quot;Frequency&quot;) ax.legend() ax.grid(True) plt.show() . Visualization_with_Seaborn . Seaborn은 matplotlib를 기반으로 하는 Python 데이터 시각화 라이브러리입니다. . import seaborn as sns sns.set() sns.set(style=&quot;darkgrid&quot;) import numpy as np import pandas as pd . In this notebook we will use the Big Mart Sales Data. You can download the data from . : https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/download/train-file . Loading dataset . data_BM = pd.read_csv(&#39;bigmart_data.csv&#39;) # drop the null values data_BM = data_BM.dropna(how=&quot;any&quot;) # multiply Item_Visibility by 100 to increase size data_BM[&quot;Visibility_Scaled&quot;] = data_BM[&quot;Item_Visibility&quot;] * 100 # view the top results data_BM.head() . Item_Identifier Item_Weight Item_Fat_Content Item_Visibility Item_Type Item_MRP Outlet_Identifier Outlet_Establishment_Year Outlet_Size Outlet_Location_Type Outlet_Type Item_Outlet_Sales Visibility_Scaled . 0 FDA15 | 9.300 | Low Fat | 0.016047 | Dairy | 249.8092 | OUT049 | 1999 | Medium | Tier 1 | Supermarket Type1 | 3735.1380 | 1.604730 | . 1 DRC01 | 5.920 | Regular | 0.019278 | Soft Drinks | 48.2692 | OUT018 | 2009 | Medium | Tier 3 | Supermarket Type2 | 443.4228 | 1.927822 | . 2 FDN15 | 17.500 | Low Fat | 0.016760 | Meat | 141.6180 | OUT049 | 1999 | Medium | Tier 1 | Supermarket Type1 | 2097.2700 | 1.676007 | . 4 NCD19 | 8.930 | Low Fat | 0.000000 | Household | 53.8614 | OUT013 | 1987 | High | Tier 3 | Supermarket Type1 | 994.7052 | 0.000000 | . 5 FDP36 | 10.395 | Regular | 0.000000 | Baking Goods | 51.4008 | OUT018 | 2009 | Medium | Tier 3 | Supermarket Type2 | 556.6088 | 0.000000 | . 1. Creating basic plots . matplotlib에서 여러 줄이 필요한 한 줄로 seaborn에서 몇 가지 기본 플롯을 만드는 방법을 살펴보겠습니다. . line chart . 일부 데이터 세트의 경우 한 변수의 변화를 시간의 함수로 이해하거나 이와 유사한 연속 변수를 이해하고 싶을 수 있습니다. seaborn에서 이는 lineplot() 함수로 직접 또는 kind=&quot;line&quot;을 설정하여 relplot()으로 수행할 수 있습니다. | . | . sns.lineplot(x=&quot;Item_Weight&quot;, y=&quot;Item_MRP&quot;,data=data_BM[:50]); . Bar Chart . Seaborn에서는 barplot 기능을 이용하여 간단하게 막대그래프를 생성할 수 있습니다. | . sns.barplot(x=&quot;Item_Type&quot;, y=&quot;Item_MRP&quot;, data=data_BM[:5]) . &lt;AxesSubplot:xlabel=&#39;Item_Type&#39;, ylabel=&#39;Item_MRP&#39;&gt; . Histogram . distplot()을 사용하여 Seaborn에서 히스토그램을 생성할 수 있습니다. 사용할 수 있는 여러 옵션이 있으며 노트북에서 더 자세히 살펴보겠습니다. | . sns.distplot(data_BM[&#39;Item_MRP&#39;]) . &lt;AxesSubplot:xlabel=&#39;Item_MRP&#39;, ylabel=&#39;Density&#39;&gt; . Box plots . Seaborn에서 boxplot을 생성하기 위해 boxplot()을 사용할 수 있습니다. | 아이템의 Item_Outlet_Sales 분포를 시각화해 봅시다. | 더 자세한 예제는 링크 참조 : https://seaborn.pydata.org/generated/seaborn.boxplot.html | . sns.boxplot(data_BM[&#39;Item_Outlet_Sales&#39;], orient=&#39;vertical&#39;) . &lt;AxesSubplot:xlabel=&#39;Item_Outlet_Sales&#39;&gt; . Violin plot . Violin plot은 상자 및 수염 플롯과 유사한 역할을 합니다. | 이러한 분포를 비교할 수 있도록 하나(또는 그 이상) 범주형 변수의 여러 수준에 대한 정량적 데이터의 분포를 보여줍니다. | 모든 플롯 구성 요소가 실제 데이터 포인트에 해당하는 상자 플롯과 달리 Violin plot은 기본 분포의 커널 밀도 추정을 특징으로 합니다. | Seaborn에서 violinplot()을 사용하여 Violin plot을 만들 수 있습니다. | . sns.violinplot(data_BM[&#39;Item_Outlet_Sales&#39;], orient=&#39;vertical&#39;, color=&#39;magenta&#39;) . &lt;AxesSubplot:xlabel=&#39;Item_Outlet_Sales&#39;&gt; . Scatter plot . 각 포인트는 데이터 세트의 관찰을 나타내는 포인트 클라우드를 사용하여 두 변수의 분포를 나타냅니다. | 이 묘사를 통해 눈은 그들 사이에 의미 있는 관계가 있는지 여부에 대한 상당한 양의 정보를 추론할 수 있습니다. | relplot() 을 kind=scatter 옵션과 함께 사용하여 seaborn에서 산점도를 그릴 수 있습니다. | . sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, data=data_BM[:200], kind=&quot;scatter&quot;); . Hue semantic . 세 번째 변수에 따라 점을 색칠하여 플롯에 다른 차원을 추가할 수도 있습니다. Seaborn에서는 이것을 &quot;Hue semantic&quot; 사용이라고 합니다. . sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, hue=&quot;Item_Type&quot;,data=data_BM[:200]); . Bubble plot . hue 시맨틱을 활용하여 Item_Visibility별로 거품을 색칠함과 동시에 개별 거품의 크기로 사용합니다. | . sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, data=data_BM[:200], kind=&quot;scatter&quot;, size=&quot;Visibility_Scaled&quot;, hue=&quot;Visibility_Scaled&quot;); . Category wise sub plot . Seaborn에서 카테고리별 플롯을 생성할 수도 있습니다. | 각 Outlet_Size에 대한 산점도를 만들었습니다. | . sns.relplot(x=&quot;Item_Weight&quot;, y=&quot;Item_Visibility&quot;, hue=&#39;Outlet_Size&#39;,style=&#39;Outlet_Size&#39;, col=&#39;Outlet_Size&#39;,data=data_BM[:100]); # col = 컬럼별로 시각화를 하고 싶을 때 사용 / 없으면 한 그림에 표시하게 됨 . Advance categorical plots in seaborn . For categorical variables we have three different families in seaborn. . Categorical scatterplots: . stripplot() (with kind=&quot;strip&quot;; the default) : 하나의 변수가 범주형인 산점도를 그립니다. | swarmplot() (with kind=&quot;swarm&quot;) : 이 함수는 stripplot()과 유사하지만 포인트가 겹치지 않도록 조정됩니다(범주형 축을 따라만). | . | Categorical distribution plots: . boxplot() (with kind=&quot;box&quot;) | violinplot() (with kind=&quot;violin&quot;) | boxenplot() (with kind=&quot;boxen&quot;) 이 스타일의 플롯은 &quot;문자 값&quot;으로 정의되는 많은 수의 분위수를 보여주기 때문에 원래 &quot;문자 값&quot; 플롯이라고 명명되었습니다. | 모든 특징이 실제 관찰과 일치하는 분포의 비모수적 표현을 플로팅하는 상자 플롯과 유사합니다. | . | . | Categorical estimate plots: . pointplot() (with kind=&quot;point&quot;) | barplot() (with kind=&quot;bar&quot;) | . | . b. Categorical distribution plots . Box Plots . Box plot은 극단값과 함께 분포의 3사분위수 값을 보여줍니다. | “whiskers”은 하위 및 상위 사분위수의 1.5 IQR 내에 있는 점으로 확장되며, 이 범위를 벗어나는 관찰은 독립적으로 표시됩니다. | 이것은 Box plot의 각 값이 데이터의 실제 관측값에 해당함을 의미합니다. | . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;box&quot;,data=data_BM); . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;violin&quot;,data=data_BM); . point plot . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;point&quot;,data=data_BM); . bar plot . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;bar&quot;,data=data_BM); . 3. Density Plots . histogram 대신 Seaborn이 sn.kdeplot으로 수행하는 커널 밀도 추정을 사용하여 분포를 원활하게 추정할 수 있습니다. . import matplotlib.pyplot as plt plt.figure(figsize=(5,5)) sns.kdeplot(data_BM[&#39;Item_Visibility&#39;], shade=True); . Histogram and Density Plot . Histogram과 KDE는 distplot을 사용하여 결합할 수 있습니다. . plt.figure(figsize=(5,5)) sns.distplot(data_BM[&#39;Item_Outlet_Sales&#39;]); . c: ProgramData Anaconda3 lib site-packages seaborn distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . 4. Pair plots . Joint plot을 더 큰 차원의 데이터세트로 일반화하면 Pair plot으로 끝납니다. 이것은 모든 값 쌍을 서로에 대해 플롯하려는 경우 다차원 데이터 간의 상관 관계를 탐색하는 데 매우 유용합니다. . | 세 가지 붓꽃 종의 꽃잎과 꽃받침 측정값을 나열하는 잘 알려진 Iris 데이터 세트를 사용하여 이것을 시연할 것입니다. . | . iris = sns.load_dataset(&quot;iris&quot;) sns.pairplot(iris, hue=&#39;species&#39;, height=2.5); . Visualization_with_Matplotlib &amp; Seaborn . 코드 출처 - 이제현님 블로그 https://jehyunlee.github.io/2020/09/30/Python-DS-34-seaborn_matplotlib/ . 1.1 Load data . 예제로 사용할 펭귄 데이터를 불러옵니다. seaborn에 내장되어 있습니다. . import pandas as pd import matplotlib.pyplot as plt import seaborn as sns penguins = sns.load_dataset(&quot;penguins&quot;) penguins.head() . species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex . 0 Adelie | Torgersen | 39.1 | 18.7 | 181.0 | 3750.0 | Male | . 1 Adelie | Torgersen | 39.5 | 17.4 | 186.0 | 3800.0 | Female | . 2 Adelie | Torgersen | 40.3 | 18.0 | 195.0 | 3250.0 | Female | . 3 Adelie | Torgersen | NaN | NaN | NaN | NaN | NaN | . 4 Adelie | Torgersen | 36.7 | 19.3 | 193.0 | 3450.0 | Female | . 1.2 Figure and Axes . matplotlib으로 도화지figure를 깔고 축공간axes를 만듭니다. 1 x 2 축공간을 구성합니다. . fig, axes = plt.subplots(ncols=2, figsize=(8,4)) fig.tight_layout() . 1.3 plot with matplotlib . matplotlib 기능을 이용해서 산점도를 그립니다. x축은 부리 길이 bill length, y축은 부리 위 아래 두께 bill depth . 색상은 종species로 합니다. Adelie, Chinstrap, Gentoo이 있습니다. 두 축공간 중 왼쪽에만 그립니다. 컬러를 다르게 주기 위해 f-string 포맷을 사용했습니다. . f-string 포맷에 대한 설명은 https://blockdmask.tistory.com/429를 참고하세요 . fig, axes = plt.subplots(ncols=2,figsize=(8,4)) species_u = penguins[&quot;species&quot;].unique() for i, s in enumerate(species_u): axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3) axes[0].legend(species_u, title=&quot;species&quot;) axes[0].set_xlabel(&quot;Bill Length (mm)&quot;) axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;) # plt.show() fig.tight_layout() . 1.4 Plot with seaborn . 단 세 줄로 거의 동일한 그림이 나왔습니다. . scatter plot의 점 크기만 살짝 작습니다. | label의 투명도만 살짝 다릅니다. | . | seaborn 명령 scatterplot()을 그대로 사용했습니다. . | x축과 y축 label도 바꾸었습니다. . ax=axes[1] 인자에서 볼 수 있듯, 존재하는 axes에 그림만 얹었습니다. | matplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통합니다. | . | . fig, axes = plt.subplots(ncols=2,figsize=(8,4)) species_u = penguins[&quot;species&quot;].unique() # plot 0 : matplotlib for i, s in enumerate(species_u): axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3) axes[0].legend(species_u, title=&quot;species&quot;) axes[0].set_xlabel(&quot;Bill Length (mm)&quot;) axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;) # plot 1 : seaborn sns.scatterplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.3, ax=axes[1]) axes[1].set_xlabel(&quot;Bill Length (mm)&quot;) axes[1].set_ylabel(&quot;Bill Depth (mm)&quot;) fig.tight_layout() . 1.5 matplotlib + seaborn &amp; seaborn + matplotlib . matplotlib과 seaborn이 자유롭게 섞일 수 있습니다. matplotlib 산점도 위에 seaborn 추세선을 얹을 수 있고, | seaborn 산점도 위에 matplotlib 중심점을 얹을 수 있습니다. | . | . fig, axes = plt.subplots(ncols=2, figsize=(8, 4)) species_u = penguins[&quot;species&quot;].unique() # plot 0 : matplotlib + seaborn for i, s in enumerate(species_u): # matplotlib 산점도 axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3 ) # seaborn 추세선 sns.regplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, data=penguins.loc[penguins[&quot;species&quot;]==s], scatter=False, ax=axes[0]) axes[0].legend(species_u, title=&quot;species&quot;) axes[0].set_xlabel(&quot;Bill Length (mm)&quot;) axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;) # plot 1 : seaborn + matplotlib # seaborn 산점도 sns.scatterplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.3, ax=axes[1]) axes[1].set_xlabel(&quot;Bill Length (mm)&quot;) axes[1].set_ylabel(&quot;Bill Depth (mm)&quot;) for i, s in enumerate(species_u): # matplotlib 중심점 axes[1].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), c=f&quot;C{i}&quot;, alpha=1, marker=&quot;x&quot;, s=100 ) fig.tight_layout() .",
            "url": "https://woojaaeon.github.io/jaong/2022/05/12/Visualization.html",
            "relUrl": "/2022/05/12/Visualization.html",
            "date": " • May 12, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Python Language Basics, IPython, and Jupyyer Notebooks",
            "content": "### When used in R for(x in array){ if(x &lt; pivot){ less.append(x) }else{ greater.append(x) } } ### When used in Python for x in array: if x &lt; pivot: less.append(x) else: greater.append(x) R에서는 ( )를 사용하지만, Python에서는 괄호가 아닌 들여쓰기를 사용한다. . Dynamic references, strong type . Python의 데이터 타입(자료형, class)에는 . int : 정수형 | float : 실수형 | str : 문자형 (주로 &quot; &quot; 또는 &#39; &#39;을 사용) | . 있다. 데이터 타입을 확인하기 위해서는 type() 함수를 사용해서 알 수 있다. . ※ 문자열(str)과 숫자형(int or float)는 계산 할 수 없다. . a = 5 b = &#39;foo&#39; c = 4.5 print(type(a),type(b),type(c)) . &lt;class &#39;int&#39;&gt; &lt;class &#39;str&#39;&gt; &lt;class &#39;float&#39;&gt; . Attributes and methods . Attributes(속성): other Python objects stored &quot;inside&quot; the object [ 객체 &quot;내부&quot;에 저장된 다른 Python 객체 ] . methods(방법): functions associated with an object that can have access to the object&#39;s internal data [ 객체의 내부 데이터에 접근할 수 있는 객체와 관련된 함수 ] . a =&#39;foo&#39; # Methods a.upper() . &#39;FOO&#39; . Import . 파이썬 모듈 만들기(in Jupyter) . Jupyter에서 textFile을 생성 | some_module.py 처럼 같은 디렉토리의 다른 파일에서 정의된 변수와 함수를 작성 후 저장 | # some_module.py PI = 3.14159 def f(x): return x + 2 def g(a,b): return a + b 새로운 .ipynb 파일을 만들어, import로 some_module을 불러온 후, 실행 | import some_module result = some_module.f(5) pi = some_module.PI pi . 3.14 . import some_module as sm ### from으로 바로 불러와서 사용할 수 있다. from some_module import PI as pi, g as gf r1 = sm.f(pi) r2 = gf(6, pi) print(r1, r2) . 5.140000000000001 9.14 . Binary operators and comparisons . a = [1,2,3] b = a c = list(a) print(a is b) print(a == b) print(a is not c) . True True True . == , != 와 is, is not 의 차이점 == 과 != 는 값 자체를 비교 | is 와 is not은 객체를 비교 | . | . Mutable and immutable objects . list, dict, Numpy array 및 대부분의 사용자 정의 유형(classes)과 같은 Python의 대부분의 객체는 변경 가능하다. . Tuple은 변경 불가능하지만, 작업속도는 List보다 더 빠르게 작업할 수 있다. . a_list = [&#39;foo&#39;,2,[4,5]] a_list[2] = (3,4) a_list . [&#39;foo&#39;, 2, (3, 4)] . a_tuple = (3,5,(4,5)) a_tuple[1] = &#39;four&#39; . TypeError Traceback (most recent call last) &lt;ipython-input-11-1990528cab52&gt; in &lt;module&gt; 1 a_tuple = (3,5,(4,5)) -&gt; 2 a_tuple[1] = &#39;four&#39; TypeError: &#39;tuple&#39; object does not support item assignment . Scalar Type . Numeric type : 숫자에 대한 기본 Python 유형은 int 및 float가 있다. . int는 임의의 큰 숫자를 저장할 수 있다. | 소수점 숫자는 float로 표시할 수 있다. | . ival = 17239871 type(ival **6) . int . fval = 7.234 type(fval**6) . float . Strings . Python 문자열을 변경할 수 없고, 인덱스를 뽑아서 수정할 수 없다. (Python strings are immutable; you cannot modify a sting) . a = &#39;This is a string&#39; a[10] = &#39;f&#39; . TypeError Traceback (most recent call last) &lt;ipython-input-19-b9d06eea4235&gt; in &lt;module&gt; 1 a = &#39;This is a string&#39; -&gt; 2 a[10] = &#39;f&#39; TypeError: &#39;str&#39; object does not support item assignment . 하지만, replace()를 써서 바꿀 수 있다. . replace()함수 | . : 텍스트 문자열의 일부를 지정된 문자 수만큼 다른 텍스트 문자열로 바꾼다. : 지정한 바이트 수에 따라 텍스트 문자열의 일부를 다른 텍스트 문자열로 바꾼다. . a = &#39;This is a string&#39; b = a.replace(&#39;string&#39;,&#39;longer string&#39;) b . &#39;This is a longer string&#39; . a + b . &#39;This is a stringThis is a longer string&#39; . 문자열 자료형( str ) 에는 다양한 이스케이프 코드들이 있다. . 주로 사용되는 코드들 . n : 문자열 안에서 줄을 바꿀 때 사용 | t : 문자열 사이에 탭 간격을 줄 때 사용 | : 문자 를 그대로 표현할 때 사용 | &#39; : 작은따옴표(&#39;)를 그대로 표현할 때 사용 | &quot; : 큰따옴표(&quot;)를 그대로 표현할 때 사용 | . a = &#39;one way of n writing a tstring&#39; print(a) . one way of writing a string . Data Type을 변환할 때 쓰는 함수 . str() : 문자열로 변환 | int() : 정수형으로 변환 | float() : 실수형으로 변환 | bool() : 결과값이 True 또는 False값으로 출력됨 | tuple() : 튜플로 변환 | list() : 리스트로 변환 | . a = 5.6 s = str(a) i = int(a) print(s,type(s),&quot; n&quot;, i,type(i),&quot; n&quot;, a, type(a)) . 5.6 &lt;class &#39;str&#39;&gt; 5 &lt;class &#39;int&#39;&gt; 5.6 &lt;class &#39;float&#39;&gt; . ※ sequence 자료형 (3가지의 자료형을 가지고 있다.) . : 모든 시퀀스는 순서가 유지되고, 정수로 인덱싱하며, 길이가 있다. : 문자열은 Unicord 문자의 시퀀스이므로 list 및 tuple 과 같은 다른 시퀀스처럼 취급될 수 있습니다. . 문자열: &#39;Hello&#39;. 문자열(string)은 문자(character)들의 시퀀스다. | 리스트: [1, 4, 5] | 튜플: (&#39;GOOG&#39;, 100, 490.1) | . s = &#39;python&#39; list(s) . [&#39;p&#39;, &#39;y&#39;, &#39;t&#39;, &#39;h&#39;, &#39;o&#39;, &#39;n&#39;] . ※ 슬라이싱(Slicing) . : 시퀀스의 일부(subsequence)를 취하는 것을 슬라이싱이라 한다. . s = &#39;python&#39; s[:3] . &#39;pyt&#39; . ※ String objects have a format methods . : 문자열 객체에는 인수를 문자열로 대체하여 새 문자열을 생성하는 데 사용할 수 있는 형식 메서드가 있다. . template = &#39;{0:.2f} {1:s} are worth US ${2:d}&#39; template.format(4.5560,&#39;Argentine Pesos&#39;,1) . &#39;4.56 Argentine Pesos are worth US $1&#39; . Cord Explain | . {0:.2f}: 소수점 둘째자리까지 . {1:s}: 문자열 . {2:d}: 정수형 . Booleans . Python의 두 Bool 값은 True 및 False로 작성된다. 비교 및 기타 조건 표현식은 True 또는 False로 평가된다. . True and False # True or False . False . None . : None은 Python null 값 유형이다. 함수가 명시적으로 값을 반환하지 않으면 암시적으로 None을 반환한다. . a = None a is None . True . Date and times . date to string, string to date | strftime(): datetime type을 string type으로 변경 strptime() : string type을 datetime type으로 변경 . Change date | timedelta() : days, seconds, microseconds 속성이 있는데, 날짜단위, 시간단위, 밀리세컨단위를 일, 초 밀리세컨 값으로 나타낸다. . dateutil Package | parse() : 형식문자열을 자동으로 찾아서 객체로 만들어줌. (단, 포맷을 맘대로 바꾸면 찾을 수 없으니 strptime 메서드를 쓰는게 좋다.) . from datetime import datetime, date, time dt = datetime(2011,10,29,20,30,21) dt . datetime.datetime(2011, 10, 29, 20, 30, 21) . dt.strftime(&#39;%m/%d/%Y %H:%M&#39;) . &#39;10/29/2011 20:30&#39; . datetime.strptime(&#39;20220317&#39;,&#39;%Y%m%d&#39;) . datetime.datetime(2022, 3, 17, 0, 0) . dt2 = datetime(2001,3,19,22,30) delta = dt2 - dt delta . datetime.timedelta(days=-3876, seconds=7179) . type(delta) . datetime.timedelta . Control Flow . if, elif, and else . : if 문은 선택적으로 하나 이상의 elif 블록과 모든 조건이 False인 경우 else 블록을 실행한다. . x = -5 if x &lt; 0: print(&#39;It is negative&#39;) elif x == 0: print(&#39;Equal to zero&#39;) elif 0 &lt; x &lt; 5: print(&#39;Positive but smaller than 5&#39;) else: print(&#39;Positive and larger than or equal to 5&#39;) . It is negative . for loops . : for문은 컬렉션(collection) 또는 반복자(iterater)를 반복하기 위한 것이다. . sequence = [1,2,None,4,None,5] total = 0 for value in sequence: if value is None: continue total += value total . 12 . sequence = [1,2,0,4,6,5,2,1] total_until_5 = 0 for value in sequence: if value ==5: break total_until_5 += value total_until_5 . 13 . ※ 중첩반복문(Nested Loop) . : 바깥 쪽 for문을 기준으로 안 쪽에 위치한 for문이 먼저 반복된다. . ※ 복합 대입 연산자(Assignment Operators) . : 연산과 할당을 합쳐놓은 것이다.(식을 간결하게 사용 가능) . a += b -&gt; a = a + b . i=0 sum=0 for i in range(1, 101): for j in range(1,i+1): # sum += j sum=sum+j print(sum) . 171700 . while loops . : 반복해서 문장을 수행해야 할 경우 while문을 사용한다. . x = 256 total = 0 while x &gt; 0: if total &gt; 500: break total += x x = x// 2 total . 504 . Pass . : pass는 Python에서 &quot;no-op&quot;(작업 없음) 문이다. (어떠한 행동을 하지 않는다.) . x = -1 if x &lt; 0: print(&#39;negative&#39;) elif x == 0: pass # x가 0이면 아무것도 하지 않음. else: print(&#39;positive!&#39;) . negative . Range . : 범위 함수는 일정한 간격의 정수 시퀀스를 생성하는 반복자를 반환한다. . range(10) . range(0, 10) . list(range(10)) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . 시작 끝과 단계가 모두 주어질 수 있다. | . list(range(0,20,2)) . [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] . Range 의 일반적인 용도는 인덱스(index)로 시퀀스를 반복하는 것이다. . seq = [1,2,3,4] for i in range(len(seq)): val = seq[i] val . 4 . list와 같은 함수를 사용하여 range에 의해 생성된 모든 정수를 다른 데이터 구조에 저장할 수 있지만 기본 반복자가 원하는 것이 되는 경우가 많다. . sum = 0 for i in range(100000): # % is the module operator if i % 3 == 0 or i % 5 == 0 : sum += i print(sum) . 2333316668 . Ternary expressions . : true-expr 및 false-expr은 모든 Python 표현식이 될 수 있다. . value = true-expr if condition else false-expr if condition: value = tru-expr else: value = false-expr x=5 &#39;Non-negative&#39; if x &gt;= 0 else &#39;Negative&#39; . &#39;Non-negative&#39; .",
            "url": "https://woojaaeon.github.io/jaong/2022/03/18/PythonBasic.html",
            "relUrl": "/2022/03/18/PythonBasic.html",
            "date": " • Mar 18, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "How to use markdown",
            "content": "Markdown Cheat Sheet . Thanks for visiting The Markdown Guide! . This Markdown cheat sheet provides a quick overview of all the Markdown syntax elements. It can’t cover every edge case, so if you need more information about any of these elements, refer to the reference guides for basic syntax and extended syntax. . Basic Syntax . These are the elements outlined in John Gruber’s original design document. All Markdown applications support these elements. . Heading . H1 . H2 . H3 . Bold . bold text . Italic . italicized text . Blockquote . blockquote . Ordered List . First item | Second item | Third item | Unordered List . First item | Second item | Third item | . Code . code . Horizontal Rule . . Link . Markdown Guide . Image . . Extended Syntax . These elements extend the basic syntax by adding additional features. Not all Markdown applications support these elements. . Table . Syntax Description . Header | Title | . Paragraph | Text | . Fenced Code Block . { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Smith&quot;, &quot;age&quot;: 25 } . Footnote . Here’s a sentence with a footnote. 1 . Heading ID . My Great Heading . Definition List . term definition Strikethrough . The world is flat. . Task List . Write the press release | Update the website | Contact the media | . Emoji . That is so funny! :joy: . (See also Copying and Pasting Emoji) . Highlight . I need to highlight these ==very important words==. . Subscript . H~2~O . Superscript . X^2^ . This is the footnote. &#8617; . |",
            "url": "https://woojaaeon.github.io/jaong/markdown/2022/03/11/markdown-cheat-sheet.html",
            "relUrl": "/markdown/2022/03/11/markdown-cheat-sheet.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": " Numpy! ",
            "content": "도구 - 넘파이(NumPy) . *넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다. 넘파이의 핵심은 강력한 N-차원 배열 객체입니다. 또한 선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.&quot; . 구글 코랩에서 실행하기 | &#48176;&#50676; &#49373;&#49457; . numpy를 임포트해 보죠. 대부분의 사람들이 np로 알리아싱하여 임포트합니다: . import numpy as np . np.zeros . zeros 함수는 0으로 채워진 배열을 만듭니다: . np.zeros(5) . array([0., 0., 0., 0., 0.]) . 2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 $3 times 4$ 크기의 행렬입니다: . np.zeros((3,4)) . array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) . &#50857;&#50612; . 넘파이에서 각 차원을 축(axis) 이라고 합니다 | 축의 개수를 랭크(rank) 라고 합니다. 예를 들어, 위의 $3 times 4$ 행렬은 랭크 2인 배열입니다(즉 2차원입니다). | 첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다. | . | 배열의 축 길이를 배열의 크기(shape)라고 합니다. 예를 들어, 위 행렬의 크기는 (3, 4)입니다. | 랭크는 크기의 길이와 같습니다. | . | 배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, $3 times 4=12$). | . a = np.zeros((3,4)) a . array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) . a.shape . (3, 4) . a.ndim # len(a.shape)와 같습니다 . 2 . a.size . 12 . N-&#52264;&#50896; &#48176;&#50676; . 임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다: . np.zeros((2,2,5)) . array([[[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]]) . &#48176;&#50676; &#53440;&#51077; . 넘파이 배열의 타입은 ndarray입니다: . type(np.zeros((3,4))) . numpy.ndarray . np.ones . ndarray를 만들 수 있는 넘파이 함수가 많습니다. . 다음은 1로 채워진 $3 times 4$ 크기의 행렬입니다: . np.ones((3,4)) . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . np.full . 주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 $3 times 4$ 크기의 행렬입니다. . np.full((3,4), np.pi) . array([[3.14159265, 3.14159265, 3.14159265, 3.14159265], [3.14159265, 3.14159265, 3.14159265, 3.14159265], [3.14159265, 3.14159265, 3.14159265, 3.14159265]]) . np.empty . 초기화되지 않은 $2 times 3$ 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다): . np.empty((2,3)) . array([[9.6677106e-317, 0.0000000e+000, 0.0000000e+000], [0.0000000e+000, 0.0000000e+000, 0.0000000e+000]]) . np.array . array 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다: . np.array([[1,2,3,4], [10, 20, 30, 40]]) . array([[ 1, 2, 3, 4], [10, 20, 30, 40]]) . np.arange . 파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다: . np.arange(1, 5) . array([1, 2, 3, 4]) . 부동 소수도 가능합니다: . np.arange(1.0, 5.0) . array([1., 2., 3., 4.]) . 파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다: . np.arange(1, 5, 0.5) . array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5]) . 부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다: . print(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다. print(np.arange(0, 5/3, 0.333333333)) print(np.arange(0, 5/3, 0.333333334)) . [0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] [0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] [0. 0.33333333 0.66666667 1. 1.33333334] . np.linspace . 이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다): . print(np.linspace(0, 5/3, 6)) . [0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] . np.rand&#50752; np.randn . 넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 $3 times 4$ 행렬을 초기화합니다: . np.random.rand(3,4) . array([[0.37892456, 0.17966937, 0.38206837, 0.34922123], [0.80462136, 0.9845914 , 0.9416127 , 0.28305275], [0.21201033, 0.54891417, 0.03781613, 0.4369229 ]]) . 다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 $3 times 4$ 행렬입니다: . np.random.randn(3,4) . array([[ 0.83811287, -0.57131751, -0.4381827 , 1.1485899 ], [ 1.45316084, -0.47259181, -1.23426057, -0.0669813 ], [ 1.01003549, 1.04381736, -0.93060038, 2.39043293]]) . 이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요): . %matplotlib inline import matplotlib.pyplot as plt . plt.hist(np.random.rand(100000), density=True, bins=100, histtype=&quot;step&quot;, color=&quot;blue&quot;, label=&quot;rand&quot;) plt.hist(np.random.randn(100000), density=True, bins=100, histtype=&quot;step&quot;, color=&quot;red&quot;, label=&quot;randn&quot;) plt.axis([-2.5, 2.5, 0, 1.1]) plt.legend(loc = &quot;upper left&quot;) plt.title(&quot;Random distributions&quot;) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Density&quot;) plt.show() . np.fromfunction . 함수를 사용하여 ndarray를 초기화할 수도 있습니다: . def my_function(z, y, x): return x + 10 * y + 100 * z np.fromfunction(my_function, (3, 2, 10)) . array([[[ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], [ 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]], [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.], [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]], [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.], [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]]) . 넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다: . [[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] [[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.] [ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]] . 위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다. . &#48176;&#50676; &#45936;&#51060;&#53552; . dtype . 넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다: . c = np.arange(1, 5) print(c.dtype, c) . int64 [1 2 3 4] . c = np.arange(1.0, 5.0) print(c.dtype, c) . float64 [1. 2. 3. 4.] . 넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다: . d = np.arange(1, 5, dtype=np.complex64) print(d.dtype, d) . complex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j] . 가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요. . itemsize . itemsize 속성은 각 아이템의 크기(바이트)를 반환합니다: . e = np.arange(1, 5, dtype=np.complex64) e.itemsize . 8 . data &#48260;&#54140; . 배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요). . f = np.array([[1,2],[1000, 2000]], dtype=np.int32) f.data . &lt;memory at 0x7f97929dd790&gt; . 파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다. . if (hasattr(f.data, &quot;tobytes&quot;)): data_bytes = f.data.tobytes() # python 3 else: data_bytes = memoryview(f.data).tobytes() # python 2 data_bytes . b&#39; x01 x00 x00 x00 x02 x00 x00 x00 xe8 x03 x00 x00 xd0 x07 x00 x00&#39; . 여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다. . &#48176;&#50676; &#53356;&#44592; &#48320;&#44221; . &#51088;&#49888;&#51012; &#48320;&#44221; . ndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다. . g = np.arange(24) print(g) print(&quot;랭크:&quot;, g.ndim) . [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] 랭크: 1 . g.shape = (6, 4) print(g) print(&quot;랭크:&quot;, g.ndim) . [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15] [16 17 18 19] [20 21 22 23]] 랭크: 2 . g.shape = (2, 3, 4) print(g) print(&quot;랭크:&quot;, g.ndim) . [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] 랭크: 3 . reshape . reshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다. . g2 = g.reshape(4,6) print(g2) print(&quot;랭크:&quot;, g2.ndim) . [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]] 랭크: 2 . 행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요). . g2[1, 2] = 999 g2 . array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 999, 9, 10, 11], [ 12, 13, 14, 15, 16, 17], [ 18, 19, 20, 21, 22, 23]]) . 이에 상응하는 g의 원소도 수정됩니다. . g . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [999, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]]]) . ravel . 마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다: . g.ravel() . array([ 0, 1, 2, 3, 4, 5, 6, 7, 999, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . &#49328;&#49696; &#50672;&#49328; . 일반적인 산술 연산자(+, -, *, /, //, ** 등)는 모두 ndarray와 사용할 수 있습니다. 이 연산자는 원소별로 적용됩니다: . a = np.array([14, 23, 32, 41]) b = np.array([5, 4, 3, 2]) print(&quot;a + b =&quot;, a + b) print(&quot;a - b =&quot;, a - b) print(&quot;a * b =&quot;, a * b) print(&quot;a / b =&quot;, a / b) print(&quot;a // b =&quot;, a // b) print(&quot;a % b =&quot;, a % b) print(&quot;a ** b =&quot;, a ** b) . a + b = [19 27 35 43] a - b = [ 9 19 29 39] a * b = [70 92 96 82] a / b = [ 2.8 5.75 10.66666667 20.5 ] a // b = [ 2 5 10 20] a % b = [4 3 2 1] a ** b = [537824 279841 32768 1681] . 여기 곱셈은 행렬 곱셈이 아닙니다. 행렬 연산은 아래에서 설명합니다. . 배열의 크기는 같아야 합니다. 그렇지 않으면 넘파이가 브로드캐스팅 규칙을 적용합니다. . &#48652;&#47196;&#46300;&#52880;&#49828;&#54021; . 일반적으로 넘파이는 동일한 크기의 배열을 기대합니다. 그렇지 않은 상황에는 브로드캐시틍 규칙을 적용합니다: . &#44508;&#52825; 1 . 배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다. . h = np.arange(5).reshape(1, 1, 5) h . array([[[0, 1, 2, 3, 4]]]) . 여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다! . h + [10, 20, 30, 40, 50] # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]] . array([[[10, 21, 32, 43, 54]]]) . &#44508;&#52825; 2 . 특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다. . k = np.arange(6).reshape(2, 3) k . array([[0, 1, 2], [3, 4, 5]]) . (2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다: . k + [[100], [200]] # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]] . array([[100, 101, 102], [203, 204, 205]]) . 규칙 1과 2를 합치면 다음과 같이 동작합니다: . k + [100, 200, 300] # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]] . array([[100, 201, 302], [103, 204, 305]]) . 또 매우 간단히 다음 처럼 해도 됩니다: . k + 1000 # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]] . array([[1000, 1001, 1002], [1003, 1004, 1005]]) . &#44508;&#52825; 3 . 규칙 1 &amp; 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다. . try: k + [33, 44] except ValueError as e: print(e) . operands could not be broadcast together with shapes (2,3) (2,) . 브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요. . &#50629;&#52880;&#49828;&#54021; . dtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다. . k1 = np.arange(0, 5, dtype=np.uint8) print(k1.dtype, k1) . uint8 [0 1 2 3 4] . k2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8) print(k2.dtype, k2) . int16 [ 5 7 9 11 13] . 모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다. . k3 = k1 + 1.5 print(k3.dtype, k3) . float64 [1.5 2.5 3.5 4.5 5.5] . &#51312;&#44148; &#50672;&#49328;&#51088; . 조건 연산자도 원소별로 적용됩니다: . m = np.array([20, -5, 30, 40]) m &lt; [15, 16, 35, 36] . array([False, True, True, False]) . 브로드캐스팅을 사용합니다: . m &lt; 25 # m &lt; [25, 25, 25, 25] 와 동일 . array([ True, True, False, False]) . 불리언 인덱싱과 함께 사용하면 아주 유용합니다(아래에서 설명하겠습니다). . m[m &lt; 25] . array([20, -5]) . &#49688;&#54617; &#54632;&#49688;&#50752; &#53685;&#44228; &#54632;&#49688; . ndarray에서 사용할 수 있는 수학 함수와 통계 함수가 많습니다. . ndarray &#47700;&#49436;&#46300; . 일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면: . a = np.array([[-2.5, 3.1, 7], [10, 11, 12]]) print(a) print(&quot;평균 =&quot;, a.mean()) . [[-2.5 3.1 7. ] [10. 11. 12. ]] 평균 = 6.766666666666667 . 이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다. . 다음은 유용한 ndarray 메서드입니다: . for func in (a.min, a.max, a.sum, a.prod, a.std, a.var): print(func.__name__, &quot;=&quot;, func()) . min = -2.5 max = 12.0 sum = 40.6 prod = -71610.0 std = 5.084835843520964 var = 25.855555555555554 . 이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면: . c=np.arange(24).reshape(2,3,4) c . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) . c.sum(axis=0) # 첫 번째 축을 따라 더함, 결과는 3x4 배열 . array([[12, 14, 16, 18], [20, 22, 24, 26], [28, 30, 32, 34]]) . c.sum(axis=1) # 두 번째 축을 따라 더함, 결과는 2x4 배열 . array([[12, 15, 18, 21], [48, 51, 54, 57]]) . 여러 축에 대해서 더할 수도 있습니다: . c.sum(axis=(0,2)) # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열 . array([ 60, 92, 124]) . 0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23 . (60, 92, 124) . &#51068;&#48152; &#54632;&#49688; . 넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다: . a = np.array([[-2.5, 3.1, 7], [10, 11, 12]]) np.square(a) . array([[ 6.25, 9.61, 49. ], [100. , 121. , 144. ]]) . 다음은 유용한 단항 일반 함수들입니다: . print(&quot;원본 ndarray&quot;) print(a) for func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos): print(&quot; n&quot;, func.__name__) print(func(a)) . 원본 ndarray [[-2.5 3.1 7. ] [10. 11. 12. ]] absolute [[ 2.5 3.1 7. ] [10. 11. 12. ]] sqrt [[ nan 1.76068169 2.64575131] [3.16227766 3.31662479 3.46410162]] exp [[8.20849986e-02 2.21979513e+01 1.09663316e+03] [2.20264658e+04 5.98741417e+04 1.62754791e+05]] log [[ nan 1.13140211 1.94591015] [2.30258509 2.39789527 2.48490665]] sign [[-1. 1. 1.] [ 1. 1. 1.]] ceil [[-2. 4. 7.] [10. 11. 12.]] modf (array([[-0.5, 0.1, 0. ], [ 0. , 0. , 0. ]]), array([[-2., 3., 7.], [10., 11., 12.]])) isnan [[False False False] [False False False]] cos [[-0.80114362 -0.99913515 0.75390225] [-0.83907153 0.0044257 0.84385396]] . &lt;ipython-input-59-d791c8e37e6f&gt;:5: RuntimeWarning: invalid value encountered in sqrt print(func(a)) &lt;ipython-input-59-d791c8e37e6f&gt;:5: RuntimeWarning: invalid value encountered in log print(func(a)) . &#51060;&#54637; &#51068;&#48152; &#54632;&#49688; . 두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다: . a = np.array([1, -2, 3, 4]) b = np.array([2, 8, -1, 7]) np.add(a, b) # a + b 와 동일 . array([ 3, 6, 2, 11]) . np.greater(a, b) # a &gt; b 와 동일 . array([False, False, True, False]) . np.maximum(a, b) . array([2, 8, 3, 7]) . np.copysign(a, b) . array([ 1., 2., -3., 4.]) . &#48176;&#50676; &#51064;&#45937;&#49905; . 1&#52264;&#50896; &#48176;&#50676; . 1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다: . a = np.array([1, 5, 3, 19, 13, 7, 3]) a[3] . 19 . a[2:5] . array([ 3, 19, 13]) . a[2:-1] . array([ 3, 19, 13, 7]) . a[:2] . array([1, 5]) . a[2::2] . array([ 3, 13, 3]) . a[::-1] . array([ 3, 7, 13, 19, 3, 5, 1]) . 물론 원소를 수정할 수 있죠: . a[3]=999 a . array([ 1, 5, 3, 999, 13, 7, 3]) . 슬라이싱을 사용해 ndarray를 수정할 수 있습니다: . a[2:5] = [997, 998, 999] a . array([ 1, 5, 997, 998, 999, 7, 3]) . &#48372;&#53685;&#51032; &#54028;&#51060;&#50028; &#48176;&#50676;&#44284; &#52264;&#51060;&#51216; . 보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다. . a[2:5] = -1 a . array([ 1, 5, -1, -1, -1, 7, 3]) . 또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다: . try: a[2:5] = [1,2,3,4,5,6] # 너무 길어요 except ValueError as e: print(e) . cannot copy sequence with size 6 to array axis with dimension 3 . 원소를 삭제할 수도 없습니다: . try: del a[2:5] except ValueError as e: print(e) . cannot delete array elements . 중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다! . a_slice = a[2:6] a_slice[1] = 1000 a # 원본 배열이 수정됩니다! . array([ 1, 5, -1, 1000, -1, 7, 3]) . a[3] = 2000 a_slice # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다! . array([ -1, 2000, -1, 7]) . 데이터를 복사하려면 copy 메서드를 사용해야 합니다: . another_slice = a[2:6].copy() another_slice[1] = 3000 a # 원본 배열이 수정되지 않습니다 . array([ 1, 5, -1, 2000, -1, 7, 3]) . a[3] = 4000 another_slice # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다 . array([ -1, 3000, -1, 7]) . &#45796;&#52264;&#50896; &#48176;&#50676; . 다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다: . b = np.arange(48).reshape(4, 12) b . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]) . b[1, 2] # 행 1, 열 2 . 14 . b[1, :] # 행 1, 모든 열 . array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . b[:, 1] # 모든 행, 열 1 . array([ 1, 13, 25, 37]) . 주의: 다음 두 표현에는 미묘한 차이가 있습니다: . b[1, :] . array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . b[1:2, :] . array([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]]) . 첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다. . &#54060;&#49884; &#51064;&#45937;&#49905;(Fancy indexing) . 관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다. . b[(0,2), 2:5] # 행 0과 2, 열 2에서 4(5-1)까지 . array([[ 2, 3, 4], [26, 27, 28]]) . b[:, (-1, 2, -1)] # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로) . array([[11, 2, 11], [23, 14, 23], [35, 26, 35], [47, 38, 47]]) . 여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다. . b[(-1, 2, -1, 2), (5, 9, 1, 9)] # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again) . array([41, 33, 37, 33]) . &#44256;&#52264;&#50896; . 고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다: . c = b.reshape(4,2,6) c . array([[[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]], [[12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]], [[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]], [[36, 37, 38, 39, 40, 41], [42, 43, 44, 45, 46, 47]]]) . c[2, 1, 4] # 행렬 2, 행 1, 열 4 . 34 . c[2, :, 3] # 행렬 2, 모든 행, 열 3 . array([27, 33]) . 어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다: . c[2, 1] # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다. . array([30, 31, 32, 33, 34, 35]) . &#49373;&#47029; &#48512;&#54840; (...) . 생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다. . c[2, ...] # 행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일 . array([[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]]) . c[2, 1, ...] # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일 . array([30, 31, 32, 33, 34, 35]) . c[2, ..., 3] # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일 . array([27, 33]) . c[..., 3] # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일 . array([[ 3, 9], [15, 21], [27, 33], [39, 45]]) . &#48520;&#47532;&#50616; &#51064;&#45937;&#49905; . 불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다. . b = np.arange(48).reshape(4, 12) b . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]) . rows_on = np.array([True, False, True, False]) b[rows_on, :] # 행 0과 2, 모든 열. b[(0, 2), :]와 동일 . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]]) . cols_on = np.array([False, True, False] * 4) b[:, cols_on] # 모든 행, 열 1, 4, 7, 10 . array([[ 1, 4, 7, 10], [13, 16, 19, 22], [25, 28, 31, 34], [37, 40, 43, 46]]) . np.ix_ . 여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다: . b[np.ix_(rows_on, cols_on)] . array([[ 1, 4, 7, 10], [25, 28, 31, 34]]) . np.ix_(rows_on, cols_on) . (array([[0], [2]]), array([[ 1, 4, 7, 10]])) . ndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다: . b[b % 3 == 1] . array([ 1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46]) . &#48152;&#48373; . ndarray를 반복하는 것은 일반적인 파이썬 배열을 반복한는 것과 매우 유사합니다. 다차원 배열을 반복하면 첫 번째 축에 대해서 수행됩니다. . c = np.arange(24).reshape(2, 3, 4) # 3D 배열 (두 개의 3x4 행렬로 구성됨) c . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) . for m in c: print(&quot;아이템:&quot;) print(m) . 아이템: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 아이템: [[12 13 14 15] [16 17 18 19] [20 21 22 23]] . for i in range(len(c)): # len(c) == c.shape[0] print(&quot;아이템:&quot;) print(c[i]) . 아이템: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 아이템: [[12 13 14 15] [16 17 18 19] [20 21 22 23]] . ndarray에 있는 모든 원소를 반복하려면 flat 속성을 사용합니다: . for i in c.flat: print(&quot;아이템:&quot;, i) . 아이템: 0 아이템: 1 아이템: 2 아이템: 3 아이템: 4 아이템: 5 아이템: 6 아이템: 7 아이템: 8 아이템: 9 아이템: 10 아이템: 11 아이템: 12 아이템: 13 아이템: 14 아이템: 15 아이템: 16 아이템: 17 아이템: 18 아이템: 19 아이템: 20 아이템: 21 아이템: 22 아이템: 23 . &#48176;&#50676; &#49939;&#44592; . 종종 다른 배열을 쌓아야 할 때가 있습니다. 넘파이는 이를 위해 몇 개의 함수를 제공합니다. 먼저 배열 몇 개를 만들어 보죠. . q1 = np.full((3,4), 1.0) q1 . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . q2 = np.full((4,4), 2.0) q2 . array([[2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.]]) . q3 = np.full((3,4), 3.0) q3 . array([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . vstack . vstack 함수를 사용하여 수직으로 쌓아보죠: . q4 = np.vstack((q1, q2, q3)) q4 . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . q4.shape . (10, 4) . q1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다). . hstack . hstack을 사용해 수평으로도 쌓을 수 있습니다: . q5 = np.hstack((q1, q3)) q5 . array([[1., 1., 1., 1., 3., 3., 3., 3.], [1., 1., 1., 1., 3., 3., 3., 3.], [1., 1., 1., 1., 3., 3., 3., 3.]]) . q5.shape . (3, 8) . q1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다: . try: q5 = np.hstack((q1, q2, q3)) except ValueError as e: print(e) . all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4 . concatenate . concatenate 함수는 지정한 축으로도 배열을 쌓습니다. . q7 = np.concatenate((q1, q2, q3), axis=0) # vstack과 동일 q7 . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . q7.shape . (10, 4) . 예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다. . stack . stack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다. . q8 = np.stack((q1, q3)) q8 . array([[[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], [[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]]) . q8.shape . (2, 3, 4) . &#48176;&#50676; &#48516;&#54624; . 분할은 쌓기의 반대입니다. 예를 들어 vsplit 함수는 행렬을 수직으로 분할합니다. . 먼저 6x4 행렬을 만들어 보죠: . r = np.arange(24).reshape(6,4) r . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]) . 수직으로 동일한 크기로 나누어 보겠습니다: . r1, r2, r3 = np.vsplit(r, 3) r1 . array([[0, 1, 2, 3], [4, 5, 6, 7]]) . r2 . array([[ 8, 9, 10, 11], [12, 13, 14, 15]]) . r3 . array([[16, 17, 18, 19], [20, 21, 22, 23]]) . split 함수는 주어진 축을 따라 배열을 분할합니다. vsplit는 axis=0으로 split를 호출하는 것과 같습니다. hsplit 함수는 axis=1로 split를 호출하는 것과 같습니다: . r4, r5 = np.hsplit(r, 2) r4 . array([[ 0, 1], [ 4, 5], [ 8, 9], [12, 13], [16, 17], [20, 21]]) . r5 . array([[ 2, 3], [ 6, 7], [10, 11], [14, 15], [18, 19], [22, 23]]) . &#48176;&#50676; &#51204;&#52824; . transpose 메서드는 주어진 순서대로 축을 뒤바꾸어 ndarray 데이터에 대한 새로운 뷰를 만듭니다. . 예를 위해 3D 배열을 만들어 보죠: . t = np.arange(24).reshape(4,2,3) t . array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]], [[12, 13, 14], [15, 16, 17]], [[18, 19, 20], [21, 22, 23]]]) . 0, 1, 2(깊이, 높이, 너비) 축을 1, 2, 0 (깊이→너비, 높이→깊이, 너비→높이) 순서로 바꾼 ndarray를 만들어 보겠습니다: . t1 = t.transpose((1,2,0)) t1 . array([[[ 0, 6, 12, 18], [ 1, 7, 13, 19], [ 2, 8, 14, 20]], [[ 3, 9, 15, 21], [ 4, 10, 16, 22], [ 5, 11, 17, 23]]]) . t1.shape . (2, 3, 4) . transpose 기본값은 차원의 순서를 역전시킵니다: . t2 = t.transpose() # t.transpose((2, 1, 0))와 동일 t2 . array([[[ 0, 6, 12, 18], [ 3, 9, 15, 21]], [[ 1, 7, 13, 19], [ 4, 10, 16, 22]], [[ 2, 8, 14, 20], [ 5, 11, 17, 23]]]) . t2.shape . (3, 2, 4) . 넘파이는 두 축을 바꾸는 swapaxes 함수를 제공합니다. 예를 들어 깊이와 높이를 뒤바꾸어 t의 새로운 뷰를 만들어 보죠: . t3 = t.swapaxes(0,1) # t.transpose((1, 0, 2))와 동일 t3 . array([[[ 0, 1, 2], [ 6, 7, 8], [12, 13, 14], [18, 19, 20]], [[ 3, 4, 5], [ 9, 10, 11], [15, 16, 17], [21, 22, 23]]]) . t3.shape . (2, 4, 3) . &#49440;&#54805; &#45824;&#49688;&#54617; . 넘파이 2D 배열을 사용하면 파이썬에서 행렬을 효율적으로 표현할 수 있습니다. 주요 행렬 연산을 간단히 둘러 보겠습니다. 선형 대수학, 벡터와 행렬에 관한 자세한 내용은 Linear Algebra tutorial를 참고하세요. . &#54665;&#47148; &#51204;&#52824; . T 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다: . m1 = np.arange(10).reshape(2,5) m1 . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . m1.T . array([[0, 5], [1, 6], [2, 7], [3, 8], [4, 9]]) . T 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다: . m2 = np.arange(5) m2 . array([0, 1, 2, 3, 4]) . m2.T . array([0, 1, 2, 3, 4]) . 먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다: . m2r = m2.reshape(1,5) m2r . array([[0, 1, 2, 3, 4]]) . m2r.T . array([[0], [1], [2], [3], [4]]) . &#54665;&#47148; &#44273;&#49480; . 두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠. . n1 = np.arange(10).reshape(2, 5) n1 . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . n2 = np.arange(15).reshape(5,3) n2 . array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) . n1.dot(n2) . array([[ 90, 100, 110], [240, 275, 310]]) . 주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다. . &#50669;&#54665;&#47148;&#44284; &#50976;&#49324; &#50669;&#54665;&#47148; . numpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다: . import numpy.linalg as linalg m3 = np.array([[1,2,3],[5,7,11],[21,29,31]]) m3 . array([[ 1, 2, 3], [ 5, 7, 11], [21, 29, 31]]) . linalg.inv(m3) . array([[-2.31818182, 0.56818182, 0.02272727], [ 1.72727273, -0.72727273, 0.09090909], [-0.04545455, 0.29545455, -0.06818182]]) . pinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다: . linalg.pinv(m3) . array([[-2.31818182, 0.56818182, 0.02272727], [ 1.72727273, -0.72727273, 0.09090909], [-0.04545455, 0.29545455, -0.06818182]]) . &#45800;&#50948; &#54665;&#47148; . 행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다): . m3.dot(linalg.inv(m3)) . array([[ 1.00000000e+00, -1.66533454e-16, 0.00000000e+00], [ 6.31439345e-16, 1.00000000e+00, -1.38777878e-16], [ 5.21110932e-15, -2.38697950e-15, 1.00000000e+00]]) . eye 함수는 NxN 크기의 단위 행렬을 만듭니다: . np.eye(3) . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . QR &#48516;&#54644; . qr 함수는 행렬을 QR 분해합니다: . q, r = linalg.qr(m3) q . array([[-0.04627448, 0.98786672, 0.14824986], [-0.23137241, 0.13377362, -0.96362411], [-0.97176411, -0.07889213, 0.22237479]]) . r . array([[-21.61018278, -29.89331494, -32.80860727], [ 0. , 0.62427688, 1.9894538 ], [ 0. , 0. , -3.26149699]]) . q.dot(r) # q.r는 m3와 같습니다 . array([[ 1., 2., 3.], [ 5., 7., 11.], [21., 29., 31.]]) . &#54665;&#47148;&#49885; . det 함수는 행렬식을 계산합니다: . linalg.det(m3) # 행렬식 계산 . 43.99999999999997 . &#44256;&#50995;&#44050;&#44284; &#44256;&#50976;&#48289;&#53552; . eig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다: . eigenvalues, eigenvectors = linalg.eig(m3) eigenvalues # λ . array([42.26600592, -0.35798416, -2.90802176]) . eigenvectors # v . array([[-0.08381182, -0.76283526, -0.18913107], [-0.3075286 , 0.64133975, -0.6853186 ], [-0.94784057, -0.08225377, 0.70325518]]) . m3.dot(eigenvectors) - eigenvalues * eigenvectors # m3.v - λ*v = 0 . array([[ 8.88178420e-15, 2.22044605e-16, -3.10862447e-15], [ 3.55271368e-15, 2.02615702e-15, -1.11022302e-15], [ 3.55271368e-14, 3.33413852e-15, -8.43769499e-15]]) . &#53945;&#51079;&#44050; &#48516;&#54644; . svd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다: . m4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]]) m4 . array([[1, 0, 0, 0, 2], [0, 0, 3, 0, 0], [0, 0, 0, 0, 0], [0, 2, 0, 0, 0]]) . U, S_diag, V = linalg.svd(m4) U . array([[ 0., 1., 0., 0.], [ 1., 0., 0., 0.], [ 0., 0., 0., -1.], [ 0., 0., 1., 0.]]) . S_diag . array([3. , 2.23606798, 2. , 0. ]) . svd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다: . S = np.zeros((4, 5)) S[np.diag_indices(4)] = S_diag S # Σ . array([[3. , 0. , 0. , 0. , 0. ], [0. , 2.23606798, 0. , 0. , 0. ], [0. , 0. , 2. , 0. , 0. ], [0. , 0. , 0. , 0. , 0. ]]) . V . array([[-0. , 0. , 1. , -0. , 0. ], [ 0.4472136 , 0. , 0. , 0. , 0.89442719], [-0. , 1. , 0. , -0. , 0. ], [ 0. , 0. , 0. , 1. , 0. ], [-0.89442719, 0. , 0. , 0. , 0.4472136 ]]) . U.dot(S).dot(V) # U.Σ.V == m4 . array([[1., 0., 0., 0., 2.], [0., 0., 3., 0., 0.], [0., 0., 0., 0., 0.], [0., 2., 0., 0., 0.]]) . &#45824;&#44033;&#50896;&#49548;&#50752; &#45824;&#44033;&#54633; . np.diag(m3) # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래) . array([ 1, 7, 31]) . np.trace(m3) # np.diag(m3).sum()와 같습니다 . 39 . &#49440;&#54805; &#48169;&#51221;&#49885; &#54400;&#44592; . solve 함수는 다음과 같은 선형 방정식을 풉니다: . $2x + 6y = 6$ | $5x + 3y = -9$ | . coeffs = np.array([[2, 6], [5, 3]]) depvars = np.array([6, -9]) solution = linalg.solve(coeffs, depvars) solution . array([-3., 2.]) . solution을 확인해 보죠: . coeffs.dot(solution), depvars # 네 같네요 . (array([ 6., -9.]), array([ 6, -9])) . 좋습니다! 다른 방식으로도 solution을 확인해 보죠: . np.allclose(coeffs.dot(solution), depvars) . True . &#48289;&#53552;&#54868; . 한 번에 하나씩 개별 배열 원소에 대해 연산을 실행하는 대신 배열 연산을 사용하면 훨씬 효율적인 코드를 만들 수 있습니다. 이를 벡터화라고 합니다. 이를 사용하여 넘파이의 최적화된 성능을 활용할 수 있습니다. . 예를 들어, $sin(xy/40.5)$ 식을 기반으로 768x1024 크기 배열을 생성하려고 합니다. 중첩 반복문 안에 파이썬의 math 함수를 사용하는 것은 나쁜 방법입니다: . import math data = np.empty((768, 1024)) for y in range(768): for x in range(1024): data[y, x] = math.sin(x*y/40.5) # 매우 비효율적입니다! . 작동은 하지만 순수한 파이썬 코드로 반복문이 진행되기 때문에 아주 비효율적입니다. 이 알고리즘을 벡터화해 보죠. 먼저 넘파이 meshgrid 함수로 좌표 벡터를 사용해 행렬을 만듭니다. . x_coords = np.arange(0, 1024) # [0, 1, 2, ..., 1023] y_coords = np.arange(0, 768) # [0, 1, 2, ..., 767] X, Y = np.meshgrid(x_coords, y_coords) X . array([[ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], ..., [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023]]) . Y . array([[ 0, 0, 0, ..., 0, 0, 0], [ 1, 1, 1, ..., 1, 1, 1], [ 2, 2, 2, ..., 2, 2, 2], ..., [765, 765, 765, ..., 765, 765, 765], [766, 766, 766, ..., 766, 766, 766], [767, 767, 767, ..., 767, 767, 767]]) . 여기서 볼 수 있듯이 X와 Y 모두 768x1024 배열입니다. X에 있는 모든 값은 수평 좌표에 해당합니다. Y에 있는 모든 값은 수직 좌표에 해당합니다. . 이제 간단히 배열 연산을 사용해 계산할 수 있습니다: . data = np.sin(X*Y/40.5) . 맷플롯립의 imshow 함수를 사용해 이 데이터를 그려보죠(matplotlib tutorial을 참조하세요). . import matplotlib.pyplot as plt import matplotlib.cm as cm fig = plt.figure(1, figsize=(7, 6)) plt.imshow(data, cmap=cm.hot) plt.show() . &#51200;&#51109;&#44284; &#47196;&#46377; . 넘파이는 ndarray를 바이너리 또는 텍스트 포맷으로 손쉽게 저장하고 로드할 수 있습니다. . &#48148;&#51060;&#45320;&#47532; .npy &#54252;&#47607; . 랜덤 배열을 만들고 저장해 보죠. . a = np.random.rand(2,3) a . array([[0.5435938 , 0.92886307, 0.01535158], [0.4157283 , 0.9102127 , 0.55129708]]) . np.save(&quot;my_array&quot;, a) . 끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다: . with open(&quot;my_array.npy&quot;, &quot;rb&quot;) as f: content = f.read() content . b&#34; x93NUMPY x01 x00v x00{&#39;descr&#39;: &#39;&lt;f8&#39;, &#39;fortran_order&#39;: False, &#39;shape&#39;: (2, 3), } nY xc1 xfc xd0 x1ee xe1? xde{3 t? xb9 xed? x80V x08 xef xa5p x8f? x96I} xe0J x9b xda? xe0U xfaav xed? xd8 xe50 xc59 xa4 xe1?&#34; . 이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다: . a_loaded = np.load(&quot;my_array.npy&quot;) a_loaded . array([[0.5435938 , 0.92886307, 0.01535158], [0.4157283 , 0.9102127 , 0.55129708]]) . &#53581;&#49828;&#53944; &#54252;&#47607; . 배열을 텍스트 포맷으로 저장해 보죠: . np.savetxt(&quot;my_array.csv&quot;, a) . 파일 내용을 확인해 보겠습니다: . with open(&quot;my_array.csv&quot;, &quot;rt&quot;) as f: print(f.read()) . 5.435937959464737235e-01 9.288630656918674955e-01 1.535157809943688001e-02 4.157283012656532994e-01 9.102126992826775620e-01 5.512970782648904944e-01 . 이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다: . np.savetxt(&quot;my_array.csv&quot;, a, delimiter=&quot;,&quot;) . 이 파일을 로드하려면 loadtxt 함수를 사용합니다: . a_loaded = np.loadtxt(&quot;my_array.csv&quot;, delimiter=&quot;,&quot;) a_loaded . array([[0.5435938 , 0.92886307, 0.01535158], [0.4157283 , 0.9102127 , 0.55129708]]) . &#50517;&#52629;&#46108; .npz &#54252;&#47607; . 여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다: . b = np.arange(24, dtype=np.uint8).reshape(2, 3, 4) b . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]], dtype=uint8) . np.savez(&quot;my_arrays&quot;, my_a=a, my_b=b) . 파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다. . with open(&quot;my_arrays.npz&quot;, &quot;rb&quot;) as f: content = f.read() repr(content)[:180] + &quot;[...]&quot; . &#39;b&#34;PK x03 x04 x14 x00 x00 x00 x00 x00 x00 x00! x00 x063 xcf xb9 xb0 x00 x00 x00 xb0 x00 x00 x00 x08 x00 x14 x00my_a.npy x01 x00 x10 x00 xb0 x00 x00 x00 x00 x00 x00 x00 xb0 x00 x00 x[...]&#39; . 다음과 같이 이 파일을 로드할 수 있습니다: . my_arrays = np.load(&quot;my_arrays.npz&quot;) my_arrays . &lt;numpy.lib.npyio.NpzFile at 0x7f9791c73d60&gt; . 게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다: . my_arrays.keys() . KeysView(&lt;numpy.lib.npyio.NpzFile object at 0x7f9791c73d60&gt;) . my_arrays[&quot;my_a&quot;] . array([[0.5435938 , 0.92886307, 0.01535158], [0.4157283 , 0.9102127 , 0.55129708]]) . &#44536; &#45796;&#51020;&#51008;? . 넘파이 기본 요소를 모두 배웠지만 훨씬 더 많은 기능이 있습니다. 이를 배우는 가장 좋은 방법은 넘파이를 직접 실습해 보고 훌륭한 넘파이 문서에서 필요한 함수와 기능을 찾아 보세요. .",
            "url": "https://woojaaeon.github.io/jaong/fastpages/jupyter/2022/03/04/numpy.html",
            "relUrl": "/fastpages/jupyter/2022/03/04/numpy.html",
            "date": " • Mar 4, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://woojaaeon.github.io/jaong/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://woojaaeon.github.io/jaong/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Learn : Data_Mining Lecture . Use program . - python . - R . - SQL . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://woojaaeon.github.io/jaong/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://woojaaeon.github.io/jaong/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}